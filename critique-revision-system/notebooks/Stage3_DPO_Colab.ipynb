{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: Critiqueâ€“Revision DPO Orchestrator (Colab)\n",
    "\n",
    "Lightweight notebook to:\n",
    "- Clone the repo (at the top)\n",
    "- Create uv venv and install deps\n",
    "- Generate critique/revision pairs (default config)\n",
    "- Train DPO with Stage 2 adapters as initialization\n",
    "- Quick evaluation and safety scoring attempt (if Stage 1 checkpoint is available)\n",
    "\n",
    "Defaults:\n",
    "- Base model: google/gemma-2b-it\n",
    "- Local save only\n",
    "- W&B disabled\n",
    "- Dataset subset: default (no special permissions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e7cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime checks and config\n",
    "import os, sys, json, random\n",
    "print('Python:', sys.version)\n",
    "\n",
    "# Colab GPU info (may be absent)\n",
    "try:\n",
    "    import subprocess\n",
    "    print(subprocess.run(['nvidia-smi'], check=False, capture_output=True, text=True).stdout[:1000])\n",
    "except Exception as e:\n",
    "    print('No GPU info available:', e)\n",
    "\n",
    "# Repository URL and directories\n",
    "REPO_URL = 'https://github.com/Jai-Dhiman/ml-learning.git'\n",
    "REPO_DIR = '/content/ml-learning'\n",
    "BASE_MODEL_ID = 'google/gemma-2b-it'\n",
    "ARTIFACTS_DIR = f'{REPO_DIR}/artifacts'\n",
    "STAGE2_ADAPTER_DIR = f'{ARTIFACTS_DIR}/stage2_artifacts/lora_adapters'\n",
    "STAGE3_DIR = f'{ARTIFACTS_DIR}/stage3_artifacts'\n",
    "PAIRS_PATH = f'{STAGE3_DIR}/pairs/pairs.jsonl'\n",
    "STAGE3_LORA_DIR = f'{STAGE3_DIR}/models/lora_adapters'\n",
    "\n",
    "# Export paths as environment variables for bash cells\n",
    "os.environ['REPO_DIR'] = REPO_DIR\n",
    "os.environ['REPO_URL'] = REPO_URL\n",
    "os.environ['BASE_MODEL_ID'] = BASE_MODEL_ID\n",
    "os.environ['ARTIFACTS_DIR'] = ARTIFACTS_DIR\n",
    "os.environ['STAGE2_ADAPTER_DIR'] = STAGE2_ADAPTER_DIR\n",
    "os.environ['STAGE3_DIR'] = STAGE3_DIR\n",
    "os.environ['PAIRS_PATH'] = PAIRS_PATH\n",
    "os.environ['STAGE3_LORA_DIR'] = STAGE3_LORA_DIR\n",
    "\n",
    "# Env flags\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'\n",
    "os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'\n",
    "print('Configured environment flags for quiet, offline-friendly operation.')\n",
    "print(f'REPO_DIR={REPO_DIR}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv\n",
    "!pip -q install -U uv\n",
    "import shutil, os\n",
    "print('uv version:', shutil.which('uv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (top of notebook)\n",
    "%%bash\n",
    "set -e\n",
    "rm -rf \"/content/ml-learning\"\n",
    "git clone https://github.com/Jai-Dhiman/ml-learning.git \"/content/ml-learning\"\n",
    "ls -la \"/content/ml-learning\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ef246",
   "metadata": {
    "id": "hf-login"
   },
   "outputs": [],
   "source": [
    "# Login to Hugging Face (required for Gemma model access)\n",
    "# Secure login without storing/printing your token.\n",
    "# Token will be set as HF_TOKEN environment variable for bash cells.\n",
    "import os\n",
    "\n",
    "# Clear any existing tokens\n",
    "os.environ.pop(\"HF_TOKEN\", None)\n",
    "os.environ.pop(\"HUGGINGFACEHUB_API_TOKEN\", None)\n",
    "\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "try:\n",
    "    import getpass as gp\n",
    "    raw = gp.getpass(\"Paste your Hugging Face token (input hidden): \")\n",
    "    token = raw.decode() if isinstance(raw, (bytes, bytearray)) else raw\n",
    "    if not isinstance(token, str):\n",
    "        raise TypeError(f\"Unexpected token type: {type(token).__name__}\")\n",
    "    token = token.strip()\n",
    "    if not token:\n",
    "        raise ValueError(\"Empty token provided\")\n",
    "    \n",
    "    # Login and set environment variable\n",
    "    login(token=token, add_to_git_credential=False)\n",
    "    os.environ['HF_TOKEN'] = token\n",
    "    \n",
    "    who = HfApi().whoami(token=token)\n",
    "    print(f\"Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "    print('HF_TOKEN environment variable set for bash cells.')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[HF Login] getpass flow failed: {e}\")\n",
    "    print(\"Falling back to interactive login widget...\")\n",
    "    login()\n",
    "    \n",
    "    # Try to get token from saved credentials\n",
    "    try:\n",
    "        from huggingface_hub import HfFolder\n",
    "        token = HfFolder.get_token()\n",
    "        if token:\n",
    "            os.environ['HF_TOKEN'] = token\n",
    "            print('HF_TOKEN environment variable set from saved credentials.')\n",
    "        who = HfApi().whoami()\n",
    "        print(f\"Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"[HF Login] Could not set HF_TOKEN env var: {e2}\")\n",
    "        print(\"You may need to run 'huggingface-cli login' in a bash cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca47e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create uv venv and install dependencies\n",
    "%%bash\n",
    "set -e\n",
    "cd \"$REPO_DIR\"\n",
    "\n",
    "# Create venv if it doesn't exist\n",
    "if [ ! -d .venv ]; then\n",
    "  echo 'Creating uv virtual environment...'\n",
    "  uv venv\n",
    "fi\n",
    "\n",
    "# Activate and install dependencies\n",
    "echo 'Installing dependencies...'\n",
    "\n",
    "# Install torch first with CUDA support if available\n",
    "uv pip install --python .venv/bin/python torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 || \\\n",
    "  uv pip install --python .venv/bin/python torch torchvision torchaudio\n",
    "\n",
    "# Install remaining dependencies\n",
    "uv pip install --python .venv/bin/python \\\n",
    "  \"transformers>=4.43.0\" \\\n",
    "  \"trl>=0.9.6\" \\\n",
    "  \"peft>=0.13.0\" \\\n",
    "  \"datasets>=2.19.0\" \\\n",
    "  \"accelerate>=0.28.0\" \\\n",
    "  sentencepiece \\\n",
    "  safetensors \\\n",
    "  einops \\\n",
    "  evaluate \\\n",
    "  \"protobuf<5\" \\\n",
    "  \"jax[cpu]==0.4.38\" \\\n",
    "  \"flax>=0.8.4,<0.9.0\" \\\n",
    "  \"optax>=0.2.2,<0.3.0\"\n",
    "\n",
    "# Verify installation\n",
    "echo 'Verifying torch installation...'\n",
    ".venv/bin/python -c \"import torch; print(f'PyTorch {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')\"\n",
    "echo 'Dependencies installed successfully!'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Stage 2 adapters exist (fail fast)\n",
    "from pathlib import Path\n",
    "stage2 = Path(STAGE2_ADAPTER_DIR)\n",
    "if not stage2.exists() or not any(stage2.iterdir()):\n",
    "    raise FileNotFoundError(\n",
    "        f'Stage 2 LoRA adapters not found at {stage2}. Please train Stage 2 first or place adapters there.'\n",
    "    )\n",
    "print('Stage 2 adapters found at:', STAGE2_ADAPTER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Set Stage 1 checkpoint env var for safety scoring if present\n",
    "import os\n",
    "s1a = f'{REPO_DIR}/artifacts/stage1_artifacts/checkpoints/best_model'\n",
    "s1b = f'{REPO_DIR}/safety-text-classifier/checkpoints/best_model'\n",
    "if os.path.isdir(s1a):\n",
    "    os.environ['STAGE1_CKPT_DIR'] = s1a\n",
    "elif os.path.isdir(s1b):\n",
    "    os.environ['STAGE1_CKPT_DIR'] = s1b\n",
    "print('STAGE1_CKPT_DIR =', os.environ.get('STAGE1_CKPT_DIR'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1135707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preflight: generate 2 pairs with quality validation\n",
    "%%bash\n",
    "set -e\n",
    "cd /content/ml-learning\n",
    "mkdir -p /content/ml-learning/artifacts/stage3_artifacts/preflight\n",
    "echo '=== Generating 2 preflight pairs for quality check ==='\n",
    "uv run python /content/ml-learning/critique-revision-system/src/critique_revision.py \\\n",
    "  --adapter-path /content/ml-learning/artifacts/stage2_artifacts/lora_adapters \\\n",
    "  --dataset-subset default \\\n",
    "  --split \"test[:2]\" \\\n",
    "  --num-examples 2 \\\n",
    "  --max-new-tokens 256 \\\n",
    "  --temperature 0.5 \\\n",
    "  --output /content/ml-learning/artifacts/stage3_artifacts/preflight/pairs_2.jsonl\n",
    "echo 'Preflight pairs at: /content/ml-learning/artifacts/stage3_artifacts/preflight/pairs_2.jsonl'\n",
    "ls -lh /content/ml-learning/artifacts/stage3_artifacts/preflight/pairs_2.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality_check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preflight: Quality validation of generated pairs\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print('=' * 60)\n",
    "print('PREFLIGHT QUALITY CHECK')\n",
    "print('=' * 60)\n",
    "\n",
    "pairs_file = Path(f'{STAGE3_DIR}/preflight/pairs_2.jsonl')\n",
    "if not pairs_file.exists():\n",
    "    raise FileNotFoundError(f'Pairs file not found: {pairs_file}')\n",
    "\n",
    "with open(pairs_file, 'r', encoding='utf-8') as f:\n",
    "    pairs = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "print(f'Loaded {len(pairs)} pairs for validation\\n')\n",
    "\n",
    "quality_issues = []\n",
    "\n",
    "for i, pair in enumerate(pairs, 1):\n",
    "    print(f'--- PAIR {i} ---')\n",
    "    print(f'Prompt: {pair.get(\"prompt\", \"N/A\")[:100]}...')\n",
    "    \n",
    "    base_resp = pair.get('base_response', '')\n",
    "    revised_resp = pair.get('revised_response', '')\n",
    "    critic_notes = pair.get('critic_notes', '')\n",
    "    \n",
    "    # Quality checks\n",
    "    issues = []\n",
    "    \n",
    "    # Check 1: Multi-turn hallucination (unwanted Human:/Assistant: in responses)\n",
    "    if 'Human:' in base_resp or 'Assistant:' in base_resp:\n",
    "        issues.append('Base response contains multi-turn dialogue (hallucinated)')\n",
    "    if 'Human:' in revised_resp or 'Assistant:' in revised_resp:\n",
    "        issues.append('Revised response contains multi-turn dialogue (hallucinated)')\n",
    "    \n",
    "    # Check 2: Empty or very short responses\n",
    "    if len(base_resp.split()) < 10:\n",
    "        issues.append(f'Base response too short ({len(base_resp.split())} words)')\n",
    "    if len(revised_resp.split()) < 10:\n",
    "        issues.append(f'Revised response too short ({len(revised_resp.split())} words)')\n",
    "    \n",
    "    # Check 3: Critique format validation\n",
    "    if not critic_notes or len(critic_notes.strip()) < 10:\n",
    "        issues.append('Critique is empty or too short')\n",
    "    \n",
    "    # Check 4: Meta-commentary instead of actual revision\n",
    "    meta_indicators = ['draft answer', 'original answer', 'Assistant answers', 'Assistant is']\n",
    "    if any(indicator in revised_resp[:100] for indicator in meta_indicators):\n",
    "        issues.append('Revised response contains meta-commentary instead of clean answer')\n",
    "    \n",
    "    # Check 5: Identical base and revised responses\n",
    "    if base_resp.strip() == revised_resp.strip():\n",
    "        issues.append('Base and revised responses are identical')\n",
    "    \n",
    "    # Check 6: Score validation\n",
    "    base_score = pair.get('base_score', 0)\n",
    "    revised_score = pair.get('revised_score', 0)\n",
    "    chosen = pair.get('chosen', 'unknown')\n",
    "    \n",
    "    print(f'Base score: {base_score:.2f}')\n",
    "    print(f'Revised score: {revised_score:.2f}')\n",
    "    print(f'Chosen: {chosen}')\n",
    "    print(f'Score delta: {revised_score - base_score:.2f}')\n",
    "    \n",
    "    if issues:\n",
    "        quality_issues.extend(issues)\n",
    "        print(f'\\nQUALITY ISSUES DETECTED:')\n",
    "        for issue in issues:\n",
    "            print(f'  - {issue}')\n",
    "    else:\n",
    "        print('No quality issues detected')\n",
    "    \n",
    "    # Show sample outputs\n",
    "    print(f'\\nBase response preview: {base_resp[:200]}...')\n",
    "    print(f'Revised response preview: {revised_resp[:200]}...')\n",
    "    print(f'Critique preview: {critic_notes[:150]}...')\n",
    "    print()\n",
    "\n",
    "print('=' * 60)\n",
    "print('QUALITY CHECK SUMMARY')\n",
    "print('=' * 60)\n",
    "print(f'Total pairs checked: {len(pairs)}')\n",
    "print(f'Total quality issues: {len(quality_issues)}')\n",
    "\n",
    "if quality_issues:\n",
    "    print('\\nISSUES FOUND:')\n",
    "    for issue in set(quality_issues):\n",
    "        count = quality_issues.count(issue)\n",
    "        print(f'  [{count}x] {issue}')\n",
    "    print('\\nWARNING: Quality issues detected in preflight pairs!')\n",
    "    print('Consider:')\n",
    "    print('  1. Adjusting generation parameters (temperature, max_tokens)')\n",
    "    print('  2. Improving prompt engineering in critique_revision.py')\n",
    "    print('  3. Using fewer examples for initial testing')\n",
    "    print('\\nProceeding with full run may produce low-quality training data.')\n",
    "else:\n",
    "    print('\\nAll quality checks passed! Safe to proceed with full generation.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preflight: 1-step DPO training to catch training-time errors with minimal cost\n",
    "%%bash\n",
    "set -e\n",
    "cd /content/ml-learning\n",
    "mkdir -p /content/ml-learning/artifacts/stage3_artifacts/preflight_run\n",
    "echo '=== Starting preflight DPO training (1 step) ==='\n",
    "uv run python /content/ml-learning/critique-revision-system/src/training/train_dpo_stage3.py \\\n",
    "  --repo-root /content/ml-learning \\\n",
    "  --pairs-path /content/ml-learning/artifacts/stage3_artifacts/preflight/pairs_2.jsonl \\\n",
    "  --base-model-id google/gemma-2b-it \\\n",
    "  --stage2-adapter-path /content/ml-learning/artifacts/stage2_artifacts/lora_adapters \\\n",
    "  --output-dir /content/ml-learning/artifacts/stage3_artifacts/preflight_run \\\n",
    "  --per-device-train-batch-size 1 \\\n",
    "  --gradient-accumulation-steps 1 \\\n",
    "  --learning-rate 5e-5 \\\n",
    "  --num-train-epochs 1 \\\n",
    "  --max-steps 1 \\\n",
    "  --beta 0.1 \\\n",
    "  --cpu-ref-model\n",
    "echo '=== Preflight DPO training completed ==='\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f60eceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preflight: sanity checks and single quick generation\n",
    "import json, os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "pre_pairs = f\"{STAGE3_DIR}/preflight/pairs_2.jsonl\"\n",
    "pre_lora = f\"{STAGE3_DIR}/preflight_run/models/lora_adapters\"\n",
    "assert Path(pre_pairs).exists(), f\"Missing preflight pairs: {pre_pairs}\"\n",
    "assert Path(pre_lora).exists(), f\"Missing preflight adapters: {pre_lora}\"\n",
    "\n",
    "with open(pre_pairs, 'r', encoding='utf-8') as f:\n",
    "    line = next(f)\n",
    "    ex = json.loads(line)\n",
    "    prompt = ex.get('prompt') or ex.get('instruction') or ex.get('input') or ex.get('user_prompt')\n",
    "    assert prompt, f\"No prompt in preflight pair keys={list(ex.keys())}\"\n",
    "    prompt = f\"User: {prompt}\\nAssistant:\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=True)\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map='auto' if torch.cuda.is_available() else None,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, pre_lora)\n",
    "inputs = tok(prompt, return_tensors='pt').to(model.device)\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_new_tokens=64, do_sample=True, top_p=0.9, temperature=0.7)\n",
    "text = tok.decode(out[0], skip_special_tokens=True)\n",
    "print('Preflight generation ok. Token count:', out.shape[-1])\n",
    "# Tiny safety scoring if Stage 1 checkpoint is available\n",
    "try:\n",
    "    import sys\n",
    "    sys.path.insert(0, f'{REPO_DIR}/helpful-finetuning/src')\n",
    "    from utils.safety_integration import SafetyFilter\n",
    "    ckpt = os.environ.get('STAGE1_CKPT_DIR')\n",
    "    if not ckpt:\n",
    "        a = f'{REPO_DIR}/artifacts/stage1_artifacts/checkpoints/best_model'\n",
    "        b = f'{REPO_DIR}/safety-text-classifier/checkpoints/best_model'\n",
    "        if os.path.isdir(a):\n",
    "            ckpt = a\n",
    "        elif os.path.isdir(b):\n",
    "            ckpt = b\n",
    "    if ckpt and os.path.isdir(ckpt):\n",
    "        cfg_path = f'{REPO_DIR}/safety-text-classifier/configs/base_config.yaml'\n",
    "        sf = SafetyFilter(cfg_path, ckpt)\n",
    "        score = float(sf.score_text(text))\n",
    "        print('Preflight safety score:', score)\n",
    "    else:\n",
    "        print('Preflight: Stage 1 checkpoint not found; skipping safety scoring.')\n",
    "except Exception as e:\n",
    "    print('Preflight safety scoring failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate critique/revision pairs (full run)\n",
    "%%bash\n",
    "set -e\n",
    "cd /content/ml-learning\n",
    "mkdir -p /content/ml-learning/artifacts/stage3_artifacts/pairs\n",
    "echo '=== Generating 400 critique/revision pairs ==='\n",
    "# Use default subset and a modest slice for a first run\n",
    "uv run python /content/ml-learning/critique-revision-system/src/critique_revision.py \\\n",
    "  --adapter-path /content/ml-learning/artifacts/stage2_artifacts/lora_adapters \\\n",
    "  --dataset-subset default \\\n",
    "  --split \"test[:400]\" \\\n",
    "  --num-examples 400 \\\n",
    "  --output /content/ml-learning/artifacts/stage3_artifacts/pairs/pairs.jsonl\n",
    "echo 'Pairs generated at: /content/ml-learning/artifacts/stage3_artifacts/pairs/pairs.jsonl'\n",
    "ls -lh /content/ml-learning/artifacts/stage3_artifacts/pairs/pairs.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DPO starting from Stage 2 adapters\n",
    "%%bash\n",
    "set -e\n",
    "cd \"$REPO_DIR\"\n",
    "mkdir -p \"$STAGE3_DIR\"\n",
    "uv run python \"$REPO_DIR/critique-revision-system/src/training/train_dpo_stage3.py\" \\\n",
    "  --repo-root \"$REPO_DIR\" \\\n",
    "  --pairs-path \"$PAIRS_PATH\" \\\n",
    "  --base-model-id \"$BASE_MODEL_ID\" \\\n",
    "  --stage2-adapter-path \"$STAGE2_ADAPTER_DIR\" \\\n",
    "  --output-dir \"$STAGE3_DIR\" \\\n",
    "  --per-device-train-batch-size 1 \\\n",
    "  --gradient-accumulation-steps 8 \\\n",
    "  --learning-rate 5e-5 \\\n",
    "  --num-train-epochs 1 \\\n",
    "  --beta 0.1 \\\n",
    "  --cpu-ref-model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation: load Stage 3 adapters and generate a few outputs\n",
    "import json, os, random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "rows = []\n",
    "with open(PAIRS_PATH, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        rows.append(json.loads(line))\n",
    "random.seed(42)\n",
    "sample = rows[:12] if len(rows) <= 12 else random.sample(rows, 12)\n",
    "\n",
    "def extract_prompt(ex):\n",
    "    for k in ['prompt','instruction','input','user_prompt']:\n",
    "        if k in ex and ex[k]:\n",
    "            return f\"User: {ex[k]}\\nAssistant:\"\n",
    "    raise KeyError('No prompt-like key found')\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=True)\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map='auto' if torch.cuda.is_available() else None,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base, STAGE3_LORA_DIR)\n",
    "model.eval()\n",
    "gen_conf = dict(max_new_tokens=256, do_sample=True, top_p=0.9, temperature=0.7)\n",
    "\n",
    "EVAL_OUT = f'{STAGE3_DIR}/eval/generated_eval.jsonl'\n",
    "Path(f'{STAGE3_DIR}/eval').mkdir(parents=True, exist_ok=True)\n",
    "with open(EVAL_OUT, 'w', encoding='utf-8') as out_f:\n",
    "    for ex in sample:\n",
    "        prompt = extract_prompt(ex)\n",
    "        inputs = tok(prompt, return_tensors='pt').to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, **gen_conf)\n",
    "        text = tok.decode(outputs[0], skip_special_tokens=True)\n",
    "        out_f.write(json.dumps({'prompt': prompt, 'generated': text}, ensure_ascii=False) + '\\n')\n",
    "print('Wrote eval generations to:', EVAL_OUT)\n",
    "\n",
    "print('Sample generations:')\n",
    "with open(EVAL_OUT, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 3: break\n",
    "        print(json.loads(line)['generated'][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt safety scoring via Stage 1 SafetyFilter (CPU JAX) if checkpoint is available\n",
    "try:\n",
    "    import sys\n",
    "    sys.path.insert(0, f'{REPO_DIR}/helpful-finetuning/src')\n",
    "    from utils.safety_integration import SafetyFilter\n",
    "    ckpt = os.environ.get('STAGE1_CKPT_DIR')\n",
    "    cfg_path = f'{REPO_DIR}/safety-text-classifier/configs/base_config.yaml'\n",
    "    if ckpt and os.path.isdir(ckpt):\n",
    "        sf = SafetyFilter(cfg_path, ckpt)\n",
    "        # Score generated eval set\n",
    "        import json\n",
    "        scores = []\n",
    "        with open(f'{STAGE3_DIR}/eval/generated_eval.jsonl','r',encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                txt = obj.get('generated','')\n",
    "                try:\n",
    "                    s = float(sf.score_text(txt))\n",
    "                    scores.append(s)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if scores:\n",
    "            print('Safety scores count:', len(scores), 'avg:', sum(scores)/len(scores))\n",
    "        else:\n",
    "            print('No safety scores computed (empty or errors).')\n",
    "    else:\n",
    "        print('Stage 1 checkpoint not found; skipping safety scoring.')\n",
    "except Exception as e:\n",
    "    print('Safety scoring unavailable or failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifact summary\n",
    "from pathlib import Path\n",
    "import json\n",
    "print('Pairs:', PAIRS_PATH, 'exists:', Path(PAIRS_PATH).exists())\n",
    "print('DPO adapters:', STAGE3_LORA_DIR, 'exists:', Path(STAGE3_LORA_DIR).exists())\n",
    "m = Path(f'{STAGE3_DIR}/metrics.json')\n",
    "if m.exists():\n",
    "    print('Metrics:')\n",
    "    print(json.dumps(json.load(open(m,'r')), indent=2))\n",
    "else:\n",
    "    print('Metrics file not found.')\n",
    "e = Path(f'{STAGE3_DIR}/eval/generated_eval.jsonl')\n",
    "print('Eval generations:', str(e), 'exists:', e.exists())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
