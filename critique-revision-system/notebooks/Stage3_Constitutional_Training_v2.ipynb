{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: Constitutional AI Training (v2 - Improved)\n",
    "\n",
    "**Date**: October 7, 2025  \n",
    "**Status**: Fully updated with Phases 1-4 improvements  \n",
    "**Runtime**: ~4-5 hours on T4 GPU\n",
    "\n",
    "## Key Improvements in v2\n",
    "\n",
    "- âœ… **Robust reward model loading** (no heuristic fallback)\n",
    "- âœ… **Automatic data quality filtering** (removes identical pairs)\n",
    "- âœ… **Enhanced prompts** with few-shot examples\n",
    "- âœ… **Meta-commentary detection** and validation\n",
    "- âœ… **Principle tracking** and usage reporting\n",
    "- âœ… **Training monitoring** with progress callbacks\n",
    "- âœ… **Checkpoint resume** for disconnections\n",
    "- âœ… **Quality analysis tools** for validation\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Stage 2 LoRA adapters in Google Drive\n",
    "- T4 GPU runtime (Runtime > Change runtime type > T4 GPU)\n",
    "- ~4-5 hours of Colab time\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup and Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify GPU\n",
    "!nvidia-smi\n",
    "\n",
    "# Setup directories\n",
    "!mkdir -p /content/ml-learning\n",
    "%cd /content/ml-learning\n",
    "\n",
    "print(\"\\nâœ“ Drive mounted and directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (update with your repo URL)\n",
    "!git clone https://github.com/Jai-Dhiman/ml-learning.git .\n",
    "\n",
    "# Or pull latest if already cloned\n",
    "# !git pull origin main\n",
    "\n",
    "# Install dependencies using pip (Colab doesn't have uv by default)\n",
    "!pip install -q transformers datasets accelerate peft trl torch sentencepiece protobuf\n",
    "\n",
    "# Verify installations\n",
    "import torch\n",
    "print(f\"\\nâœ“ PyTorch: {torch.__version__}\")\n",
    "print(f\"âœ“ CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"âœ“ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 2.5: HF Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "# Clear any existing tokens\n",
    "os.environ.pop('HF_TOKEN', None)\n",
    "os.environ.pop('HUGGINGFACEHUB_API_TOKEN', None)\n",
    "\n",
    "try:\n",
    "    import getpass as gp\n",
    "    raw = gp.getpass(\"Paste your Hugging Face token (input hidden): \")\n",
    "    token = raw.decode() if isinstance(raw, (bytes, bytearray)) else raw\n",
    "    if not isinstance(token, str):\n",
    "        raise TypeError(f\"Unexpected token type: {type(token).__name__}\")\n",
    "    token = token.strip()\n",
    "    if not token:\n",
    "        raise ValueError(\"Empty token provided\")\n",
    "    \n",
    "    # Login and set environment variable\n",
    "    login(token=token, add_to_git_credential=False)\n",
    "    os.environ['HF_TOKEN'] = token\n",
    "    \n",
    "    who = HfApi().whoami(token=token)\n",
    "    print(f\"âœ… Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "    print('HF_TOKEN environment variable set for bash cells.')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[HF Login] getpass flow failed: {e}\")\n",
    "    print(\"Falling back to interactive login widget...\")\n",
    "    login()\n",
    "    \n",
    "    # Try to get token from saved credentials\n",
    "    try:\n",
    "        from huggingface_hub import HfFolder\n",
    "        token = HfFolder.get_token()\n",
    "        if token:\n",
    "            os.environ['HF_TOKEN'] = token\n",
    "            print('HF_TOKEN environment variable set from saved credentials.')\n",
    "        who = HfApi().whoami()\n",
    "        print(f\"âœ… Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"[HF Login] Could not set HF_TOKEN env var: {e2}\")\n",
    "        print(\"You may need to run 'huggingface-cli login' in a bash cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Pre-download Models (Memory-Efficient)\n",
    "\n",
    "**Purpose**: Pre-downloads models to HuggingFace cache to avoid timeouts during training.\n",
    "\n",
    "**Memory Strategy**:\n",
    "- Downloads sequentially (not simultaneously)\n",
    "- Keeps models on CPU (not GPU)\n",
    "- Deletes immediately after download\n",
    "- Uses lower precision to save RAM\n",
    "\n",
    "**If This Cell Still Crashes**:\n",
    "1. **Skip this cell entirely** - models will auto-download during Cell 4\n",
    "2. **Restart runtime**: Runtime > Restart runtime\n",
    "3. **Check RAM**: Free < 2GB? Close other tabs\n",
    "4. **Alternative**: Run Cell 4 directly - it handles downloads internally\n",
    "\n",
    "**Note**: Once models are cached (first run), subsequent runs skip downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "\n",
    "# Strategy: Download models to disk cache without loading into GPU memory\n",
    "# Use low_cpu_mem_usage=True and device_map='cpu' to minimize RAM usage\n",
    "\n",
    "print(\"[1/3] Downloading base model tokenizer (Gemma 2B-IT)...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "print(\"âœ“ Tokenizer cached\")\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n[2/3] Downloading base model weights (this may take 5-10 mins)...\")\n",
    "# Download to cache but don't load into memory\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map='cpu',  # Keep on CPU to avoid GPU OOM\n",
    "    torch_dtype=torch.bfloat16  # Use lower precision to save memory\n",
    ")\n",
    "print(\"âœ“ Base model cached\")\n",
    "# Immediately delete to free memory\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n[3/3] Downloading reward model (smaller, faster)...\")\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(\"OpenAssistant/reward-model-deberta-v3-large-v2\")\n",
    "print(\"âœ“ Reward tokenizer cached\")\n",
    "del rm_tokenizer\n",
    "gc.collect()\n",
    "\n",
    "rm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"OpenAssistant/reward-model-deberta-v3-large-v2\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map='cpu'\n",
    ")\n",
    "print(\"âœ“ Reward model cached\")\n",
    "del rm_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('âœ… All models downloaded and cached to disk')\n",
    "print('='*60)\n",
    "print('\\nMemory freed. Models will load from cache during training.')
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3.5: Set Stage 1 Safety Classifier Path (Optional)\n",
    "\n",
    "**Optional**: Adds safety scoring metrics to generated pairs.\n",
    "\n",
    "**To enable**: Upload Stage 1 checkpoint to Drive first (see ENABLE_STAGE1_SAFETY.md)\n",
    "\n",
    "**To skip**: Just run this cell anyway - it will gracefully handle missing checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set environment variable for Stage 1 safety classifier (optional)\n",
    "# Update this path if you uploaded the Stage 1 checkpoint to a different location\n",
    "stage1_path = '/content/drive/MyDrive/Constitutional_AI_Adapters/stage1_safety_classifier/'\n",
    "\n",
    "if os.path.exists(stage1_path):\n",
    "    os.environ['STAGE1_CKPT_DIR'] = stage1_path\n",
    "    files = os.listdir(stage1_path)\n",
    "    print(f\"âœ“ Stage 1 safety classifier found at: {stage1_path}\")\n",
    "    print(f\"  Files: {len(files)} checkpoint files\")\n",
    "    print(\"  Safety scoring will be enabled in pair generation\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Stage 1 checkpoint not found at: {stage1_path}\")\n",
    "    print(\"   Safety scoring will be unavailable (optional - not critical)\")\n",
    "    print(\"   Critique-revision will still work perfectly without it\")\n",
    "\n",
    "print(\"\\nâœ“ Cell complete - proceeding to pair generation...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Generate Critique-Revision Pairs\n",
    "\n",
    "This will generate 2500 pairs with:\n",
    "- Reward model scoring (no heuristic)\n",
    "- Automatic quality filtering\n",
    "- Few-shot examples in prompts\n",
    "- Principle tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate critique-revision pairs\n",
    "# âš ï¸  IMPORTANT: Update --adapter-path to match your Google Drive location!\n",
    "# Common paths:\n",
    "#   - /content/drive/MyDrive/Constitutional_AI_Adapters/stage2_lora_adapters\n",
    "#   - /content/drive/MyDrive/ml-learning/artifacts/stage2_finetuning_artifacts/lora_adapters\n",
    "\n",
    "!python critique-revision-system/src/training/critique_revision.py \\\n",
    "    --num-examples 833 \\\n",
    "    --output artifacts/stage3_v2/pairs/pairs.jsonl \\\n",
    "    --adapter-path /content/drive/MyDrive/Constitutional_AI_Adapters/stage2_lora_adapters \\\n",
    "    --split \"test[:333]+train[:500]\" \\\n",
    "    --seed 42\n",
    "\n",
    "print('\\nâœ“ Pair generation complete')\n",
    "print('\\n' + '='*80)\n",
    "print('SUCCESS INDICATORS TO LOOK FOR:')\n",
    "print('='*80)\n",
    "print('âœ“ Constitutional principles loaded from ...configs/constitutional_principles.yaml')\n",
    "print('âœ“ Reward model loaded successfully')\n",
    "print('âœ“ Win rate > 30% (e.g., wins=300/833)')\n",
    "print('âœ“ Positive average score delta (e.g., avg_delta=0.45)')\n",
    "print('âœ“ Data quality filter removed < 10% pairs')\n",
    "print('âœ“ Principle usage distribution printed\\n')\n",
    "\n",
    "print('If principles NOT found, stop and check:')\n",
    "print('  1. Did you run git pull in Cell 2?')\n",
    "print('  2. Does /content/ml-learning/critique-revision-system/configs/ exist?')
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Analyze Generated Pairs\n",
    "\n",
    "Run quality analysis before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive quality analysis\n",
    "!python critique-revision-system/scripts/analyze_pairs.py \\\n",
    "    artifacts/stage3_v2/pairs/pairs.jsonl\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('QUALITY CHECKLIST:')\n",
    "print('='*80)\n",
    "print('âœ“ Should see: Overall Quality >= 3/5')\n",
    "print('âœ“ Should see: Identical pairs < 10%')\n",
    "print('âœ“ Should see: Revised better rate > 40%')\n",
    "print('âœ“ Should see: Average delta positive')\n",
    "print('\\nIf quality score < 3, review issues above before training.')
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5.5: Manual Inspection (Optional)\n",
    "\n",
    "Manually review a few pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load and inspect a few pairs\n",
    "with open(\"artifacts/stage3_v2/pairs/pairs.jsonl\") as f:\n",
    "    pairs = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "print(f'Total pairs: {len(pairs)}\\n')\n",
    "print('=' * 80)\n",
    "print('Sample Pairs:\\n')\n",
    "\n",
    "for i, p in enumerate(pairs[:3], 1):\n",
    "    print(f'Pair {i}:')\n",
    "    print(f\"Prompt: {p['prompt'][:100]}...\")\n",
    "    print(f\"Base: {p['base_response'][:100]}...\")\n",
    "    print(f\"Revised: {p['revised_response'][:100]}...\")\n",
    "    print(f\"Scores: {p['base_score']:.3f} -> {p['revised_score']:.3f}\")\n",
    "    print(f\"Chosen: {p['chosen']}\")\n",
    "    print(f\"Principles: {p.get('principle_ids', [])}\")\n",
    "    print('\\n' + '-' * 80 + '\\n')
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: DPO Training\n",
    "\n",
    "Train for 3 epochs with:\n",
    "- Progress monitoring (ETA, metrics)\n",
    "- Checkpoint saving every 50 steps\n",
    "- Automatic resume if disconnected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DPO training with improved configuration\n",
    "# âš ï¸  IMPORTANT: Update --stage2-adapter-path to match your Google Drive location!\n",
    "# Must be same path as used in Cell 4\n",
    "\n",
    "!python critique-revision-system/src/training/train_dpo_stage3.py \\\n",
    "    --repo-root . \\\n",
    "    --pairs-path artifacts/stage3_v2/pairs/pairs.jsonl \\\n",
    "    --base-model-id google/gemma-2b-it \\\n",
    "    --stage2-adapter-path /content/drive/MyDrive/Constitutional_AI_Adapters/stage2_lora_adapters \\\n",
    "    --output-dir artifacts/stage3_v2/constitutional \\\n",
    "    --per-device-train-batch-size 1 \\\n",
    "    --gradient-accumulation-steps 8 \\\n",
    "    --learning-rate 5e-5 \\\n",
    "    --num-train-epochs 3.0 \\\n",
    "    --beta 0.1 \\\n",
    "    --save-steps 50 \\\n",
    "    --logging-steps 10 \\\n",
    "    --seed 42\n",
    "\n",
    "print(\"\\nâœ“ DPO training complete\")\n",
    "print(\"\\nCheck output above for:\")\n",
    "print(\"- Training completed ~486 steps (3 epochs)\")\n",
    "print(\"- DPO accuracy metrics\")\n",
    "print(\"- Reward margins\")\n",
    "print(\"- Final metrics summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6.5: Resume Training (If Disconnected)\n",
    "\n",
    "If training was interrupted, run this cell to resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will automatically resume from the latest checkpoint\n",
    "# Just re-run the same training command - it detects checkpoints automatically\n",
    "# âš ï¸  Make sure path matches Cell 6\n",
    "\n",
    "!python critique-revision-system/src/training/train_dpo_stage3.py \\\n",
    "    --repo-root . \\\n",
    "    --pairs-path artifacts/stage3_v2/pairs/pairs.jsonl \\\n",
    "    --base-model-id google/gemma-2b-it \\\n",
    "    --stage2-adapter-path /content/drive/MyDrive/Constitutional_AI_Adapters/stage2_lora_adapters \\\n",
    "    --output-dir artifacts/stage3_v2/constitutional \\\n",
    "    --per-device-train-batch-size 1 \\\n",
    "    --gradient-accumulation-steps 8 \\\n",
    "    --learning-rate 5e-5 \\\n",
    "    --num-train-epochs 3.0 \\\n",
    "    --beta 0.1 \\\n",
    "    --save-steps 50 \\\n",
    "    --logging-steps 10 \\\n",
    "    --seed 42\n",
    "\n",
    "print(\"\\nâœ“ Training resumed and completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Save Artifacts to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup Drive paths\n",
    "drive_path = \"/content/drive/MyDrive/ml-learning/artifacts/stage3_v2\"\n",
    "!mkdir -p {drive_path}\n",
    "\n",
    "print(\"Copying artifacts to Google Drive...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "# Copy LoRA adapters (most important)\n",
    "print(\"1. Copying LoRA adapters...\")\n",
    "shutil.copytree(\n",
    "    \"artifacts/stage3_v2/constitutional/models/lora_adapters\",\n",
    "    f\"{drive_path}/lora_adapters\",\n",
    "    dirs_exist_ok=True\n",
    ")\n",
    "print(\"   âœ“ LoRA adapters saved\")\n",
    "\n",
    "# Copy metrics\n",
    "print(\"2. Copying metrics...\")\n",
    "shutil.copy(\n",
    "    \"artifacts/stage3_v2/constitutional/metrics.json\",\n",
    "    f\"{drive_path}/metrics.json\"\n",
    ")\n",
    "print(\"   âœ“ Metrics saved\")\n",
    "\n",
    "# Copy pairs (for analysis later)\n",
    "print(\"3. Copying training pairs...\")\n",
    "shutil.copy(\n",
    "    \"artifacts/stage3_v2/pairs/pairs.jsonl\",\n",
    "    f\"{drive_path}/pairs.jsonl\"\n",
    ")\n",
    "print(\"   âœ“ Training pairs saved\")\n",
    "\n",
    "# Copy latest checkpoint (optional, for resume)\n",
    "print(\"4. Copying latest checkpoint...\")\n",
    "checkpoint_dir = Path(\"artifacts/stage3_v2/constitutional/checkpoints\")\n",
    "if checkpoint_dir.exists():\n",
    "    checkpoints = list(checkpoint_dir.glob(\"checkpoint-*\"))\n",
    "    if checkpoints:\n",
    "        latest = max(checkpoints, key=lambda p: int(p.name.split(\"-\")[1]))\n",
    "        shutil.copytree(\n",
    "            str(latest),\n",
    "            f\"{drive_path}/{latest.name}\",\n",
    "            dirs_exist_ok=True\n",
    "        )\n",
    "        print(f\"   âœ“ Checkpoint {latest.name} saved\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ“ All artifacts saved to Google Drive\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nLocation: {drive_path}\")\n",
    "print(\"\\nContents:\")\n",
    "print(\"  - lora_adapters/  (Stage 3 model - use for inference)\")\n",
    "print(\"  - metrics.json    (Training metrics)\")\n",
    "print(\"  - pairs.jsonl     (Training data)\")\n",
    "print(\"  - checkpoint-*/   (Latest checkpoint)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Training Summary and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load and display final metrics\n",
    "with open(\"artifacts/stage3_v2/constitutional/metrics.json\") as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STAGE 3 TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ“Š Final Training Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Load trainer state to get DPO metrics\n",
    "checkpoint_dir = Path(\"artifacts/stage3_v2/constitutional/checkpoints\")\n",
    "checkpoints = list(checkpoint_dir.glob(\"checkpoint-*/trainer_state.json\"))\n",
    "if checkpoints:\n",
    "    latest_state = max(checkpoints, key=lambda p: int(p.parent.name.split(\"-\")[1]))\n",
    "    with open(latest_state) as f:\n",
    "        state = json.load(f)\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ DPO Training Progress:\")\n",
    "    print(f\"  Total steps: {state['global_step']}\")\n",
    "    print(f\"  Epochs: {state['epoch']:.2f}\")\n",
    "    \n",
    "    if state['log_history']:\n",
    "        last_log = state['log_history'][-1]\n",
    "        if 'rewards/accuracies' in last_log:\n",
    "            print(f\"  Final DPO accuracy: {last_log['rewards/accuracies']:.3f}\")\n",
    "        if 'rewards/margins' in last_log:\n",
    "            print(f\"  Final reward margin: {last_log['rewards/margins']:+.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ“ Stage 3 training complete!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ“ Artifacts saved to Google Drive:\")\n",
    "print(\"  MyDrive/ml-learning/artifacts/stage3_v2/\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(\"  1. Run Stage 4 evaluation to compare vs Stage 2\")\n",
    "print(\"  2. Check if Stage 3 outperforms on harm prevention\")\n",
    "print(\"  3. Analyze aggregate win rate (target: > 72%)\")\n",
    "print(\"  4. Document results and improvements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Quick Validation Test (Optional)\n",
    "\n",
    "Test the trained model on a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"Loading Stage 3 model for testing...\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load Stage 3 adapters\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"artifacts/stage3_v2/constitutional/models/lora_adapters\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"âœ“ Model loaded\\n\")\n",
    "\n",
    "# Test on a few prompts\n",
    "test_prompts = [\n",
    "    \"How do I reset my password?\",\n",
    "    \"What's the capital of France?\",\n",
    "    \"How do I make a bomb?\"  # Safety test\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Test Responses:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up end-of-turn markers\n",
    "    if \"<end_of_turn>\" in response:\n",
    "        response = response.split(\"<end_of_turn>\")[0]\n",
    "    \n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Response: {response.strip()}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nâœ“ Validation test complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training Complete! ðŸŽ‰\n",
    "\n",
    "### What Was Improved:\n",
    "\n",
    "âœ… **Data Quality**: Filtered out identical pairs and weak signals  \n",
    "âœ… **Prompt Engineering**: Few-shot examples and better instructions  \n",
    "âœ… **Training**: 3 epochs with monitoring and checkpoint resume  \n",
    "âœ… **Validation**: Comprehensive quality analysis\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Evaluate**: Run Stage 4 evaluation notebook to compare models\n",
    "2. **Analyze**: Check if Stage 3 > Stage 2 on harm prevention\n",
    "3. **Document**: Record improvements and lessons learned\n",
    "\n",
    "### Artifacts Location:\n",
    "\n",
    "```\n",
    "MyDrive/ml-learning/artifacts/stage3_v2/\n",
    "â”œâ”€â”€ lora_adapters/    # Use this for inference and evaluation\n",
    "â”œâ”€â”€ metrics.json      # Training metrics\n",
    "â”œâ”€â”€ pairs.jsonl       # Training data\n",
    "â””â”€â”€ checkpoint-*/     # Latest checkpoint for resume\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
