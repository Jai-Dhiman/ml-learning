{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constitutional AI Evaluation - Google Colab\n",
    "\n",
    "**Purpose**: Run comprehensive evaluation of all 3 models (Base, Stage 2, Stage 3) on Constitutional AI test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\" Change runtime type to GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Clone Repository\n",
    "\n",
    "**Note**: Update the GitHub URL once you push your repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change this to your GitHub repository URL\n",
    "REPO_URL = \"https://github.com/Jai-Dhiman/ml-learning.git\"\n",
    "\n",
    "# Clone repository\n",
    "if not os.path.exists('ml-learning'):\n",
    "    !git clone {REPO_URL}\n",
    "else:\n",
    "    print(\"‚úì Repository already cloned\")\n",
    "\n",
    "# Change to project directory\n",
    "%cd ml-learning/constitutional-ai-stage4\n",
    "\n",
    "# Verify structure\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers peft datasets accelerate sentencepiece protobuf\n",
    "\n",
    "print(\"‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Optional - Mount Google Drive\n",
    "\n",
    "Mount Google Drive to automatically save results. Skip this cell if you want to download results manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create results directory in Google Drive\n",
    "DRIVE_RESULTS_DIR = '/content/drive/MyDrive/constitutional_ai_evaluation_results'\n",
    "!mkdir -p {DRIVE_RESULTS_DIR}\n",
    "\n",
    "print(f\"‚úì Google Drive mounted. Results will be saved to: {DRIVE_RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Validate Setup\n",
    "\n",
    "Check that all artifacts and models are accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validation script\n",
    "!python3 src/inference/validate_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Run Quick Test (Optional)\n",
    "\n",
    "Test the evaluation pipeline with 5 prompts to ensure everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with 5 prompts\n",
    "!python3 src/evaluation/evaluation_runner.py \\\n",
    "  --models stage3_constitutional \\\n",
    "  --max-prompts 5\n",
    "\n",
    "print(\"\\n‚úì Quick test complete! If this worked, proceed to full evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Run Full Evaluation (‚è∞ 4-6 hours)\n",
    "\n",
    "**This will take 4-6 hours on T4 GPU, 2-3 hours on L4 GPU**\n",
    "\n",
    "The evaluation will:\n",
    "1. Load all 3 models (Base, Stage 2, Stage 3)\n",
    "2. Generate responses for 50 test prompts\n",
    "3. Evaluate each response on 4 constitutional principles\n",
    "4. Save results to JSON and CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "print(f\"Starting evaluation at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nThis will take approximately 4-6 hours on T4 GPU...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run full evaluation\n",
    "!python3 src/evaluation/evaluation_runner.py \\\n",
    "  --models base stage2_helpful stage3_constitutional \\\n",
    "  --max-prompts 50 \\\n",
    "  --output-dir artifacts/evaluation/final_results \\\n",
    "  --save-csv --save-json\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "duration = (end_time - start_time) / 3600  # Convert to hours\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n‚úì Evaluation complete!\")\n",
    "print(f\"Duration: {duration:.2f} hours\")\n",
    "print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Display Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load JSON results\n",
    "with open('artifacts/evaluation/final_results/results.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EVALUATION RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display aggregate scores\n",
    "if 'comparison_summary' in results:\n",
    "    print(\"\\nAggregate Scores by Model:\")\n",
    "    for model, scores in results['comparison_summary'].get('aggregate_scores', {}).items():\n",
    "        print(f\"  {model}: {scores:.4f}\")\n",
    "\n",
    "# Load and display CSV\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Detailed Comparison Table:\")\n",
    "print(\"=\" * 70)\n",
    "df = pd.read_csv('artifacts/evaluation/final_results/comparison.csv')\n",
    "print(df.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Results saved to:\")\n",
    "print(\"  - artifacts/evaluation/final_results/results.json\")\n",
    "print(\"  - artifacts/evaluation/final_results/comparison.csv\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Copy Results to Google Drive (if mounted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if Google Drive is mounted\n",
    "if os.path.exists('/content/drive/MyDrive'):\n",
    "    DRIVE_RESULTS_DIR = '/content/drive/MyDrive/constitutional_ai_evaluation_results'\n",
    "    \n",
    "    # Copy results\n",
    "    !cp -r artifacts/evaluation/final_results/* {DRIVE_RESULTS_DIR}/\n",
    "    \n",
    "    print(f\"‚úì Results copied to Google Drive: {DRIVE_RESULTS_DIR}\")\n",
    "    print(\"  You can access these files from your Google Drive at any time!\")\n",
    "else:\n",
    "    print(\"Google Drive not mounted. Use Cell 10 to download results manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Download Results (Manual Download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create a zip file of all results\n",
    "!zip -r evaluation_results.zip artifacts/evaluation/final_results/\n",
    "\n",
    "print(\"Downloading results...\")\n",
    "files.download('evaluation_results.zip')\n",
    "\n",
    "print(\"\\n‚úì Results downloaded!\")\n",
    "print(\"\\nExtract the zip file locally to access:\")\n",
    "print(\"  - results.json (complete evaluation data)\")\n",
    "print(\"  - comparison.csv (model comparison table)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Generate Quick Visualizations (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load results\n",
    "df = pd.read_csv('artifacts/evaluation/final_results/comparison.csv')\n",
    "\n",
    "# Extract model names and scores (assumes specific column structure)\n",
    "# Adjust column names based on actual CSV structure\n",
    "models = df['model'].tolist() if 'model' in df.columns else ['Base', 'Stage 2', 'Stage 3']\n",
    "principles = ['harm_prevention', 'truthfulness', 'helpfulness', 'fairness']\n",
    "\n",
    "# Create radar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(principles), endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "for model in models:\n",
    "    # Extract scores for this model (placeholder logic)\n",
    "    # Adjust based on actual data structure\n",
    "    scores = [0.5, 0.6, 0.7, 0.65]  # Replace with actual scores from df\n",
    "    scores += scores[:1]\n",
    "    ax.plot(angles, scores, 'o-', linewidth=2, label=model)\n",
    "    ax.fill(angles, scores, alpha=0.25)\n",
    "\n",
    "ax.set_theta_offset(np.pi / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels([p.replace('_', ' ').title() for p in principles])\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel('Score', labelpad=30)\n",
    "ax.set_title('Constitutional Principle Scores by Model', size=16, pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('principle_comparison_radar.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n‚úì Radar chart saved to: principle_comparison_radar.png\")\n",
    "plt.show()\n",
    "\n",
    "# Download visualization\n",
    "files.download('principle_comparison_radar.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Evaluation Complete! üéâ**\n",
    "\n",
    "You now have:\n",
    "- Complete evaluation results (JSON)\n",
    "- Model comparison table (CSV)\n",
    "- Optional visualizations\n",
    "\n",
    "**Next Steps**:\n",
    "1. Review results in `results.json` and `comparison.csv`\n",
    "2. Create statistical analysis scripts (significance testing, effect sizes)\n",
    "3. Generate publication-quality figures\n",
    "4. Write the paper!\n",
    "\n",
    "See `RESEARCH_PUBLICATION_PLAN.md` for detailed next steps."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
