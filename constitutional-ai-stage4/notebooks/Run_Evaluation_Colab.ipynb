{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constitutional AI Evaluation - Google Colab\n",
    "\n",
    "**Purpose**: Run comprehensive evaluation of all 3 models (Base, Stage 2, Stage 3) on Constitutional AI test set.\n",
    "\n",
    "**Important**: This notebook requires LoRA adapters stored in Google Drive at:\n",
    "- `MyDrive/Constitutional_AI_Adapters/stage2_lora_adapters/`\n",
    "- `MyDrive/Constitutional_AI_Adapters/stage3_lora_adapters/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\" Change runtime type to GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf-login"
   },
   "outputs": [],
   "source": [
    "# Login to Hugging Face (required for Gemma model access)\n",
    "# Secure login without storing/printing your token.\n",
    "# Token will be set as HF_TOKEN environment variable for bash cells.\n",
    "import os\n",
    "\n",
    "# Clear any existing tokens\n",
    "os.environ.pop(\"HF_TOKEN\", None)\n",
    "os.environ.pop(\"HUGGINGFACEHUB_API_TOKEN\", None)\n",
    "\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "try:\n",
    "    import getpass as gp\n",
    "    raw = gp.getpass(\"Paste your Hugging Face token (input hidden): \")\n",
    "    token = raw.decode() if isinstance(raw, (bytes, bytearray)) else raw\n",
    "    if not isinstance(token, str):\n",
    "        raise TypeError(f\"Unexpected token type: {type(token).__name__}\")\n",
    "    token = token.strip()\n",
    "    if not token:\n",
    "        raise ValueError(\"Empty token provided\")\n",
    "    \n",
    "    # Login and set environment variable\n",
    "    login(token=token, add_to_git_credential=False)\n",
    "    os.environ[\"HF_TOKEN\"] = token\n",
    "    \n",
    "    who = HfApi().whoami(token=token)\n",
    "    print(f\"Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "    print(\"HF_TOKEN environment variable set for bash cells.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[HF Login] getpass flow failed: {e}\")\n",
    "    print(\"Falling back to interactive login widget...\")\n",
    "    login()\n",
    "    \n",
    "    # Try to get token from saved credentials\n",
    "    try:\n",
    "        from huggingface_hub import HfFolder\n",
    "        token = HfFolder.get_token()\n",
    "        if token:\n",
    "            os.environ[\"HF_TOKEN\"] = token\n",
    "            print(\"HF_TOKEN environment variable set from saved credentials.\")\n",
    "        who = HfApi().whoami()\n",
    "        print(f\"Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"[HF Login] Could not set HF_TOKEN env var: {e2}\")\n",
    "        print(\"You may need to run 'huggingface-cli login' in a bash cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change this to your GitHub repository URL\n",
    "REPO_URL = \"https://github.com/Jai-Dhiman/ml-learning.git\"\n",
    "\n",
    "# Clone repository\n",
    "if not os.path.exists('ml-learning'):\n",
    "    !git clone {REPO_URL}\n",
    "else:\n",
    "    print(\"\u2713 Repository already cloned\")\n",
    "\n",
    "# Change to project directory\n",
    "%cd ml-learning/constitutional-ai-stage4\n",
    "\n",
    "# Verify structure\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Install Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers peft datasets accelerate sentencepiece protobuf\n",
    "\n",
    "print(\"\u2713 Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Mount Google Drive\n",
    "\n",
    "**Required**: Mount Drive to access LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\n\u2713 Google Drive mounted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define adapter paths in Google Drive\n",
    "STAGE2_ADAPTER_PATH = \"/content/drive/MyDrive/Constitutional_AI_Adapters/stage2_lora_adapters\"\n",
    "STAGE3_ADAPTER_PATH = \"/content/drive/MyDrive/Constitutional_AI_Adapters/stage3_lora_adapters\"\n",
    "\n",
    "print(f\"Stage 2 adapters: {STAGE2_ADAPTER_PATH}\")\n",
    "print(f\"Stage 3 adapters: {STAGE3_ADAPTER_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure Adapter Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define adapter paths in Google Drive\n",
    "STAGE2_ADAPTER_PATH = \"/content/drive/MyDrive/Constitutional_AI_Adapters/stage2_lora_adapters\"\n",
    "STAGE3_ADAPTER_PATH = \"/content/drive/MyDrive/Constitutional_AI_Adapters/stage3_lora_adapters\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ADAPTER PATH VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def validate_adapter_path(path, stage_name):\n",
    "    \"\"\"Validate that adapter directory exists and contains required files.\"\"\"\n",
    "    print(f\"\\n{stage_name}:\")\n",
    "    print(f\"  Path: {path}\")\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"  \u2717 ERROR: Directory not found!\")\n",
    "        print(f\"\\n  Please ensure your adapters are uploaded to:\")\n",
    "        print(f\"  {path}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"  \u2713 Directory exists\")\n",
    "    \n",
    "    # Check required files\n",
    "    required_files = ['adapter_config.json', 'adapter_model.safetensors']\n",
    "    all_found = True\n",
    "    \n",
    "    for file_name in required_files:\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"  \u2713 {file_name} ({size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\"  \u2717 Missing: {file_name}\")\n",
    "            all_found = False\n",
    "    \n",
    "    return all_found\n",
    "\n",
    "# Validate both adapter paths\n",
    "stage2_valid = validate_adapter_path(STAGE2_ADAPTER_PATH, \"Stage 2 Adapters\")\n",
    "stage3_valid = validate_adapter_path(STAGE3_ADAPTER_PATH, \"Stage 3 Adapters\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if stage2_valid and stage3_valid:\n",
    "    print(\"\u2713 ALL ADAPTERS VALIDATED SUCCESSFULLY\")\n",
    "    print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"\u2717 VALIDATION FAILED\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nPlease fix the issues above before proceeding.\")\n",
    "    print(\"\\nYour adapters should be organized as:\")\n",
    "    print(\"  MyDrive/\")\n",
    "    print(\"    \u2514\u2500\u2500 Constitutional_AI_Adapters/\")\n",
    "    print(\"        \u251c\u2500\u2500 stage2_lora_adapters/\")\n",
    "    print(\"        \u2502   \u251c\u2500\u2500 adapter_config.json\")\n",
    "    print(\"        \u2502   \u2514\u2500\u2500 adapter_model.safetensors\")\n",
    "    print(\"        \u2514\u2500\u2500 stage3_lora_adapters/\")\n",
    "    print(\"            \u251c\u2500\u2500 adapter_config.json\")\n",
    "    print(\"            \u2514\u2500\u2500 adapter_model.safetensors\")\n",
    "    raise RuntimeError(\"Adapter validation failed. Please upload your adapters to Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Validate Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validation script with custom adapter paths\n",
    "!python3 src/inference/validate_setup.py \\\n",
    "  --stage2-adapter-path \"{STAGE2_ADAPTER_PATH}\" \\\n",
    "  --stage3-adapter-path \"{STAGE3_ADAPTER_PATH}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Quick Test (Optional)\n",
    "\n",
    "Test with 5 prompts (2-3 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test with 5 prompts\n!python3 src/evaluation/evaluation_runner.py \\\n  --test-file artifacts/evaluation/extended_test_prompts.jsonl \\\n  --models stage3_constitutional \\\n  --max-prompts 5 \\\n  --stage2-adapter-path \"{STAGE2_ADAPTER_PATH}\" \\\n  --stage3-adapter-path \"{STAGE3_ADAPTER_PATH}\"\n  --stage2-adapter-path \"{STAGE2_ADAPTER_PATH}\" \\\n  --stage3-adapter-path \"{STAGE3_ADAPTER_PATH}\"\n\nprint(\"\\n\u2713 Quick test complete! If this worked, proceed to full evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Full Evaluation (6-8 hours)\n",
    "\n",
    "Evaluates all 3 models on 110 prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\nfrom datetime import datetime\n\n# Record start time\nstart_time = time.time()\nprint(f\"Starting evaluation at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(\"\\nThis will take approximately 6-8 hours on T4 GPU (110 prompts \u00d7 3 models)...\\n\")\nprint(\"=\" * 70)\n\n# Run full evaluation with extended test set (110 prompts)\n!python3 src/evaluation/evaluation_runner.py \\\n  --test-file artifacts/evaluation/extended_test_prompts.jsonl \\\n  --models base stage2_helpful stage3_constitutional \\\n  --max-prompts 110 \\\n  --stage2-adapter-path \"{STAGE2_ADAPTER_PATH}\" \\\n  --stage3-adapter-path \"{STAGE3_ADAPTER_PATH}\"\n  --output-dir artifacts/evaluation/final_results \\\n  --stage2-adapter-path \"{STAGE2_ADAPTER_PATH}\" \\\n  --stage3-adapter-path \"{STAGE3_ADAPTER_PATH}\"\n\n# Record end time\nend_time = time.time()\nduration = (end_time - start_time) / 3600  # Convert to hours\n\nprint(\"=\" * 70)\nprint(f\"\\n\u2713 Evaluation complete!\")\nprint(f\"Duration: {duration:.2f} hours\")\nprint(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if results exist\n",
    "results_file = Path('artifacts/evaluation/final_results')\n",
    "\n",
    "if not results_file.exists():\n",
    "    print(\"\u26a0\ufe0f  Results directory not found.\")\n",
    "    print(\"Make sure Cell 7 (Full Evaluation) completed successfully.\")\n",
    "    print(\"\\nThe evaluation saves results to: artifacts/evaluation/final_results/\")\n",
    "else:\n",
    "    # Find the most recent results file\n",
    "    result_files = sorted(results_file.glob('evaluation_results_*.json'))\n",
    "    \n",
    "    if not result_files:\n",
    "        print(\"\u26a0\ufe0f  No results files found in artifacts/evaluation/final_results/\")\n",
    "        print(\"\\nMake sure Cell 7 completed successfully.\")\n",
    "        print(\"\\nAvailable files:\")\n",
    "        !ls -lah artifacts/evaluation/final_results/\n",
    "    else:\n",
    "        # Load most recent results\n",
    "        latest_results = result_files[-1]\n",
    "        print(f\"Loading results from: {latest_results}\")\n",
    "        \n",
    "        with open(latest_results, 'r') as f:\n",
    "            results = json.load(f)\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(\"EVALUATION RESULTS SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Display aggregate scores\n",
    "        if 'comparison_summary' in results:\n",
    "            print(\"\\nAggregate Scores by Model:\")\n",
    "            for model, scores in results['comparison_summary'].get('aggregate_scores', {}).items():\n",
    "                print(f\"  {model}: {scores:.4f}\")\n",
    "        \n",
    "        # Load and display CSV (find most recent summary file)\n",
    "        summary_files = sorted(results_file.glob('evaluation_summary_*.csv'))\n",
    "        \n",
    "        if summary_files:\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"Detailed Comparison Table:\")\n",
    "            print(\"=\" * 70)\n",
    "            df = pd.read_csv(summary_files[-1])\n",
    "            print(df.to_string())\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"Results saved to:\")\n",
    "        print(f\"  - {latest_results}\")\n",
    "        if summary_files:\n",
    "            print(f\"  - {summary_files[-1]}\")\n",
    "        print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Backup to Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create results directory in Google Drive\n",
    "DRIVE_RESULTS_DIR = '/content/drive/MyDrive/constitutional_ai_evaluation_results'\n",
    "!mkdir -p \"{DRIVE_RESULTS_DIR}\"\n",
    "\n",
    "# Copy results\n",
    "!cp -r artifacts/evaluation/final_results/* \"{DRIVE_RESULTS_DIR}\"/\n",
    "\n",
    "print(f\"\u2713 Results copied to Google Drive: {DRIVE_RESULTS_DIR}\")\n",
    "print(\"  You can access these files from your Google Drive at any time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Download (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create a zip file of all results\n",
    "!zip -r evaluation_results.zip artifacts/evaluation/final_results/\n",
    "\n",
    "print(\"Downloading results...\")\n",
    "files.download('evaluation_results.zip')\n",
    "\n",
    "print(\"\\n\u2713 Results downloaded!\")\n",
    "print(\"\\nExtract the zip file locally to access:\")\n",
    "print(\"  - results.json (complete evaluation data)\")\n",
    "print(\"  - comparison.csv (model comparison table)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load results\n",
    "df = pd.read_csv('artifacts/evaluation/final_results/comparison.csv')\n",
    "\n",
    "# Extract model names and scores (assumes specific column structure)\n",
    "# Adjust column names based on actual CSV structure\n",
    "models = df['model'].tolist() if 'model' in df.columns else ['Base', 'Stage 2', 'Stage 3']\n",
    "principles = ['harm_prevention', 'truthfulness', 'helpfulness', 'fairness']\n",
    "\n",
    "# Create radar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(principles), endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "for model in models:\n",
    "    # Extract scores for this model (placeholder logic)\n",
    "    # Adjust based on actual data structure\n",
    "    scores = [0.5, 0.6, 0.7, 0.65]  # Replace with actual scores from df\n",
    "    scores += scores[:1]\n",
    "    ax.plot(angles, scores, 'o-', linewidth=2, label=model)\n",
    "    ax.fill(angles, scores, alpha=0.25)\n",
    "\n",
    "ax.set_theta_offset(np.pi / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels([p.replace('_', ' ').title() for p in principles])\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel('Score', labelpad=30)\n",
    "ax.set_title('Constitutional Principle Scores by Model', size=16, pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('principle_comparison_radar.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n\u2713 Radar chart saved to: principle_comparison_radar.png\")\n",
    "plt.show()\n",
    "\n",
    "# Download visualization\n",
    "files.download('principle_comparison_radar.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Evaluation Complete! \ud83c\udf89**\n",
    "\n",
    "You now have:\n",
    "- Complete evaluation results (JSON)\n",
    "- Model comparison table (CSV)\n",
    "- Optional visualizations\n",
    "\n",
    "**Next Steps**:\n",
    "1. Review results in `results.json` and `comparison.csv`\n",
    "2. Create statistical analysis scripts (significance testing, effect sizes)\n",
    "3. Generate publication-quality figures\n",
    "4. Write the paper!\n",
    "\n",
    "See `RESEARCH_PUBLICATION_PLAN.md` for detailed next steps."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}