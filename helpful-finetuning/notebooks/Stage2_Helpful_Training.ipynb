{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 2: Helpful Fine-Tuning\n",
        "\n",
        "QLoRA fine-tuning on Anthropic/hh-rlhf helpful subset.\n",
        "\n",
        "**Expected time**: ~2-3 hours on T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Setup\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content')\n",
        "\n",
        "!git clone https://github.com/Jai-Dhiman/ml-learning.git\n",
        "os.chdir('/content/ml-learning/helpful-finetuning')\n",
        "\n",
        "print(f\"✅ Ready in: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Install Dependencies\n",
        "!pip install -q transformers==4.36.2 datasets==2.16.1 peft==0.7.1 trl==0.7.10\n",
        "!pip install -q accelerate==0.25.0 bitsandbytes==0.41.3 torch==2.1.2 pyyaml\n",
        "\n",
        "print(\"✅ Dependencies installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: PREFLIGHT TEST (1 sample)\n",
        "import os\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PREFLIGHT: Testing with 1 sample\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!python src/training/train_qlora.py \\\n",
        "  --model-name google/gemma-2b-it \\\n",
        "  --dataset Anthropic/hh-rlhf \\\n",
        "  --dataset-split 'train[:1]' \\\n",
        "  --output-dir /tmp/preflight_stage2 \\\n",
        "  --lora-r 8 \\\n",
        "  --lora-alpha 16 \\\n",
        "  --batch-size 1 \\\n",
        "  --gradient-accumulation-steps 1 \\\n",
        "  --learning-rate 2e-4 \\\n",
        "  --max-steps 1 \\\n",
        "  --max-seq-length 512 \\\n",
        "  --logging-steps 1 \\\n",
        "  --save-steps 1000\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ PREFLIGHT PASSED - Ready for full training\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Full Training (10K samples, 3 epochs)\n",
        "!mkdir -p artifacts/stage2_helpful\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STAGE 2: HELPFUL FINE-TUNING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!python src/training/train_qlora.py \\\n",
        "  --model-name google/gemma-2b-it \\\n",
        "  --dataset Anthropic/hh-rlhf \\\n",
        "  --dataset-split 'train[:10000]' \\\n",
        "  --output-dir artifacts/stage2_helpful \\\n",
        "  --lora-r 16 \\\n",
        "  --lora-alpha 32 \\\n",
        "  --lora-dropout 0.05 \\\n",
        "  --batch-size 4 \\\n",
        "  --gradient-accumulation-steps 4 \\\n",
        "  --learning-rate 2e-4 \\\n",
        "  --num-epochs 3 \\\n",
        "  --max-seq-length 512 \\\n",
        "  --logging-steps 10 \\\n",
        "  --save-steps 500\n",
        "\n",
        "print(\"\\n✅ Stage 2 training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Save to Drive\n",
        "!mkdir -p /content/drive/MyDrive/ml-learning/artifacts/stage2_helpful\n",
        "!cp -r artifacts/stage2_helpful/* /content/drive/MyDrive/ml-learning/artifacts/stage2_helpful/\n",
        "\n",
        "print(\"✅ Saved to Google Drive\")\n",
        "print(\"Location: MyDrive/ml-learning/artifacts/stage2_helpful/\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
