{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 2: Helpful Fine-Tuning\n",
        "\n",
        "QLoRA fine-tuning on Anthropic/hh-rlhf helpful subset.\n",
        "\n",
        "**Expected time**: ~2-3 hours on T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Setup\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content')\n",
        "\n",
        "!git clone https://github.com/Jai-Dhiman/ml-learning.git\n",
        "os.chdir('/content/ml-learning/helpful-finetuning')\n",
        "\n",
        "print(f\"✅ Ready in: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Login to Hugging Face (required for Gemma model access)\n",
        "# Secure login without storing/printing your token.\n",
        "# If getpass has issues in Colab, this cell will fall back to the interactive widget provided by huggingface_hub.login().\n",
        "import os\n",
        "os.environ.pop(\"HF_TOKEN\", None)\n",
        "os.environ.pop(\"HUGGINGFACEHUB_API_TOKEN\", None)\n",
        "from huggingface_hub import login, HfApi\n",
        "try:\n",
        "    import getpass as gp\n",
        "    raw = gp.getpass(\"Paste your Hugging Face token (input hidden): \")\n",
        "    token = raw.decode() if isinstance(raw, (bytes, bytearray)) else raw\n",
        "    if not isinstance(token, str):\n",
        "        raise TypeError(f\"Unexpected token type: {type(token).__name__}\")\n",
        "    token = token.strip()\n",
        "    if not token:\n",
        "        raise ValueError(\"Empty token provided\")\n",
        "    login(token=token, add_to_git_credential=False)\n",
        "    who = HfApi().whoami(token=token)\n",
        "    print(f\"Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
        "except Exception as e:\n",
        "    print(f\"[HF Login] getpass flow failed: {e}\")\n",
        "    print(\"Falling back to interactive login widget...\")\n",
        "    login()\n",
        "    try:\n",
        "        who = HfApi().whoami()\n",
        "        print(f\"Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
        "    except Exception as e2:\n",
        "        print(f\"[HF Login] Verification skipped: {e2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Install Dependencies (using uv and pyproject.toml)\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Check GPU availability\n",
        "print('GPU available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU name:', torch.cuda.get_device_name(0))\n",
        "    !nvidia-smi\n",
        "    # Helpful memory settings on Colab\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'\n",
        "\n",
        "# Install uv package manager\n",
        "!pip install -q uv\n",
        "\n",
        "# Create and sync virtual environment with pyproject.toml dependencies\n",
        "print('\\n=== Setting up virtual environment ===')\n",
        "!uv python install 3.11\n",
        "!uv venv --python 3.11\n",
        "\n",
        "# Sync dependencies from pyproject.toml\n",
        "print('\\n=== Installing project dependencies ===')\n",
        "!bash -lc 'source .venv/bin/activate && uv sync'\n",
        "\n",
        "# Install PyTorch with CUDA 12.1 support\n",
        "print('\\n=== Installing PyTorch with CUDA 12.1 ===')\n",
        "!bash -lc \"source .venv/bin/activate && python -m pip install --index-url https://download.pytorch.org/whl/cu121 torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\"\n",
        "\n",
        "# Install JAX/Flax/Optax for Stage 1 safety classifier (if needed)\n",
        "print('\\n=== Installing JAX ecosystem for safety classifier ===')\n",
        "!bash -lc 'source .venv/bin/activate && python -m pip install \"jax[cpu]==0.4.38\" \"flax>=0.8.4,<0.9.0\" \"optax>=0.2.2,<0.3.0\"'\n",
        "\n",
        "# Ensure numpy compatibility\n",
        "!bash -lc 'source .venv/bin/activate && python -m pip install \"numpy<2.0.0\" --force-reinstall'\n",
        "\n",
        "# Quick dataset preflight test\n",
        "print('\\n=== Testing dataset access ===')\n",
        "!bash -lc 'source .venv/bin/activate && python -c \"from datasets import load_dataset; ds=load_dataset(\\\"Anthropic/hh-rlhf\\\",\\\"default\\\",split=\\\"test[:1]\\\"); print(\\\"Dataset preflight OK - tiny load:\\\", len(ds))\"'\n",
        "\n",
        "print('\\n✅ Dependencies installed successfully!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: PREFLIGHT TEST (Config validation only)\n",
        "print(\"=\"*70)\n",
        "print(\"PREFLIGHT: Validating configuration and dataset access\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!bash -lc 'cd /content/ml-learning/helpful-finetuning && source .venv/bin/activate && WANDB_DISABLED=true python -m src.training.train_qlora --config configs/base_config.yaml --override configs/colab_config.yaml --preflight-only --disable-wandb'\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ PREFLIGHT PASSED - Ready for full training\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Full Training\n",
        "print(\"=\"*70)\n",
        "print(\"STAGE 2: HELPFUL FINE-TUNING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!bash -lc 'cd /content/ml-learning/helpful-finetuning && source .venv/bin/activate && WANDB_DISABLED=true python -m src.training.train_qlora --config configs/base_config.yaml --override configs/colab_config.yaml --disable-wandb'\n",
        "\n",
        "print(\"\\n✅ Stage 2 training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Save Results to Google Drive\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print('=' * 70)\n",
        "print('SAVING TRAINING RESULTS TO GOOGLE DRIVE')\n",
        "print('=' * 70)\n",
        "\n",
        "# Define source and destination paths\n",
        "base_dir = '/content/ml-learning/helpful-finetuning'\n",
        "drive_base = '/content/drive/MyDrive/ml-learning/stage2_results'\n",
        "\n",
        "# Create destination directory\n",
        "os.makedirs(drive_base, exist_ok=True)\n",
        "\n",
        "# Find and copy all result directories\n",
        "results_found = False\n",
        "\n",
        "# 1. Copy outputs directory (training checkpoints)\n",
        "outputs_dir = os.path.join(base_dir, 'outputs')\n",
        "if os.path.exists(outputs_dir):\n",
        "    dest = os.path.join(drive_base, 'outputs')\n",
        "    print(f'Copying {outputs_dir} -> {dest}')\n",
        "    if os.path.exists(dest):\n",
        "        shutil.rmtree(dest)\n",
        "    shutil.copytree(outputs_dir, dest)\n",
        "    size = sum(f.stat().st_size for f in Path(dest).rglob('*') if f.is_file())\n",
        "    print(f'✅ Copied outputs ({size / 1024**2:.1f} MB)')\n",
        "    results_found = True\n",
        "else:\n",
        "    print('⚠️ No outputs directory found')\n",
        "\n",
        "# 2. Copy final_model\n",
        "final_model_dir = os.path.join(base_dir, 'final_model')\n",
        "if os.path.exists(final_model_dir):\n",
        "    dest = os.path.join(drive_base, 'final_model')\n",
        "    print(f'\\nCopying {final_model_dir} -> {dest}')\n",
        "    if os.path.exists(dest):\n",
        "        shutil.rmtree(dest)\n",
        "    shutil.copytree(final_model_dir, dest)\n",
        "    size = sum(f.stat().st_size for f in Path(dest).rglob('*') if f.is_file())\n",
        "    print(f'✅ Copied final_model ({size / 1024**2:.1f} MB)')\n",
        "    results_found = True\n",
        "else:\n",
        "    print('⚠️ No final_model directory found')\n",
        "\n",
        "# 3. Copy LoRA adapters from artifacts\n",
        "lora_dir = '/content/ml-learning/artifacts/stage2_artifacts/lora_adapters'\n",
        "if os.path.exists(lora_dir):\n",
        "    dest = os.path.join(drive_base, 'lora_adapters')\n",
        "    print(f'\\nCopying {lora_dir} -> {dest}')\n",
        "    if os.path.exists(dest):\n",
        "        shutil.rmtree(dest)\n",
        "    shutil.copytree(lora_dir, dest)\n",
        "    size = sum(f.stat().st_size for f in Path(dest).rglob('*') if f.is_file())\n",
        "    print(f'✅ Copied lora_adapters ({size / 1024**2:.1f} MB)')\n",
        "    results_found = True\n",
        "else:\n",
        "    print('⚠️ No lora_adapters directory found')\n",
        "\n",
        "# Summary\n",
        "print('\\n' + '=' * 70)\n",
        "if results_found:\n",
        "    print('✅ Results saved to Google Drive!')\n",
        "    print(f'Location: MyDrive/ml-learning/stage2_results/')\n",
        "    print('\\nTo download from Colab:')\n",
        "    print('  1. Click the folder icon on the left sidebar')\n",
        "    print('  2. Navigate to: drive/MyDrive/ml-learning/stage2_results/')\n",
        "    print('  3. Right-click on folders and select Download')\n",
        "else:\n",
        "    print('❌ No results found to save!')\n",
        "    print('\\nSearching for any checkpoint files...')\n",
        "    !find /content/ml-learning -name '*checkpoint*' -o -name 'adapter_*' -o -name 'pytorch_model.bin' 2>/dev/null | head -20\n",
        "print('=' * 70)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
