{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 2: Helpful Response Fine-tuning (Gemma-7B-IT, QLoRA) â€” Colab Notebook\n",
        "\n",
        "This notebook fine-tunes Google Gemma-7B-IT using QLoRA on the Anthropic Helpful-Harmless dataset, logs experiments to Weights & Biases, and optionally evaluates helpfulness and safety deltas using your Stage 1 safety classifier.\n",
        "\n",
        "Notes:\n",
        "- You need to accept the Gemma model license on Hugging Face Hub with your account before training.\n",
        "- You will login to Hugging Face and W&B via Colab widgets (no plaintext secrets).\n",
        "- If you have a Stage 1 package zip in Google Drive (safety_text_classifier_trained_*.zip), this notebook will auto-extract it for safety filtering and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# Minimal setup for Colab: ensure GPU and install uv (we'll use repo-pinned deps)\n",
        "import torch\n",
        "assert torch.cuda.is_available(), 'CUDA not available. Please enable GPU in Runtime > Change runtime type > Hardware accelerator: GPU.'\n",
        "!pip -q install -U uv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu-check"
      },
      "outputs": [],
      "source": [
        "# GPU check & memory tweaks\n",
        "import torch, os\n",
        "print('GPU available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU name:', torch.cuda.get_device_name(0))\n",
        "    # Helpful memory settings on Colab\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Repository setup\n",
        "You have two options:\n",
        "- A) Mount Google Drive if you already have your repo under Drive (recommended)\n",
        "- B) Clone your GitHub repository (replace the placeholder URL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "repo-setup"
      },
      "outputs": [],
      "source": [
        "# Clone repo from GitHub and (optionally) mount Drive for model assets\n",
        "USE_DRIVE_FOR_ASSETS = True  # Mount Drive to fetch large checkpoints only\n",
        "\n",
        "import os, glob\n",
        "from pathlib import Path\n",
        "repo_root = '/content/ml-learning'\n",
        "\n",
        "# Always clone or pull latest code from GitHub\n",
        "if not os.path.exists(repo_root):\n",
        "    !git clone https://github.com/Jai-Dhiman/ml-learning {repo_root}\n",
        "else:\n",
        "    print('Repo path exists; pulling latest changes...')\n",
        "%cd {repo_root}\n",
        "!git pull --ff-only\n",
        "\n",
        "# Mount Drive only for model artifacts (e.g., Stage 1 zip)\n",
        "if USE_DRIVE_FOR_ASSETS:\n",
        "    from google.colab import drive\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        print('Drive mounted for model assets.')\n",
        "    except Exception as e:\n",
        "        print('Drive not mounted. Proceeding without Drive assets. Error:', e)\n",
        "\n",
        "%cd {repo_root}/helpful-finetuning\n",
        "!pwd\n",
        "\n",
        "# Create and sync a project-local environment pinned to repo deps\n",
        "!uv python install 3.11\n",
        "!uv venv --python 3.11\n",
        "!bash -lc 'source .venv/bin/activate && uv sync'\n",
        "\n",
        "# Ensure bitsandbytes (GPU) and Triton are present in the venv (bootstrap pip in venv, then pip)\n",
"!bash -lc \"cd /content/ml-learning/helpful-finetuning && source .venv/bin/activate && which python && python -V && curl -sS https://bootstrap.pypa.io/get-pip.py -o /tmp/get-pip.py && python /tmp/get-pip.py && python -m pip install --upgrade pip setuptools wheel && python -m pip uninstall -y bitsandbytes triton torch torchvision torchaudio || true\"\n",
"!bash -lc \"source .venv/bin/activate && python -m pip install --index-url https://download.pytorch.org/whl/cu121 torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1\"\n",
"!bash -lc \"source .venv/bin/activate && python -m pip install triton==2.3.1\"\n",
"!bash -lc \"source .venv/bin/activate && ( \\\n    python -m pip install --no-cache-dir --force-reinstall --no-deps --index-url https://huggingface.github.io/bitsandbytes-wheels/cu121/ bitsandbytes==0.44.1 \\\n || python -m pip install --no-cache-dir --force-reinstall --no-deps --index-url https://huggingface.github.io/bitsandbytes-wheels/cu121/ bitsandbytes==0.44.0 \\\n || python -m pip install --no-cache-dir --force-reinstall --no-deps --index-url https://huggingface.github.io/bitsandbytes-wheels/cu121/ bitsandbytes==0.43.2 \\\n || python -m pip install --no-cache-dir --force-reinstall --no-deps --index-url https://jllllll.github.io/bitsandbytes-wheels/cu121/ bitsandbytes==0.44.1 \\\n || python -m pip install --no-cache-dir --force-reinstall --no-deps --index-url https://jllllll.github.io/bitsandbytes-wheels/cu121/ bitsandbytes==0.44.0 \\\n || python -m pip install --no-cache-dir --force-reinstall --no-deps --index-url https://jllllll.github.io/bitsandbytes-wheels/cu121/ bitsandbytes==0.43.2 \\\n)\"\n",
"!bash -lc \"source .venv/bin/activate && python -c 'import importlib.metadata as im, torch; import bitsandbytes as bnb; print(\\\"torch version:\\\", torch.__version__, \\\"CUDA:\\\", torch.version.cuda, \\\"is_available:\\\", torch.cuda.is_available()); print(\\\"triton version:\\\", im.version(\\\"triton\\\")); print(\\\"bnb file:\\\", bnb.__file__); print(\\\"bnb version:\\\", getattr(bnb, \\\"__version__\\\", \\\"n/a\\\")); print(\\\"bnb dist:\\\", im.version(\\\"bitsandbytes\\\"))'\"\n",
"!bash -lc \"source .venv/bin/activate && python -m pip show bitsandbytes\"\n",
"!bash -lc 'source .venv/bin/activate && python -m pip show bitsandbytes'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stage1-zip"
      },
      "outputs": [],
      "source": [
        "# If a Stage 1 zip exists in Drive, auto-extract to expected path for safety filtering/eval\n",
        "import os, glob\n",
        "dst_dir = '/content/ml-learning/safety-text-classifier'\n",
        "os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "# Preferred exact path (provided by user)\n",
        "exact_zip = '/content/drive/MyDrive/safety-text-classifier/safety_text_classifier_trained_20250916_0632.zip'\n",
        "candidates = []\n",
        "if os.path.exists(exact_zip):\n",
        "    candidates = [exact_zip]\n",
        "else:\n",
        "    # Fallback patterns\n",
        "    pats = [\n",
        "        '/content/drive/MyDrive/safety_text_classifier_trained_*.zip',\n",
        "        '/content/drive/MyDrive/safety-text-classifier/safety_text_classifier_trained_*.zip',\n",
        "    ]\n",
        "    for p in pats:\n",
        "        candidates.extend(glob.glob(p))\n",
        "\n",
        "if candidates:\n",
        "    candidates.sort(reverse=True)\n",
        "    print('Found Stage 1 package:', candidates[0])\n",
        "    !unzip -o \"{candidates[0]}\" -d {dst_dir}\n",
        "else:\n",
        "    print('No Stage 1 zip found on Drive. If checkpoints are in the repo path, safety filter will use them.')\n",
        "    print('Otherwise safety filter defaults to safe to avoid blocking training.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf-login"
      },
      "outputs": [],
      "source": [
        "# Login to Hugging Face (required for Gemma model access)\n",
        "# Secure login without storing/printing your token.\n",
        "# If getpass has issues in Colab, this cell will fall back to the interactive widget provided by huggingface_hub.login().\n",
        "import os\n",
        "os.environ.pop(\"HF_TOKEN\", None)\n",
        "os.environ.pop(\"HUGGINGFACEHUB_API_TOKEN\", None)\n",
        "from huggingface_hub import login, HfApi\n",
        "try:\n",
        "    import getpass as gp\n",
        "    raw = gp.getpass(\"Paste your Hugging Face token (input hidden): \")\n",
        "    token = raw.decode() if isinstance(raw, (bytes, bytearray)) else raw\n",
        "    if not isinstance(token, str):\n",
        "        raise TypeError(f\"Unexpected token type: {type(token).__name__}\")\n",
        "    token = token.strip()\n",
        "    if not token:\n",
        "        raise ValueError(\"Empty token provided\")\n",
        "    login(token=token, add_to_git_credential=False)\n",
        "    who = HfApi().whoami(token=token)\n",
        "    print(f\"Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
        "except Exception as e:\n",
        "    print(f\"[HF Login] getpass flow failed: {e}\")\n",
        "    print(\"Falling back to interactive login widget...\")\n",
        "    login()\n",
        "    try:\n",
        "        who = HfApi().whoami()\n",
        "        print(f\"Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
        "    except Exception as e2:\n",
        "        print(f\"[HF Login] Verification skipped: {e2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wandb-login"
      },
      "outputs": [],
      "source": [
        "# Login to Weights & Biases for experiment tracking\n",
        "import wandb\n",
        "wandb.login()  # Enter W&B API key in widget\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset-preflight"
      },
      "outputs": [],
      "source": [
        "# Ensure numpy compatibility and run a tiny dataset load in venv (no heredocs)\n",
"!bash -lc 'source .venv/bin/activate && python -m pip install \"numpy<2.0.0\" --force-reinstall'\n",
        "!bash -lc 'source .venv/bin/activate && python -c \"from datasets import load_dataset; ds=load_dataset(\\\"Anthropic/hh-rlhf\\\",\\\"default\\\",split=\\\"test[:1]\\\"); print(\\\"Dataset preflight OK - tiny load:\\\", len(ds))\"'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train: Gemma-7B-IT with QLoRA (Colab-optimized overrides)\n",
        "- Base config: `configs/base_config.yaml`\n",
        "- Overrides:   `configs/colab_config.yaml` (smaller batch/seq_len, GA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
"!bash -lc 'cd /content/ml-learning/helpful-finetuning && source .venv/bin/activate && python -m src.training.train_qlora --config configs/base_config.yaml --override configs/colab_config.yaml'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate (quick subset)\n",
        "Computes a simple helpfulness heuristic vs base and safety deltas using Stage 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate"
      },
      "outputs": [],
      "source": [
        "# Run evaluation (uses ./lora_adapters if present)\n",
"!bash -lc 'cd /content/ml-learning/helpful-finetuning && source .venv/bin/activate && python -m src.evaluation.evaluate_helpfulness --config configs/base_config.yaml'\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
