{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 2: Helpful Response Fine-tuning (Gemma-7B-IT, QLoRA) â€” Colab Notebook\n",
        "\n",
        "This notebook fine-tunes Google Gemma-7B-IT using QLoRA on the Anthropic Helpful-Harmless dataset, logs experiments to Weights & Biases, and optionally evaluates helpfulness and safety deltas using your Stage 1 safety classifier.\n",
        "\n",
        "Notes:\n",
        "- You need to accept the Gemma model license on Hugging Face Hub with your account before training.\n",
        "- You will login to Hugging Face and W&B via Colab widgets (no plaintext secrets).\n",
        "- If you have a Stage 1 package zip in Google Drive (safety_text_classifier_trained_*.zip), this notebook will auto-extract it for safety filtering and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# Install dependencies (QLoRA stack; make safety/JAX optional)\n",
        "!pip -q install -U \"pyarrow<20.0.0\"\n",
        "!pip -q install -U bitsandbytes==0.43.1 accelerate datasets wandb evaluate pyyaml tqdm sentencepiece\n",
        "!pip -q install -U git+https://github.com/huggingface/transformers.git\n",
        "!pip -q install -U git+https://github.com/huggingface/peft.git\n",
        "# JAX: avoid Colab plugin mismatch; only install if missing\n",
        "import subprocess, sys\n",
        "from importlib.metadata import version, PackageNotFoundError\n",
        "\n",
        "def pip_quiet(args):\n",
        "    return subprocess.run([sys.executable, '-m', 'pip', 'install', '-q'] + args).returncode == 0\n",
        "\n",
        "# Remove potentially incompatible CUDA plugin (Colab ships newer jaxlib)\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', 'jax-cuda12-plugin'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "# Try to import existing JAX; if unavailable, install CPU-only JAX to avoid GPU plugin issues\n",
        "try:\n",
        "    import jax\n",
        "    try:\n",
        "        jax_ver = version('jax')\n",
        "    except PackageNotFoundError:\n",
        "        jax_ver = 'unknown'\n",
        "    try:\n",
        "        jaxlib_ver = version('jaxlib')\n",
        "    except PackageNotFoundError:\n",
        "        jaxlib_ver = 'unknown'\n",
        "    print('JAX present:', jax_ver, 'jaxlib:', jaxlib_ver)\n",
        "except Exception as e:\n",
        "    print('JAX not present or broken, installing CPU-only JAX...', e)\n",
        "    pip_quiet([\"jax[cpu]==0.4.38\"])\n",
        "    pip_quiet([\"flax>=0.8.4,<0.9.0\", \"optax>=0.2.2,<0.3.0\"])\n",
        "    import jax\n",
        "    print('JAX version:', jax.__version__)\n",
        "\n",
        "# If JAX imports, print devices (may be CPU)\n",
        "try:\n",
        "    import jax\n",
        "    print('JAX devices:', jax.devices())\n",
        "except Exception as e:\n",
        "    print('JAX devices check failed:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu-check"
      },
      "outputs": [],
      "source": [
        "# GPU check & memory tweaks\n",
        "import torch, os\n",
        "print('GPU available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU name:', torch.cuda.get_device_name(0))\n",
        "    # Helpful memory settings on Colab\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Repository setup\n",
        "You have two options:\n",
        "- A) Mount Google Drive if you already have your repo under Drive (recommended)\n",
        "- B) Clone your GitHub repository (replace the placeholder URL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "repo-setup"
      },
      "outputs": [],
      "source": [
        "# Option A: Mount Google Drive and work from there (if you keep your repo in Drive)\n",
        "USE_DRIVE = True  # set to False to use GitHub clone instead\n",
        "\n",
        "import os, glob\n",
        "from pathlib import Path\n",
        "repo_root = '/content/ml-learning'\n",
        "\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        print('Drive mounted. Expecting repo under /content/drive/MyDrive/ml-learning')\n",
        "        if os.path.exists('/content/drive/MyDrive/ml-learning'):\n",
        "            !rsync -a --delete /content/drive/MyDrive/ml-learning/ /content/ml-learning/\n",
        "        else:\n",
        "            print('Repo not found in Drive. Proceeding with placeholder GitHub clone.')\n",
        "            USE_DRIVE = False\n",
        "    except Exception as e:\n",
        "        print('Drive not mounted. Using GitHub clone. Error:', e)\n",
        "        USE_DRIVE = False\n",
        "\n",
        "if not USE_DRIVE:\n",
        "    # Option B: Clone from GitHub (replace with your repo URL)\n",
        "    if not os.path.exists(repo_root):\n",
        "        !git clone https://github.com/yourusername/ml-learning.git {repo_root}\n",
        "\n",
        "%cd {repo_root}/helpful-finetuning\n",
        "!pwd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stage1-zip"
      },
      "outputs": [],
      "source": [
        "# If a Stage 1 zip exists in Drive, auto-extract to expected path for safety filtering/eval\n",
        "import os, glob\n",
        "zip_candidates = glob.glob('/content/drive/MyDrive/safety_text_classifier_trained_*.zip')\n",
        "if zip_candidates:\n",
        "    os.makedirs('/content/ml-learning/safety-text-classifier', exist_ok=True)\n",
        "    print('Found Stage 1 package:', zip_candidates[0])\n",
        "    !unzip -o \"{zip_candidates[0]}\" -d /content/ml-learning/safety-text-classifier\n",
        "else:\n",
        "    print('No Stage 1 zip found. If checkpoints are in the repo path, safety filter will use them.\n",
        "Otherwise safety filter defaults to safe to avoid blocking training.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "login"
      },
      "outputs": [],
      "source": [
        "# Login to Hugging Face and Weights & Biases (widgets)\n",
        "from huggingface_hub import login\n",
        "import wandb\n",
        "login()  # Enter HF token in widget (required for Gemma)\n",
        "wandb.login()  # Enter W&B API key in widget\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train: Gemma-7B-IT with QLoRA (Colab-optimized overrides)\n",
        "- Base config: `configs/base_config.yaml`\n",
        "- Overrides:   `configs/colab_config.yaml` (smaller batch/seq_len, GA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "!python -m src.training.train_qlora --config configs/base_config.yaml --override configs/colab_config.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate (quick subset)\n",
        "Computes a simple helpfulness heuristic vs base and safety deltas using Stage 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate"
      },
      "outputs": [],
      "source": [
        "# Run evaluation (uses ./lora_adapters if present)\n",
        "!python -m src.evaluation.evaluate_helpfulness --config configs/base_config.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Interactive demo (Gradio)\n",
        "Launch a lightweight UI with share link to compare base vs fine-tuned and see safety overlay.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Launch demo (optional)\n",
        "USE_DEMO = False  #@param {type:\"boolean\"}\n",
        "if USE_DEMO:\n",
        "    import gradio as gr\n",
        "    from src.inference.generate import GemmaInference\n",
        "    from src.utils.safety_integration import SafetyFilter\n",
        "    base = GemmaInference('google/gemma-7b-it', adapter_path=None, load_in_4bit=True)\n",
        "    ft   = GemmaInference('google/gemma-7b-it', adapter_path='./lora_adapters', load_in_4bit=True)\n",
        "    safety = SafetyFilter(\n",
        "        classifier_config_path='../safety-text-classifier/configs/base_config.yaml',\n",
        "        checkpoint_dir='../safety-text-classifier/checkpoints/best_model',\n",
        "    )\n",
        "    def compare(prompt, temperature, top_p, max_length, safety_threshold):\n",
        "        b = base.generate(prompt, max_length=max_length, temperature=temperature, top_p=top_p)\n",
        "        f = ft.generate(prompt,   max_length=max_length, temperature=temperature, top_p=top_p)\n",
        "        bs = safety.score_text(b)\n",
        "        fs = safety.score_text(f)\n",
        "        bf = 'ðŸŸ¢' if bs >= safety_threshold else 'ðŸ”´'\n",
        "        ff = 'ðŸŸ¢' if fs >= safety_threshold else 'ðŸ”´'\n",
        "        return b, f, f\"{bf} Safety: {bs:.2f}\", f\"{ff} Safety: {fs:.2f}\"\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown('## Stage 2: Base vs Fine-tuned (Gemma-7B-IT + QLoRA)')\n",
        "        prompt = gr.Textbox(label='Prompt', lines=4)\n",
        "        with gr.Row():\n",
        "            temperature = gr.Slider(0.1, 1.0, value=0.7, step=0.05, label='Temperature')\n",
        "            top_p       = gr.Slider(0.1, 1.0, value=0.9, step=0.05, label='Top-p')\n",
        "            max_length  = gr.Slider(64, 1024, value=512, step=16, label='Max length')\n",
        "            safety_thr  = gr.Slider(0.0, 1.0, value=0.8, step=0.05, label='Safety threshold')\n",
        "        go = gr.Button('Generate')\n",
        "        base_out = gr.Textbox(label='Base response', lines=10)\n",
        "        ft_out   = gr.Textbox(label='Fine-tuned response', lines=10)\n",
        "        base_s   = gr.Label(label='Base safety')\n",
        "        ft_s     = gr.Label(label='Fine-tuned safety')\n",
        "        go.click(compare, inputs=[prompt, temperature, top_p, max_length, safety_thr], outputs=[base_out, ft_out, base_s, ft_s])\n",
        "    app.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
