{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 2: Helpful Response Fine-tuning (Gemma-7B-IT, QLoRA) â€” Colab Notebook\n",
        "\n",
        "This notebook fine-tunes Google Gemma-7B-IT using QLoRA on the Anthropic Helpful-Harmless dataset, logs experiments to Weights & Biases, and optionally evaluates helpfulness and safety deltas using your Stage 1 safety classifier.\n",
        "\n",
        "Notes:\n",
        "- You need to accept the Gemma model license on Hugging Face Hub with your account before training.\n",
        "- You will login to Hugging Face and W&B via Colab widgets (no plaintext secrets).\n",
        "- If you have a Stage 1 package zip in Google Drive (safety_text_classifier_trained_*.zip), this notebook will auto-extract it for safety filtering and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# Install dependencies (QLoRA stack; make safety/JAX optional)\n",
        "# First, ensure CUDA is properly available for BitsAndBytes\n",
        "import torch\n",
        "assert torch.cuda.is_available(), 'CUDA not available. Please enable GPU in Runtime menu.'\n",
        "cuda_version = torch.version.cuda\n",
        "print(f'CUDA version: {cuda_version}')\n",
        "\n",
        "# Env tweaks: quiet bnb, force JAX to CPU to avoid CUDA plugin noise\n",
        "import os\n",
        "os.environ['BITSANDBYTES_NOWELCOME'] = '1'\n",
        "os.environ['JAX_PLATFORM_NAME'] = 'cpu'\n",
        "# Hint BitsAndBytes which CUDA to use (common Colab versions)\n",
        "if cuda_version and cuda_version.startswith('12.1'):\n",
        "    os.environ['BNB_CUDA_VERSION'] = '121'\n",
        "elif cuda_version and cuda_version.startswith('12.6'):\n",
        "    os.environ['BNB_CUDA_VERSION'] = '126'\n",
        "\n",
        "# Install core dependencies\n",
        "!pip -q install -U \"pyarrow<20.0.0\"\n",
        "!pip -q install -U accelerate datasets wandb evaluate pyyaml tqdm sentencepiece\n",
        "!pip -q install -U trl==0.9.4\n",
        "\n",
        "# Install BitsAndBytes (BNB) with proper CUDA support and Triton dependency\n",
        "import sys, subprocess\n",
        "def pip_install(args):\n",
        "    return subprocess.run([sys.executable, '-m', 'pip'] + args, check=False)\n",
        "\n",
        "# Clean any preinstalled BNB\n",
        "pip_install(['uninstall', '-y', 'bitsandbytes'])\n",
        "\n",
        "# Install Triton version compatible with BNB's triton ops\n",
        "pip_install(['install', '-U', 'triton==2.2.0'])\n",
        "\n",
        "# Install BNB: if CUDA 12.6, prefer community wheel index for cu126. Else standard PyPI.\n",
        "if cuda_version and str(cuda_version).startswith('12.6'):\n",
        "    print('Installing bitsandbytes for CUDA 12.6 (community wheels)...')\n",
        "    pip_install(['install', '--pre', '-U', '--extra-index-url', 'https://jllllll.github.io/bitsandbytes-wheels/cu126/', 'bitsandbytes'])\n",
        "else:\n",
        "    pip_install(['install', 'bitsandbytes==0.43.1'])\n",
        "\n",
        "# Install transformers and peft from latest\n",
        "!pip -q install -U git+https://github.com/huggingface/transformers.git\n",
        "!pip -q install -U git+https://github.com/huggingface/peft.git\n",
        "\n",
        "# Verify BitsAndBytes CUDA support\n",
        "import bitsandbytes as bnb\n",
        "print(f'BitsAndBytes version: {bnb.__version__}')\n",
        "# This should not raise an error if CUDA is properly supported\n",
        "try:\n",
        "    bnb.cuda_setup.main.CUDASetup.get_instance()\n",
        "    print('BitsAndBytes CUDA support: OK')\n",
        "except Exception as e:\n",
        "    print(f'BitsAndBytes CUDA warning (may still work): {e}')\n",
        "    print('If quantized loading fails later, rerun this cell to reinstall BNB for your CUDA version.')\n",
        "\n",
        "# Optional: ensure pyarrow < 20 to silence cudf warnings\n",
        "pip_install(['install', '--no-deps', '--force-reinstall', 'pyarrow<20.0.0'])\n",
        "\n",
        "# JAX: avoid Colab plugin mismatch; only install if missing\n",
        "import subprocess, sys\n",
        "from importlib.metadata import version, PackageNotFoundError\n",
        "\n",
        "def pip_quiet(args):\n",
        "    return subprocess.run([sys.executable, '-m', 'pip', 'install', '-q'] + args).returncode == 0\n",
        "\n",
        "# Remove potentially incompatible CUDA plugin (Colab ships newer jaxlib)\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', 'jax-cuda12-plugin'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "# Try to import existing JAX; if unavailable, install CPU-only JAX to avoid GPU plugin issues\n",
        "try:\n",
        "    import jax\n",
        "    try:\n",
        "        jax_ver = version('jax')\n",
        "    except PackageNotFoundError:\n",
        "        jax_ver = 'unknown'\n",
        "    try:\n",
        "        jaxlib_ver = version('jaxlib')\n",
        "    except PackageNotFoundError:\n",
        "        jaxlib_ver = 'unknown'\n",
        "    print('JAX present:', jax_ver, 'jaxlib:', jaxlib_ver)\n",
        "except Exception as e:\n",
        "    print('JAX not present or broken, installing CPU-only JAX...', e)\n",
        "    pip_quiet([\"jax[cpu]==0.4.38\"])\n",
        "    pip_quiet([\"flax>=0.8.4,<0.9.0\", \"optax>=0.2.2,<0.3.0\"])\n",
        "    import jax\n",
        "    print('JAX version:', jax.__version__)\n",
        "\n",
        "# If JAX imports, print devices (may be CPU)\n",
        "try:\n",
        "    import jax\n",
        "    print('JAX devices:', jax.devices())\n",
        "except Exception as e:\n",
        "    print('JAX devices check failed:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu-check"
      },
      "outputs": [],
      "source": [
        "# GPU check & memory tweaks\n",
        "import torch, os\n",
        "print('GPU available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU name:', torch.cuda.get_device_name(0))\n",
        "    # Helpful memory settings on Colab\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Repository setup\n",
        "You have two options:\n",
        "- A) Mount Google Drive if you already have your repo under Drive (recommended)\n",
        "- B) Clone your GitHub repository (replace the placeholder URL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "repo-setup"
      },
      "outputs": [],
      "source": [
        "# Clone repo from GitHub and (optionally) mount Drive for model assets\n",
        "USE_DRIVE_FOR_ASSETS = True  # Mount Drive to fetch large checkpoints only\n",
        "\n",
        "import os, glob\n",
        "from pathlib import Path\n",
        "repo_root = '/content/ml-learning'\n",
        "\n",
        "# Always clone the latest code from GitHub\n",
        "if not os.path.exists(repo_root):\n",
        "    !git clone https://github.com/Jai-Dhiman/ml-learning {repo_root}\n",
        "else:\n",
        "    print('Repo path exists; pulling latest changes...')\n",
        "    %cd {repo_root}\n",
        "    !git pull --ff-only\n",
        "\n",
        "# Mount Drive only for model artifacts (e.g., Stage 1 zip)\n",
        "if USE_DRIVE_FOR_ASSETS:\n",
        "    from google.colab import drive\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        print('Drive mounted for model assets.')\n",
        "    except Exception as e:\n",
        "        print('Drive not mounted. Proceeding without Drive assets. Error:', e)\n",
        "\n",
        "%cd {repo_root}/helpful-finetuning\n",
        "!pwd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stage1-zip"
      },
      "outputs": [],
      "source": [
        "# If a Stage 1 zip exists in Drive, auto-extract to expected path for safety filtering/eval\n",
        "import os, glob\n",
        "dst_dir = '/content/ml-learning/safety-text-classifier'\n",
        "os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "# Preferred exact path (provided by user)\n",
        "exact_zip = '/content/drive/MyDrive/safety-text-classifier/safety_text_classifier_trained_20250916_0632.zip'\n",
        "candidates = []\n",
        "if os.path.exists(exact_zip):\n",
        "    candidates = [exact_zip]\n",
        "else:\n",
        "    # Fallback patterns\n",
        "    pats = [\n",
        "        '/content/drive/MyDrive/safety_text_classifier_trained_*.zip',\n",
        "        '/content/drive/MyDrive/safety-text-classifier/safety_text_classifier_trained_*.zip',\n",
        "    ]\n",
        "    for p in pats:\n",
        "        candidates.extend(glob.glob(p))\n",
        "\n",
        "if candidates:\n",
        "    candidates.sort(reverse=True)\n",
        "    print('Found Stage 1 package:', candidates[0])\n",
        "    !unzip -o \"{candidates[0]}\" -d {dst_dir}\n",
        "else:\n",
        "    print('No Stage 1 zip found on Drive. If checkpoints are in the repo path, safety filter will use them.')\n",
        "    print('Otherwise safety filter defaults to safe to avoid blocking training.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf-login"
      },
      "outputs": [],
      "source": [
        "# Login to Hugging Face (required for Gemma model access)\n",
        "# Secure login without storing/printing your token.\n",
        "# If getpass has issues in Colab, this cell will fall back to the interactive widget provided by huggingface_hub.login().\n",
        "import os\n",
        "os.environ.pop(\"HF_TOKEN\", None)\n",
        "os.environ.pop(\"HUGGINGFACEHUB_API_TOKEN\", None)\n",
        "from huggingface_hub import login, HfApi\n",
        "try:\n",
        "    import getpass as gp\n",
        "    raw = gp.getpass(\"Paste your Hugging Face token (input hidden): \")\n",
        "    token = raw.decode() if isinstance(raw, (bytes, bytearray)) else raw\n",
        "    if not isinstance(token, str):\n",
        "        raise TypeError(f\"Unexpected token type: {type(token).__name__}\")\n",
        "    token = token.strip()\n",
        "    if not token:\n",
        "        raise ValueError(\"Empty token provided\")\n",
        "    login(token=token, add_to_git_credential=False)\n",
        "    who = HfApi().whoami(token=token)\n",
        "    print(f\"Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
        "except Exception as e:\n",
        "    print(f\"[HF Login] getpass flow failed: {e}\")\n",
        "    print(\"Falling back to interactive login widget...\")\n",
        "    login()\n",
        "    try:\n",
        "        who = HfApi().whoami()\n",
        "        print(f\"Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
        "    except Exception as e2:\n",
        "        print(f\"[HF Login] Verification skipped: {e2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wandb-login"
      },
      "outputs": [],
      "source": [
        "# Login to Weights & Biases for experiment tracking\n",
        "import wandb\n",
        "wandb.login()  # Enter W&B API key in widget\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset-preflight"
      },
      "outputs": [],
      "source": [
        "# Preflight: verify Anthropic/hh-rlhf subset and splits (no fallbacks)\n",
        "from datasets import get_dataset_config_names, get_dataset_split_names\n",
        "import yaml\n",
        "base = yaml.safe_load(open('configs/base_config.yaml'))\n",
        "try:\n",
        "    override = yaml.safe_load(open('configs/colab_config.yaml'))\n",
        "except Exception:\n",
        "    override = {}\n",
        "cfg = dict(base)\n",
        "if isinstance(override, dict):\n",
        "    for k, v in override.items():\n",
        "        if isinstance(v, dict) and k in cfg and isinstance(cfg[k], dict):\n",
        "            cfg[k] = {**cfg[k], **v}\n",
        "        else:\n",
        "            cfg[k] = v\n",
        "dcfg = cfg.get('dataset', {})\n",
        "name = dcfg.get('name')\n",
        "subset = dcfg.get('subset')\n",
        "train_split = dcfg.get('train_split')\n",
        "eval_split = dcfg.get('eval_split')\n",
        "print('Selected dataset config:', dcfg)\n",
        "assert name == 'Anthropic/hh-rlhf', f\"Stage 2 requires Anthropic/hh-rlhf, got: {name}\"\n",
        "assert subset, 'dataset.subset is required (e.g., helpful-base)'\n",
        "configs = get_dataset_config_names(name, revision='main')\n",
        "print('Available subsets (main):', configs)\n",
        "assert subset in configs, f\"Invalid subset {subset}. Available on main: {configs}\"\n",
        "def _base_split(s):\n",
        "    return s.split('[')[0].split(':')[0].strip() if s else s\n",
        "splits = get_dataset_split_names(name, subset, revision='main')\n",
        "print(f'Available splits for {name}/{subset} (main):', splits)\n",
        "if train_split:\n",
        "    assert _base_split(train_split) in splits, f\"Invalid train_split {train_split}. Available: {splits}\"\n",
        "if eval_split:\n",
        "    assert _base_split(eval_split) in splits, f\"Invalid eval_split {eval_split}. Available: {splits}\"\n",
        "print('Dataset preflight OK.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train: Gemma-7B-IT with QLoRA (Colab-optimized overrides)\n",
        "- Base config: `configs/base_config.yaml`\n",
        "- Overrides:   `configs/colab_config.yaml` (smaller batch/seq_len, GA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "!python -m src.training.train_qlora --config configs/base_config.yaml --override configs/colab_config.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate (quick subset)\n",
        "Computes a simple helpfulness heuristic vs base and safety deltas using Stage 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate"
      },
      "outputs": [],
      "source": [
        "# Run evaluation (uses ./lora_adapters if present)\n",
        "!python -m src.evaluation.evaluate_helpfulness --config configs/base_config.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Interactive demo (Gradio)\n",
        "Launch a lightweight UI with share link to compare base vs fine-tuned and see safety overlay.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "demo"
      },
      "outputs": [],
      "source": [
        "#@title Launch demo (optional)\n",
        "USE_DEMO = False  #@param {type:\"boolean\"}\n",
        "if USE_DEMO:\n",
        "    import gradio as gr\n",
        "    from src.inference.generate import GemmaInference\n",
        "    from src.utils.safety_integration import SafetyFilter\n",
        "    base = GemmaInference('google/gemma-7b-it', adapter_path=None, load_in_4bit=True)\n",
        "    ft   = GemmaInference('google/gemma-7b-it', adapter_path='./lora_adapters', load_in_4bit=True)\n",
        "    safety = SafetyFilter(\n",
        "        classifier_config_path='../safety-text-classifier/configs/base_config.yaml',\n",
        "        checkpoint_dir='../safety-text-classifier/checkpoints/best_model',\n",
        "    )\n",
        "    def compare(prompt, temperature, top_p, max_length, safety_threshold):\n",
        "        b = base.generate(prompt, max_length=max_length, temperature=temperature, top_p=top_p)\n",
        "        f = ft.generate(prompt,   max_length=max_length, temperature=temperature, top_p=top_p)\n",
        "        bs = safety.score_text(b)\n",
        "        fs = safety.score_text(f)\n",
        "        bf = 'ðŸŸ¢' if bs >= safety_threshold else 'ðŸ”´'\n",
        "        ff = 'ðŸŸ¢' if fs >= safety_threshold else 'ðŸ”´'\n",
        "        return b, f, f\"{bf} Safety: {bs:.2f}\", f\"{ff} Safety: {fs:.2f}\"\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown('## Stage 2: Base vs Fine-tuned (Gemma-7B-IT + QLoRA)')\n",
        "        prompt = gr.Textbox(label='Prompt', lines=4)\n",
        "        with gr.Row():\n",
        "            temperature = gr.Slider(0.1, 1.0, value=0.7, step=0.05, label='Temperature')\n",
        "            top_p       = gr.Slider(0.1, 1.0, value=0.9, step=0.05, label='Top-p')\n",
        "            max_length  = gr.Slider(64, 1024, value=512, step=16, label='Max length')\n",
        "            safety_thr  = gr.Slider(0.0, 1.0, value=0.8, step=0.05, label='Safety threshold')\n",
        "        go = gr.Button('Generate')\n",
        "        base_out = gr.Textbox(label='Base response', lines=10)\n",
        "        ft_out   = gr.Textbox(label='Fine-tuned response', lines=10)\n",
        "        base_s   = gr.Label(label='Base safety')\n",
        "        ft_s     = gr.Label(label='Fine-tuned safety')\n",
        "        go.click(compare, inputs=[prompt, temperature, top_p, max_length, safety_thr], outputs=[base_out, ft_out, base_s, ft_s])\n",
        "    app.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
