{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage 2: Helpful Response Fine-tuning (Gemma-7B-IT, QLoRA) â€” Colab Notebook\n",
        "\n",
        "This notebook fine-tunes Google Gemma-7B-IT using QLoRA on the Anthropic Helpful-Harmless dataset, logs experiments to Weights & Biases, and optionally evaluates helpfulness and safety deltas using your Stage 1 safety classifier.\n",
        "\n",
        "Notes:\n",
        "- You need to accept the Gemma model license on Hugging Face Hub with your account before training.\n",
        "- You will login to Hugging Face and W&B via Colab widgets (no plaintext secrets).\n",
        "- If you have a Stage 1 package zip in Google Drive (safety_text_classifier_trained_*.zip), this notebook will auto-extract it for safety filtering and evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# Minimal setup for Colab: ensure GPU and install uv (we'll use repo-pinned deps)\n",
        "import torch\n",
        "assert torch.cuda.is_available(), 'CUDA not available. Please enable GPU in Runtime > Change runtime type > Hardware accelerator: GPU.'\n",
        "!pip -q install -U uv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu-check"
      },
      "outputs": [],
      "source": [
        "# GPU check & memory tweaks\n",
        "import torch, os\n",
        "print('GPU available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU name:', torch.cuda.get_device_name(0))\n",
        "    # Helpful memory settings on Colab\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:512'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Repository setup\n",
        "You have two options:\n",
        "- A) Mount Google Drive if you already have your repo under Drive (recommended)\n",
        "- B) Clone your GitHub repository (replace the placeholder URL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "repo-setup"
      },
      "outputs": [],
      "source": [
        "# Clone repo from GitHub and (optionally) mount Drive for model assets\n",
        "USE_DRIVE_FOR_ASSETS = True  # Mount Drive to fetch large checkpoints only\n",
        "\n",
        "import os, glob\n",
        "from pathlib import Path\n",
        "repo_root = '/content/ml-learning'\n",
        "\n",
        "# Always clone or pull latest code from GitHub\n",
        "if not os.path.exists(repo_root):\n",
        "    !git clone https://github.com/Jai-Dhiman/ml-learning {repo_root}\n",
        "else:\n",
        "    print('Repo path exists; pulling latest changes...')\n",
        "%cd {repo_root}\n",
        "!git pull --ff-only\n",
        "\n",
        "# Mount Drive only for model artifacts (e.g., Stage 1 zip)\n",
        "if USE_DRIVE_FOR_ASSETS:\n",
        "    from google.colab import drive\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        print('Drive mounted for model assets.')\n",
        "    except Exception as e:\n",
        "        print('Drive not mounted. Proceeding without Drive assets. Error:', e)\n",
        "\n",
        "%cd {repo_root}/helpful-finetuning\n",
        "!pwd\n",
        "\n",
        "# Create and sync a project-local environment pinned to repo deps\n",
        "!uv venv\n",
        "!bash -lc 'source .venv/bin/activate && uv sync'\n",
        "\n",
        "# Ensure bitsandbytes (GPU) and Triton are present in the env\n",
"!bash -lc 'source .venv/bin/activate && uv pip uninstall -y bitsandbytes || true'\n",
"!bash -lc 'source .venv/bin/activate && python - <<\\'PY\\'\nimport torch, re, os\nv = torch.version.cuda or \"\"\ndigits = re.sub(r\"\\\\D\", \"\", v) or \"121\"\nprint(\"Detected CUDA version:\", v, \"-> BNB_CUDA_VERSION:\", digits)\nopen(\"/tmp/bnb_cuda_version\", \"w\").write(digits)\nPY'\n",
"!bash -lc 'export BNB_CUDA_VERSION=$(cat /tmp/bnb_cuda_version); source .venv/bin/activate && uv pip install -U triton==2.2.0 && uv pip install --pre -U --extra-index-url https://jllllll.github.io/bitsandbytes-wheels/cu${BNB_CUDA_VERSION}/ bitsandbytes==0.43.1'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stage1-zip"
      },
      "outputs": [],
      "source": [
        "# If a Stage 1 zip exists in Drive, auto-extract to expected path for safety filtering/eval\n",
        "import os, glob\n",
        "dst_dir = '/content/ml-learning/safety-text-classifier'\n",
        "os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "# Preferred exact path (provided by user)\n",
        "exact_zip = '/content/drive/MyDrive/safety-text-classifier/safety_text_classifier_trained_20250916_0632.zip'\n",
        "candidates = []\n",
        "if os.path.exists(exact_zip):\n",
        "    candidates = [exact_zip]\n",
        "else:\n",
        "    # Fallback patterns\n",
        "    pats = [\n",
        "        '/content/drive/MyDrive/safety_text_classifier_trained_*.zip',\n",
        "        '/content/drive/MyDrive/safety-text-classifier/safety_text_classifier_trained_*.zip',\n",
        "    ]\n",
        "    for p in pats:\n",
        "        candidates.extend(glob.glob(p))\n",
        "\n",
        "if candidates:\n",
        "    candidates.sort(reverse=True)\n",
        "    print('Found Stage 1 package:', candidates[0])\n",
        "    !unzip -o \"{candidates[0]}\" -d {dst_dir}\n",
        "else:\n",
        "    print('No Stage 1 zip found on Drive. If checkpoints are in the repo path, safety filter will use them.')\n",
        "    print('Otherwise safety filter defaults to safe to avoid blocking training.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf-login"
      },
      "outputs": [],
      "source": [
        "# Login to Hugging Face (required for Gemma model access)\n",
        "# Secure login without storing/printing your token.\n",
        "# If getpass has issues in Colab, this cell will fall back to the interactive widget provided by huggingface_hub.login().\n",
        "import os\n",
        "os.environ.pop(\"HF_TOKEN\", None)\n",
        "os.environ.pop(\"HUGGINGFACEHUB_API_TOKEN\", None)\n",
        "from huggingface_hub import login, HfApi\n",
        "try:\n",
        "    import getpass as gp\n",
        "    raw = gp.getpass(\"Paste your Hugging Face token (input hidden): \")\n",
        "    token = raw.decode() if isinstance(raw, (bytes, bytearray)) else raw\n",
        "    if not isinstance(token, str):\n",
        "        raise TypeError(f\"Unexpected token type: {type(token).__name__}\")\n",
        "    token = token.strip()\n",
        "    if not token:\n",
        "        raise ValueError(\"Empty token provided\")\n",
        "    login(token=token, add_to_git_credential=False)\n",
        "    who = HfApi().whoami(token=token)\n",
        "    print(f\"Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
        "except Exception as e:\n",
        "    print(f\"[HF Login] getpass flow failed: {e}\")\n",
        "    print(\"Falling back to interactive login widget...\")\n",
        "    login()\n",
        "    try:\n",
        "        who = HfApi().whoami()\n",
        "        print(f\"Logged in as: {who.get('name') or who.get('email') or 'OK'}\")\n",
        "    except Exception as e2:\n",
        "        print(f\"[HF Login] Verification skipped: {e2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wandb-login"
      },
      "outputs": [],
      "source": [
        "# Login to Weights & Biases for experiment tracking\n",
        "import wandb\n",
        "wandb.login()  # Enter W&B API key in widget\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset-preflight"
      },
      "outputs": [],
      "source": [
        "# Preflight: verify Anthropic/hh-rlhf subset and splits (no fallbacks)\n",
        "from datasets import get_dataset_config_names, get_dataset_split_names\n",
        "import yaml\n",
        "base = yaml.safe_load(open('configs/base_config.yaml'))\n",
        "try:\n",
        "    override = yaml.safe_load(open('configs/colab_config.yaml'))\n",
        "except Exception:\n",
        "    override = {}\n",
        "cfg = dict(base)\n",
        "if isinstance(override, dict):\n",
        "    for k, v in override.items():\n",
        "        if isinstance(v, dict) and k in cfg and isinstance(cfg[k], dict):\n",
        "            cfg[k] = {**cfg[k], **v}\n",
        "        else:\n",
        "            cfg[k] = v\n",
        "dcfg = cfg.get('dataset', {})\n",
        "name = dcfg.get('name')\n",
        "subset = dcfg.get('subset')\n",
        "train_split = dcfg.get('train_split')\n",
        "eval_split = dcfg.get('eval_split')\n",
        "print('Selected dataset config:', dcfg)\n",
        "assert name == 'Anthropic/hh-rlhf', f\"Stage 2 requires Anthropic/hh-rlhf, got: {name}\"\n",
        "assert subset, 'dataset.subset is required (e.g., helpful-base)'\n",
        "configs = get_dataset_config_names(name, revision='main')\n",
        "print('Available subsets (main):', configs)\n",
        "assert subset in configs, f\"Invalid subset {subset}. Available on main: {configs}\"\n",
        "def _base_split(s):\n",
        "    return s.split('[')[0].split(':')[0].strip() if s else s\n",
        "splits = get_dataset_split_names(name, subset, revision='main')\n",
        "print(f'Available splits for {name}/{subset} (main):', splits)\n",
        "if train_split:\n",
        "    assert _base_split(train_split) in splits, f\"Invalid train_split {train_split}. Available: {splits}\"\n",
        "if eval_split:\n",
        "    assert _base_split(eval_split) in splits, f\"Invalid eval_split {eval_split}. Available: {splits}\"\n",
        "print('Dataset preflight OK.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train: Gemma-7B-IT with QLoRA (Colab-optimized overrides)\n",
        "- Base config: `configs/base_config.yaml`\n",
        "- Overrides:   `configs/colab_config.yaml` (smaller batch/seq_len, GA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
"!bash -lc 'source .venv/bin/activate && uv run --with \"jax[cpu]==0.4.38\" --with \"flax>=0.8.4,<0.9.0\" --with \"optax>=0.2.2,<0.3.0\" python -m src.training.train_qlora --config configs/base_config.yaml --override configs/colab_config.yaml'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate (quick subset)\n",
        "Computes a simple helpfulness heuristic vs base and safety deltas using Stage 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate"
      },
      "outputs": [],
      "source": [
        "# Run evaluation (uses ./lora_adapters if present)\n",
"!bash -lc 'source .venv/bin/activate && uv run --with \"jax[cpu]==0.4.38\" --with \"flax>=0.8.4,<0.9.0\" --with \"optax>=0.2.2,<0.3.0\" python -m src.evaluation.evaluate_helpfulness --config configs/base_config.yaml'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Interactive demo (Gradio)\n",
        "Launch a lightweight UI with share link to compare base vs fine-tuned and see safety overlay.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "demo"
      },
      "outputs": [],
      "source": [
        "#@title Launch demo (optional)\n",
        "USE_DEMO = False  #@param {type:\"boolean\"}\n",
        "if USE_DEMO:\n",
        "    import gradio as gr\n",
        "    from src.inference.generate import GemmaInference\n",
        "    from src.utils.safety_integration import SafetyFilter\n",
        "    base = GemmaInference('google/gemma-7b-it', adapter_path=None, load_in_4bit=True)\n",
        "    ft   = GemmaInference('google/gemma-7b-it', adapter_path='./lora_adapters', load_in_4bit=True)\n",
        "    safety = SafetyFilter(\n",
        "        classifier_config_path='../safety-text-classifier/configs/base_config.yaml',\n",
        "        checkpoint_dir='../safety-text-classifier/checkpoints/best_model',\n",
        "    )\n",
        "    def compare(prompt, temperature, top_p, max_length, safety_threshold):\n",
        "        b = base.generate(prompt, max_length=max_length, temperature=temperature, top_p=top_p)\n",
        "        f = ft.generate(prompt,   max_length=max_length, temperature=temperature, top_p=top_p)\n",
        "        bs = safety.score_text(b)\n",
        "        fs = safety.score_text(f)\n",
        "        bf = 'ðŸŸ¢' if bs >= safety_threshold else 'ðŸ”´'\n",
        "        ff = 'ðŸŸ¢' if fs >= safety_threshold else 'ðŸ”´'\n",
        "        return b, f, f\"{bf} Safety: {bs:.2f}\", f\"{ff} Safety: {fs:.2f}\"\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown('## Stage 2: Base vs Fine-tuned (Gemma-7B-IT + QLoRA)')\n",
        "        prompt = gr.Textbox(label='Prompt', lines=4)\n",
        "        with gr.Row():\n",
        "            temperature = gr.Slider(0.1, 1.0, value=0.7, step=0.05, label='Temperature')\n",
        "            top_p       = gr.Slider(0.1, 1.0, value=0.9, step=0.05, label='Top-p')\n",
        "            max_length  = gr.Slider(64, 1024, value=512, step=16, label='Max length')\n",
        "            safety_thr  = gr.Slider(0.0, 1.0, value=0.8, step=0.05, label='Safety threshold')\n",
        "        go = gr.Button('Generate')\n",
        "        base_out = gr.Textbox(label='Base response', lines=10)\n",
        "        ft_out   = gr.Textbox(label='Fine-tuned response', lines=10)\n",
        "        base_s   = gr.Label(label='Base safety')\n",
        "        ft_s     = gr.Label(label='Fine-tuned safety')\n",
        "        go.click(compare, inputs=[prompt, temperature, top_p, max_length, safety_thr], outputs=[base_out, ft_out, base_s, ft_s])\n",
        "    app.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
