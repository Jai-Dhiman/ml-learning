# Test Configuration for Quick Training Verification
model:
  name: "safety_transformer"
  vocab_size: 32000
  embedding_dim: 768
  num_layers: 6
  num_heads: 12
  feedforward_dim: 3072
  max_sequence_length: 512
  dropout_rate: 0.1
  num_classes: 4

training:
  batch_size: 4  # Smaller batch for testing
  learning_rate: 0.00002
  warmup_steps: 10
  max_steps: 50  # Just 50 steps for testing
  eval_every: 25
  save_every: 50
  gradient_clip_norm: 1.0
  optimizer: "adamw"
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999
  schedule: "cosine_with_warmup"
  min_lr_ratio: 0.1

data:
  datasets: []  # Use only synthetic data for testing
  max_length: 512
  tokenizer: "sentence-transformers/all-MiniLM-L6-v2"
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  text_augmentation: false
  augmentation_prob: 0.1

evaluation:
  metrics:
    - "accuracy"
    - "precision" 
    - "recall"
    - "f1"
  fairness_metrics: []
  robustness_tests: []

logging:
  wandb:
    project: "test-run"
    entity: "jaidhiman-capture"
    tags: ["test"]
  log_level: "INFO"
  log_dir: "logs"

paths:
  data_dir: "data"
  model_dir: "models"
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  cache_dir: ".cache"