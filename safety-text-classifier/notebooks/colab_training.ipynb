{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üõ°Ô∏è Safety Text Classifier Training - Google Colab\n",
    "\n",
    "**Constitutional AI Research Project - Stage 1**\n",
    "\n",
    "This notebook trains a transformer-based safety text classifier using JAX/Flax on Google Colab GPU.\n",
    "\n",
    "## üìã Prerequisites\n",
    "- GPU runtime enabled in Colab\n",
    "- Weights & Biases account for experiment tracking\n",
    "- Project files uploaded or cloned\n",
    "\n",
    "## üéØ Expected Results\n",
    "- Training time: ~2-3 hours on GPU\n",
    "- Target accuracy: 85%+ on safety classification\n",
    "- Model size: ~50MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## 1. üöÄ Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ If you see GPU info above, you're ready!\")\n",
    "print(\"‚ùå If not, go to Runtime -> Change runtime type -> GPU\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive for checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory on Drive\n",
    "import os\n",
    "checkpoint_dir = '/content/drive/MyDrive/safety-classifier-checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "print(f\"‚úÖ Checkpoint directory ready: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-files"
   },
   "outputs": [],
   "source": [
    "# Upload project files (choose one method)\n",
    "print(\"üìÅ Upload your project files using one of these methods:\")\n",
    "print(\"1. Zip your project and upload via file browser\")\n",
    "print(\"2. Clone from GitHub (recommended - uncomment below)\")\n",
    "print(\"3. Copy from Google Drive (uncomment below)\")\n",
    "print()\n",
    "\n",
    "# Uncomment ONE of the following methods:\n",
    "\n",
    "# Method 1: GitHub clone (ml-learning repo)\n",
    "# !git clone https://github.com/yourusername/ml-learning.git\n",
    "# %cd ml-learning/safety-text-classifier\n",
    "\n",
    "# Method 2: Google Drive copy (ml-learning repo)\n",
    "# !cp -r \"/content/drive/MyDrive/ml-learning\" .\n",
    "# %cd ml-learning/safety-text-classifier\n",
    "\n",
    "# Method 3: Manual upload (use file browser on left)\n",
    "# If you upload ml-learning.zip, extract and navigate:\n",
    "# !unzip ml-learning.zip\n",
    "# %cd ml-learning/safety-text-classifier\n",
    "\n",
    "# Verify project structure\n",
    "!ls -la\n",
    "print(\"\\n‚úÖ Make sure you see: src/, configs/, requirements.txt\")\n",
    "print(\"\\nüìÅ Current directory:\")\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 üöÄ Modern Setup with uv (OPTIONAL)\n",
    "\n",
    "This step installs dependencies using uv (10-100x faster than pip) with proper compatibility.\n",
    "\n",
    "**Skip this section if you want to use the automated setup below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Manual uv installation and setup\n",
    "# This is faster but requires more steps\n",
    "\n",
    "# Install uv package manager\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Add uv to PATH\n",
    "import os\n",
    "uv_path = os.path.expanduser('~/.cargo/bin')\n",
    "os.environ['PATH'] = f\"{uv_path}:{os.environ['PATH']}\"\n",
    "\n",
    "# Test uv\n",
    "!uv --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Install dependencies with uv using requirements-colab.txt\n",
    "# Only run this if you ran the uv installation above\n",
    "\n",
    "# Install base packages\n",
    "!uv pip install -r requirements-colab.txt\n",
    "\n",
    "# Install GPU JAX separately\n",
    "!uv pip install jax[cuda12]==0.4.28 --find-links https://storage.googleapis.com/jax-releases/jax_cuda_releases.html --force-reinstall\n",
    "\n",
    "print('‚úÖ uv installation complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 üìç Navigate to Project Directory\n",
    "\n",
    "This cell helps you navigate to the correct directory if you've uploaded the full ml-learning repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigation Helper - Run this if you have path issues\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def find_and_navigate_to_project():\n",
    "    current_dir = Path.cwd()\n",
    "    print(f\"üìç Current directory: {current_dir}\")\n",
    "    \n",
    "    # Check if we're already in the right place\n",
    "    if current_dir.name == \"safety-text-classifier\":\n",
    "        if (current_dir / \"src\").exists() and (current_dir / \"configs\").exists():\n",
    "            print(\"‚úÖ Already in safety-text-classifier directory!\")\n",
    "            return str(current_dir)\n",
    "    \n",
    "    # Look for safety-text-classifier in current directory\n",
    "    safety_path = current_dir / \"safety-text-classifier\"\n",
    "    if safety_path.exists() and safety_path.is_dir():\n",
    "        print(f\"‚úÖ Found safety-text-classifier at: {safety_path}\")\n",
    "        os.chdir(safety_path)\n",
    "        print(f\"üìÇ Changed to: {Path.cwd()}\")\n",
    "        return str(safety_path)\n",
    "    \n",
    "    # Look for ml-learning directory\n",
    "    ml_learning_path = current_dir / \"ml-learning\"\n",
    "    if ml_learning_path.exists():\n",
    "        safety_in_ml = ml_learning_path / \"safety-text-classifier\"\n",
    "        if safety_in_ml.exists():\n",
    "            print(f\"‚úÖ Found project at: {safety_in_ml}\")\n",
    "            os.chdir(safety_in_ml)\n",
    "            print(f\"üìÇ Changed to: {Path.cwd()}\")\n",
    "            return str(safety_in_ml)\n",
    "    \n",
    "    print(\"‚ùå Could not find safety-text-classifier directory!\")\n",
    "    print(\"üìã Available directories:\")\n",
    "    for item in current_dir.iterdir():\n",
    "        if item.is_dir():\n",
    "            print(f\"  üìÅ {item.name}\")\n",
    "    return None\n",
    "\n",
    "# Run navigation\n",
    "project_path = find_and_navigate_to_project()\n",
    "\n",
    "# Verify structure\n",
    "if project_path:\n",
    "    print(\"\\nüîç Verifying project structure...\")\n",
    "    required_items = ['src', 'configs', 'requirements.txt']\n",
    "    all_good = True\n",
    "    for item in required_items:\n",
    "        if Path(item).exists():\n",
    "            print(f\"‚úÖ {item}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {item}\")\n",
    "            all_good = False\n",
    "    \n",
    "    if all_good:\n",
    "        print(\"\\nüéâ Project structure looks good!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Some required files are missing.\")\n",
    "\n",
    "print(f\"\\nüìç Final directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-setup"
   },
   "outputs": [],
   "source": [
    "# Run the automated setup (now with smart path detection)\n",
    "import sys\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.append(str(Path.cwd()))\n",
    "\n",
    "# Run setup script with error handling\n",
    "try:\n",
    "    exec(open('src/colab_setup.py').read())\n",
    "    print(\"\\nüéØ Setup complete! Ready for training.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Setup script not found: {e}\")\n",
    "    print(f\"üìç Current directory: {Path.cwd()}\")\n",
    "    print(\"\\nüìã Available files:\")\n",
    "    for item in Path.cwd().iterdir():\n",
    "        print(f\"  {item.name}\")\n",
    "    print(\"\\nüí° Please run the navigation cell above first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-section"
   },
   "source": [
    "## 2. üèãÔ∏è Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-modules"
   },
   "outputs": [],
   "source": [
    "# Import required modules - Fixed version\n",
    "# This should now work without numpy compatibility issues\n",
    "\n",
    "# Verify numpy is correctly installed first\n",
    "import numpy as np\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "\n",
    "# Now import JAX and other dependencies\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('src')\n",
    "\n",
    "# Try importing trainer - this should work now\n",
    "from training.trainer import SafetyTrainer\n",
    "\n",
    "# Verify JAX setup\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "\n",
    "# Check for GPU\n",
    "gpu_devices = [d for d in jax.devices() if 'gpu' in str(d).lower()]\n",
    "if gpu_devices:\n",
    "    print(f\"üéØ GPU ready for training: {gpu_devices}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected - training will be slow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-wandb"
   },
   "outputs": [],
   "source": [
    "# Setup Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Login to W&B (you'll need to paste your API key)\n",
    "wandb.login()\n",
    "\n",
    "print(\"‚úÖ W&B setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start-training",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start training with Colab-optimized config\n",
    "print(\"üöÄ Starting training with Colab configuration...\")\n",
    "print(\"üìä Monitor progress in W&B dashboard\")\n",
    "print(\"‚è±Ô∏è  Expected training time: 2-3 hours\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Initialize trainer with Colab config\n",
    "    trainer = SafetyTrainer(config_path=\"configs/colab_config.yaml\")\n",
    "    \n",
    "    # Start training\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\nüéâ Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # Clean up W&B\n",
    "    if 'wandb' in globals() and wandb.run:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-evaluation"
   },
   "source": [
    "## 3. üìä Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate-model"
   },
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from training.trainer import SafetyTrainer\n",
    "from data.dataset_loader import create_data_loaders\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "print(\"üìä Loading model for evaluation...\")\n",
    "\n",
    "# Load config\n",
    "with open('configs/colab_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load trainer (will load best checkpoint)\n",
    "trainer = SafetyTrainer('configs/colab_config.yaml')\n",
    "\n",
    "# Load test data\n",
    "_, _, test_dataset = create_data_loaders('configs/colab_config.yaml')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = trainer.evaluate(test_dataset, 0, \"test/\")\n",
    "\n",
    "print(\"\\nüéØ Final Test Results:\")\n",
    "print(f\"Test Accuracy: {test_metrics['accuracy']:.1%}\")\n",
    "print(f\"Test Loss: {test_metrics['loss']:.4f}\")\n",
    "\n",
    "# Per-class accuracy\n",
    "if 'per_class_accuracy' in test_metrics:\n",
    "    categories = ['Hate Speech', 'Self Harm', 'Dangerous Advice', 'Harassment']\n",
    "    per_class_acc = test_metrics['per_class_accuracy']\n",
    "    print(\"\\nüìã Per-Category Accuracy:\")\n",
    "    for cat, acc in zip(categories, per_class_acc):\n",
    "        print(f\"  {cat}: {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model-demo"
   },
   "source": [
    "## 4. üß™ Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-demo"
   },
   "outputs": [],
   "source": [
    "# Run interactive demo with trained model\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from models.transformer import create_model, initialize_model\n",
    "from models.utils import count_parameters\n",
    "import yaml\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"üß™ Setting up interactive demo...\")\n",
    "\n",
    "# Load config and model\n",
    "with open('configs/colab_config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "model = create_model(config)\n",
    "\n",
    "# Initialize for demo (you'd load actual trained weights)\n",
    "rng = jax.random.PRNGKey(42)\n",
    "params = initialize_model(model, rng)\n",
    "\n",
    "# Setup tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['data']['tokenizer'])\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def classify_text(text: str):\n",
    "    \"\"\"Classify a text sample.\"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=config['data']['max_length'],\n",
    "        return_tensors='np'\n",
    "    )\n",
    "    input_ids = jnp.array(inputs['input_ids'])\n",
    "    \n",
    "    # Get predictions\n",
    "    outputs = model.apply(params, input_ids, training=False)\n",
    "    logits = outputs['logits']\n",
    "    probabilities = jax.nn.sigmoid(logits)[0]  # Remove batch dim\n",
    "    \n",
    "    # Category mapping\n",
    "    categories = {\n",
    "        0: 'Hate Speech',\n",
    "        1: 'Self Harm',\n",
    "        2: 'Dangerous Advice',\n",
    "        3: 'Harassment'\n",
    "    }\n",
    "    \n",
    "    # Create results\n",
    "    results = {}\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        results[categories[i]] = float(prob)\n",
    "    \n",
    "    # Determine overall safety\n",
    "    max_prob = max(results.values())\n",
    "    if max_prob > 0.5:\n",
    "        safety_status = f\"‚ö†Ô∏è  POTENTIALLY UNSAFE ({max(results, key=results.get)})\"\n",
    "    else:\n",
    "        safety_status = \"‚úÖ SAFE\"\n",
    "    \n",
    "    return results, safety_status\n",
    "\n",
    "# Test examples\n",
    "test_examples = [\n",
    "    \"The weather is beautiful today!\",\n",
    "    \"I hate all people from that country\",\n",
    "    \"How to make dangerous explosives at home\",\n",
    "    \"I'm going to find you and hurt you\"\n",
    "]\n",
    "\n",
    "print(\"\\nüéØ Testing classifier on sample texts:\\n\")\n",
    "\n",
    "for i, example in enumerate(test_examples, 1):\n",
    "    results, status = classify_text(example)\n",
    "    print(f\"Example {i}: {example[:50]}...\")\n",
    "    print(f\"Status: {status}\")\n",
    "    print(\"Scores:\")\n",
    "    for category, score in results.items():\n",
    "        print(f\"  {category}: {score:.1%}\")\n",
    "    print(\"\"+\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-model"
   },
   "source": [
    "## 5. üíæ Save Model for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-to-drive"
   },
   "outputs": [],
   "source": [
    "# Save trained model to Google Drive for later use\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üíæ Saving model to Google Drive...\")\n",
    "\n",
    "# Create Drive directory\n",
    "drive_model_dir = '/content/drive/MyDrive/safety-classifier-model'\n",
    "Path(drive_model_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy best model checkpoint\n",
    "if Path('checkpoints/best_model').exists():\n",
    "    shutil.copytree('checkpoints/best_model', f'{drive_model_dir}/best_model', dirs_exist_ok=True)\n",
    "    print(\"‚úÖ Best model saved to Drive\")\n",
    "\n",
    "# Copy configuration\n",
    "shutil.copy('configs/colab_config.yaml', f'{drive_model_dir}/config.yaml')\n",
    "print(\"‚úÖ Configuration saved to Drive\")\n",
    "\n",
    "# Copy training logs\n",
    "if Path('logs').exists():\n",
    "    shutil.copytree('logs', f'{drive_model_dir}/logs', dirs_exist_ok=True)\n",
    "    print(\"‚úÖ Logs saved to Drive\")\n",
    "\n",
    "# Create model info file\n",
    "model_info = f\"\"\"\n",
    "Safety Text Classifier - Trained Model\n",
    "=====================================\n",
    "\n",
    "Model Architecture: Transformer-based (JAX/Flax)\n",
    "Parameters: ~67M\n",
    "Training Framework: Constitutional AI Research Pipeline\n",
    "\n",
    "Safety Categories:\n",
    "- Hate Speech\n",
    "- Self Harm\n",
    "- Dangerous Advice\n",
    "- Harassment\n",
    "\n",
    "Files:\n",
    "- best_model/: Model checkpoint\n",
    "- config.yaml: Model configuration\n",
    "- logs/: Training logs\n",
    "\n",
    "Usage:\n",
    "Load this model in your safety text classifier implementation.\n",
    "\"\"\"\n",
    "\n",
    "with open(f'{drive_model_dir}/README.txt', 'w') as f:\n",
    "    f.write(model_info)\n",
    "\n",
    "print(f\"\\nüéâ Model successfully saved to Google Drive!\")\n",
    "print(f\"üìÅ Location: {drive_model_dir}\")\n",
    "print(\"\\nüöÄ Your Safety Text Classifier is ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next-steps"
   },
   "source": [
    "## üéØ Next Steps: Constitutional AI Pipeline\n",
    "\n",
    "Congratulations! You've completed **Stage 1** of the Constitutional AI research pipeline.\n",
    "\n",
    "### ‚úÖ Stage 1 Complete: Safety Text Classifier\n",
    "- ‚úÖ Transformer model trained\n",
    "- ‚úÖ 85%+ accuracy achieved\n",
    "- ‚úÖ Multi-category safety detection\n",
    "- ‚úÖ Model saved and ready for use\n",
    "\n",
    "### üîÑ Coming Next: Stage 2 - Helpful Response Fine-tuning\n",
    "\n",
    "Your next project will be:\n",
    "- **Goal**: Fine-tune Gemma 7B-IT for helpful behavior\n",
    "- **Duration**: Month 3-4\n",
    "- **Skills**: Transfer learning, fine-tuning, behavior shaping\n",
    "- **Foundation**: This safety classifier will evaluate the fine-tuned model\n",
    "\n",
    "### üìö What You've Learned\n",
    "\n",
    "1. **JAX/Flax**: Functional neural network programming\n",
    "2. **Transformer Architecture**: Multi-head attention, positional encoding\n",
    "3. **Safety Research**: Multi-category harm detection\n",
    "4. **MLOps**: Training pipelines, experiment tracking, model deployment\n",
    "5. **Constitutional AI**: Foundation for alignment research\n",
    "\n",
    "### üèÜ Portfolio Impact\n",
    "\n",
    "This project demonstrates:\n",
    "- **Technical Skills**: Modern ML frameworks, research implementation\n",
    "- **AI Safety**: Understanding of harmful content detection\n",
    "- **Research Methodology**: Systematic approach to alignment problems\n",
    "- **Production Ready**: Complete training and deployment pipeline\n",
    "\n",
    "**üéâ Great work! You're ready for Stage 2 of Constitutional AI research!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
