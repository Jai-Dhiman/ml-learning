{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# 🛡️ Safety Text Classifier Training - Google Colab\n",
    "\n",
    "**Constitutional AI Research Project - Stage 1**\n",
    "\n",
    "This notebook trains a transformer-based safety text classifier using JAX/Flax on Google Colab GPU.\n",
    "\n",
    "## 📋 Prerequisites\n",
    "- GPU runtime enabled in Colab\n",
    "- Weights & Biases account for experiment tracking\n",
    "- Project files uploaded or cloned\n",
    "\n",
    "## 🎯 Expected Results\n",
    "- Training time: ~2-3 hours on GPU\n",
    "- Target accuracy: 85%+ on safety classification\n",
    "- Model size: ~50MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. 🚀 Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories for checkpoints and models\n",
    "checkpoint_dir = '/content/drive/MyDrive/safety-classifier-checkpoints'\n",
    "model_dir = '/content/drive/MyDrive/safety-classifier-models'\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(f\"✅ Checkpoint directory: {checkpoint_dir}\")\n",
    "print(f\"✅ Model directory: {model_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-uv"
   },
   "outputs": [],
   "source": [
    "# Install uv package manager (10-100x faster than pip)\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Add uv to PATH\n",
    "import os\n",
    "uv_path = os.path.expanduser('~/.cargo/bin')\n",
    "os.environ['PATH'] = f\"{uv_path}:{os.environ['PATH']}\"\n",
    "\n",
    "# Test uv\n",
    "!uv --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "navigate-project"
   },
   "outputs": [],
   "source": [
    "# Navigate to project directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def find_and_navigate_to_project():\n",
    "    \"\"\"Find and navigate to the safety-text-classifier project directory.\"\"\"\n",
    "    current_dir = Path.cwd()\n",
    "    print(f\"📍 Starting from: {current_dir}\")\n",
    "    \n",
    "    # Check if we're already in the right place\n",
    "    if current_dir.name == \"safety-text-classifier\" and (current_dir / \"src\").exists():\n",
    "        print(\"✅ Already in safety-text-classifier directory!\")\n",
    "        return str(current_dir)\n",
    "    \n",
    "    # Common search paths\n",
    "    search_paths = [\n",
    "        current_dir / \"safety-text-classifier\",\n",
    "        current_dir / \"ml-learning\" / \"safety-text-classifier\"\n",
    "    ]\n",
    "    \n",
    "    for path in search_paths:\n",
    "        if path.exists() and (path / \"src\").exists():\n",
    "            print(f\"✅ Found project at: {path}\")\n",
    "            os.chdir(path)\n",
    "            return str(path)\n",
    "    \n",
    "    print(\"❌ Could not find safety-text-classifier directory!\")\n",
    "    print(\"📋 Available directories:\")\n",
    "    for item in current_dir.iterdir():\n",
    "        if item.is_dir() and not item.name.startswith('.'):\n",
    "            print(f\"  📁 {item.name}\")\n",
    "    return None\n",
    "\n",
    "# Navigate to project\n",
    "project_path = find_and_navigate_to_project()\n",
    "\n",
    "if project_path:\n",
    "    # Verify structure\n",
    "    required_items = ['src', 'configs', 'pyproject.toml']\n",
    "    missing = [item for item in required_items if not Path(item).exists()]\n",
    "    \n",
    "    if not missing:\n",
    "        print(\"\\n🎉 Project structure verified!\")\n",
    "        print(f\"📁 Working directory: {Path.cwd()}\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Missing: {missing}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Please upload the safety-text-classifier folder to Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 2. 🏋️ Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-imports"
   },
   "outputs": [],
   "source": [
    "# Setup Python path and test imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to Python path\n",
    "src_path = str(Path.cwd() / \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "    print(f\"✅ Added to Python path: {src_path}\")\n",
    "\n",
    "# Test imports\n",
    "try:\n",
    "    from training.trainer import SafetyTrainer\n",
    "    from data.dataset_loader import create_data_loaders\n",
    "    print(\"✅ Successfully imported training modules\")\n",
    "    \n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    print(f\"✅ JAX {jax.__version__} ready\")\n",
    "    print(f\"🎯 Available devices: {jax.devices()}\")\n",
    "    \n",
    "    # Check for GPU\n",
    "    gpu_devices = [d for d in jax.devices() if 'gpu' in str(d).lower()]\n",
    "    if gpu_devices:\n",
    "        print(f\"🚀 GPU acceleration ready: {len(gpu_devices)} GPU(s)\")\n",
    "    else:\n",
    "        print(\"⚠️ No GPU detected - training will be slower\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"\\n🔧 Troubleshooting:\")\n",
    "    print(f\"  - Current directory: {os.getcwd()}\")\n",
    "    print(f\"  - Python path: {sys.path[:3]}...\")\n",
    "    print(f\"  - Contents of src/: {os.listdir('src') if os.path.exists('src') else 'src not found'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup-wandb"
   },
   "outputs": [],
   "source": [
    "# Setup Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Login to W&B\n",
    "wandb.login()\n",
    "\n",
    "print(\"✅ W&B authentication complete\")\n",
    "print(\"📊 Training metrics will be logged to W&B dashboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-model"
   },
   "outputs": [],
   "source": [
    "# Start training with enhanced model saving\n",
    "print(\"🚀 Starting training with Colab configuration...\")\n",
    "print(\"📊 Monitor progress in W&B dashboard\")\n",
    "print(\"⏱️  Expected training time: 2-3 hours\")\n",
    "print(\"💾 Model will be saved to Google Drive and W&B\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Initialize trainer with Colab config\n",
    "    trainer = SafetyTrainer(config_path=\"configs/colab_config.yaml\")\n",
    "    \n",
    "    # Update config to use Drive paths\n",
    "    trainer.config['paths']['checkpoint_dir'] = '/content/drive/MyDrive/safety-classifier-checkpoints'\n",
    "    trainer.config['paths']['model_dir'] = '/content/drive/MyDrive/safety-classifier-models'\n",
    "    \n",
    "    # Start training\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\n🎉 Training completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # Clean up W&B\n",
    "    if 'wandb' in globals() and wandb.run:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 3. 📊 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate-model"
   },
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "import yaml\n",
    "\n",
    "print(\"📊 Evaluating trained model...\")\n",
    "\n",
    "try:\n",
    "    # Load trainer and checkpoint\n",
    "    trainer = SafetyTrainer('configs/colab_config.yaml')\n",
    "    \n",
    "    # Update paths to use Drive\n",
    "    trainer.config['paths']['checkpoint_dir'] = '/content/drive/MyDrive/safety-classifier-checkpoints'\n",
    "    \n",
    "    # Load the best checkpoint\n",
    "    trainer.load_checkpoint()\n",
    "    \n",
    "    # Load test data\n",
    "    _, _, test_dataset = create_data_loaders('configs/colab_config.yaml')\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_metrics = trainer.evaluate(test_dataset, 0, \"test/\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎯 FINAL TEST RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Test Accuracy: {test_metrics['accuracy']:.1%}\")\n",
    "    print(f\"Test Loss: {test_metrics['loss']:.4f}\")\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    if 'per_class_accuracy' in test_metrics:\n",
    "        categories = ['Hate Speech', 'Self Harm', 'Dangerous Advice', 'Harassment']\n",
    "        per_class_acc = test_metrics['per_class_accuracy']\n",
    "        print(\"\\n📋 Per-Category Accuracy:\")\n",
    "        for cat, acc in zip(categories, per_class_acc):\n",
    "            print(f\"  {cat}: {acc:.1%}\")\n",
    "    \n",
    "    # Check Stage 1 completion\n",
    "    accuracy = float(test_metrics['accuracy'])\n",
    "    target_accuracy = 0.85\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎖️  STAGE 1 STATUS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if accuracy >= target_accuracy:\n",
    "        print(f\"✅ STAGE 1 COMPLETE! ({accuracy:.1%} ≥ {target_accuracy:.0%})\")\n",
    "        print(\"🎉 Ready to proceed to Stage 2: Helpful Response Fine-tuning\")\n",
    "    else:\n",
    "        print(f\"❌ Stage 1 incomplete ({accuracy:.1%} < {target_accuracy:.0%})\")\n",
    "        print(\"📝 Consider adjusting hyperparameters or training longer\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Evaluation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-and-download"
   },
   "source": [
    "## 4. 💾 Save and Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-to-wandb"
   },
   "outputs": [],
   "source": [
    "# Save model to W&B artifacts\n",
    "import wandb\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"💾 Saving trained model to W&B artifacts...\")\n",
    "\n",
    "try:\n",
    "    # Initialize W&B run for model saving\n",
    "    wandb.init(\n",
    "        project=\"constitutional-ai-research-colab\",\n",
    "        job_type=\"model-save\",\n",
    "        name=\"model-artifacts\"\n",
    "    )\n",
    "    \n",
    "    # Create model artifact\n",
    "    model_artifact = wandb.Artifact(\n",
    "        name=\"safety-text-classifier\",\n",
    "        type=\"model\",\n",
    "        description=\"Trained safety text classifier - Stage 1 complete\"\n",
    "    )\n",
    "    \n",
    "    # Add checkpoint directory to artifact\n",
    "    checkpoint_dir = '/content/drive/MyDrive/safety-classifier-checkpoints'\n",
    "    if os.path.exists(checkpoint_dir) and os.listdir(checkpoint_dir):\n",
    "        model_artifact.add_dir(checkpoint_dir, name=\"checkpoints\")\n",
    "        print(f\"✅ Added checkpoints from {checkpoint_dir}\")\n",
    "    else:\n",
    "        print(f\"⚠️  No checkpoints found at {checkpoint_dir}\")\n",
    "    \n",
    "    # Add config file\n",
    "    if os.path.exists('configs/colab_config.yaml'):\n",
    "        model_artifact.add_file('configs/colab_config.yaml', name=\"config.yaml\")\n",
    "        print(\"✅ Added configuration file\")\n",
    "    \n",
    "    # Log the artifact\n",
    "    wandb.log_artifact(model_artifact)\n",
    "    \n",
    "    print(f\"🎉 Model saved to W&B! Artifact: {model_artifact.name}:{model_artifact.version}\")\n",
    "    print(f\"🔗 View at: https://wandb.ai/{wandb.run.entity}/{wandb.run.project}/artifacts\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ W&B save failed: {e}\")\n",
    "    \n",
    "finally:\n",
    "    if wandb.run:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-model"
   },
   "outputs": [],
   "source": [
    "# Create downloadable model package\n",
    "import zipfile\n",
    "import os\n",
    "from google.colab import files\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"📦 Creating downloadable model package...\")\n",
    "\n",
    "try:\n",
    "    # Create package directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    package_name = f\"safety_classifier_{timestamp}\"\n",
    "    package_dir = f\"/content/{package_name}\"\n",
    "    \n",
    "    os.makedirs(package_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy checkpoints\n",
    "    checkpoint_source = '/content/drive/MyDrive/safety-classifier-checkpoints'\n",
    "    checkpoint_dest = os.path.join(package_dir, 'checkpoints')\n",
    "    \n",
    "    if os.path.exists(checkpoint_source) and os.listdir(checkpoint_source):\n",
    "        shutil.copytree(checkpoint_source, checkpoint_dest)\n",
    "        print(f\"✅ Copied checkpoints to package\")\n",
    "    else:\n",
    "        print(f\"⚠️  No checkpoints found to package\")\n",
    "    \n",
    "    # Copy essential files\n",
    "    files_to_copy = [\n",
    "        'configs/colab_config.yaml',\n",
    "        'src/training/trainer.py',\n",
    "        'src/models/transformer.py',\n",
    "        'src/data/dataset_loader.py'\n",
    "    ]\n",
    "    \n",
    "    for file_path in files_to_copy:\n",
    "        if os.path.exists(file_path):\n",
    "            dest_path = os.path.join(package_dir, file_path)\n",
    "            os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "            shutil.copy2(file_path, dest_path)\n",
    "            print(f\"✅ Copied {file_path}\")\n",
    "    \n",
    "    # Create README\n",
    "    readme_content = f\"\"\"# Safety Text Classifier - Trained Model\n",
    "\n",
    "**Training completed**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "**Stage 1**: Complete\n",
    "\n",
    "## Contents\n",
    "- `checkpoints/`: Model checkpoints\n",
    "- `configs/colab_config.yaml`: Training configuration\n",
    "- `src/`: Essential source code\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from src.training.trainer import SafetyTrainer\n",
    "\n",
    "# Load trained model\n",
    "trainer = SafetyTrainer('configs/colab_config.yaml')\n",
    "trainer.load_checkpoint()\n",
    "```\n",
    "\"\"\"\n",
    "    \n",
    "    with open(os.path.join(package_dir, 'README.md'), 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    # Create zip file\n",
    "    zip_path = f\"/content/{package_name}.zip\"\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(package_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, package_dir)\n",
    "                zipf.write(file_path, arcname)\n",
    "    \n",
    "    # Get zip size\n",
    "    zip_size = os.path.getsize(zip_path) / (1024*1024)\n",
    "    \n",
    "    print(f\"\\n🎉 Model package created successfully!\")\n",
    "    print(f\"📦 Package: {package_name}.zip\")\n",
    "    print(f\"📏 Size: {zip_size:.1f} MB\")\n",
    "    print(f\"\\n📥 Downloading...\")\n",
    "    \n",
    "    # Download the package\n",
    "    files.download(zip_path)\n",
    "    \n",
    "    print(f\"\\n✅ Download complete!\")\n",
    "    print(f\"\\n🔧 To use locally:\")\n",
    "    print(f\"1. Extract the zip file\")\n",
    "    print(f\"2. Update config paths to point to local directories\")\n",
    "    print(f\"3. Run your evaluation script\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Package creation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next-steps"
   },
   "source": [
    "## 🎯 Next Steps\n",
    "\n",
    "With Stage 1 complete, you're ready for:\n",
    "\n",
    "1. **Stage 2**: Helpful Response Fine-tuning\n",
    "2. **Model Deployment**: Set up inference server\n",
    "3. **Demo Creation**: Interactive safety classifier interface\n",
    "\n",
    "### Local Setup\n",
    "After downloading the model package:\n",
    "\n",
    "```bash\n",
    "# Extract model\n",
    "unzip safety_classifier_*.zip\n",
    "cd safety_classifier_*\n",
    "\n",
    "# Update paths in your local config\n",
    "# Then run evaluation\n",
    "python3 evaluate_model.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}