{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "safety-classifier-title"
      },
      "source": [
        "# Safety Text Classifier Training - Constitutional AI Stage 1\n",
        "\n",
        "**Project**: Constitutional AI Research Pipeline  \n",
        "**Stage**: 1 of 4 - Safety Text Classifier Foundation  \n",
        "**Framework**: JAX/Flax with GPU acceleration  \n",
        "**Target**: 85%+ accuracy on 4-class safety classification  \n",
        "\n",
        "This notebook trains a transformer-based safety text classifier to detect:\n",
        "- Hate speech and harassment\n",
        "- Self-harm instructions  \n",
        "- Dangerous advice and misinformation\n",
        "- Toxic and offensive content\n",
        "\n",
        "## ğŸš€ Getting Started\n",
        "1. **Runtime**: Change to GPU (Runtime â†’ Change runtime type â†’ GPU)\n",
        "2. **Execute all cells** to train the model\n",
        "3. **Download trained model** at the end\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-section"
      },
      "source": [
        "## ğŸ“¦ Environment Setup\n",
        "\n",
        "Install compatible JAX/Flax versions and dependencies for Colab GPU training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-dependencies"
      },
      "outputs": [],
      "source": [
        "# Install compatible JAX/Flax for Colab GPU\n",
        "!pip install -q \"jax[cuda12_pip]\" \"flax\" \"optax>=0.1.7\"\n",
        "!pip install -q \"datasets>=2.19.0\" \"transformers>=4.40.0\" \"wandb>=0.17.0\"\n",
        "!pip install -q \"scikit-learn>=1.3.0\" \"matplotlib>=3.7.0\" \"seaborn>=0.12.0\"\n",
        "!pip install -q \"tqdm>=4.66.0\" \"pyyaml>=6.0.0\"\n",
        "# Fix checkpoint compatibility\n",
        "!pip install -q \"orbax-checkpoint<0.6.0\"\n",
        "\n",
        "print(\"âœ… Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "verify-gpu"
      },
      "outputs": [],
      "source": [
        "# Verify GPU and JAX setup\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax\n",
        "import optax\n",
        "\n",
        "print(f\"ğŸ”§ JAX version: {jax.__version__}\")\n",
        "print(f\"ğŸ”§ Flax version: {flax.__version__}\")\n",
        "print(f\"ğŸ”§ JAX devices: {jax.devices()}\")\n",
        "\n",
        "# Test GPU\n",
        "if jax.devices()[0].device_kind == 'gpu':\n",
        "    print(\"ğŸš€ GPU acceleration enabled!\")\n",
        "else:\n",
        "    print(\"âš ï¸  Using CPU (consider enabling GPU runtime)\")\n",
        "\n",
        "# Quick JAX test\n",
        "x = jnp.ones((1000, 1000))\n",
        "result = jnp.dot(x, x)\n",
        "print(f\"âœ… JAX working: {result.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project-setup"
      },
      "source": [
        "## ğŸ“ Project Structure Setup\n",
        "\n",
        "Create the necessary directories and configuration files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create-structure"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "# Create project structure\n",
        "directories = ['configs', 'checkpoints', 'logs', 'data', 'src/models', 'src/data', 'src/training']\n",
        "for directory in directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "print(\"ğŸ“ Project structure created!\")\n",
        "print(\"\\nğŸ“‹ Directories:\")\n",
        "for directory in directories:\n",
        "    print(f\"   âœ… {directory}/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create-config"
      },
      "outputs": [],
      "source": [
        "# Create Colab-optimized configuration\n",
        "config = {\n",
        "    'model': {\n",
        "        'name': 'safety_transformer',\n",
        "        'vocab_size': 32000,\n",
        "        'embedding_dim': 512,  # Smaller for Colab\n",
        "        'num_layers': 4,       # Reduced layers\n",
        "        'num_heads': 8,\n",
        "        'feedforward_dim': 2048,\n",
        "        'max_sequence_length': 256,  # Shorter sequences\n",
        "        'dropout_rate': 0.1,\n",
        "        'num_classes': 4\n",
        "    },\n",
        "    'training': {\n",
        "        'batch_size': 16,      # Colab-friendly batch size\n",
        "        'learning_rate': 0.0001,\n",
        "        'warmup_steps': 500,\n",
        "        'max_steps': 3000,     # Shorter training for demo\n",
        "        'eval_every': 500,\n",
        "        'save_every': 1000,\n",
        "        'gradient_clip_norm': 1.0,\n",
        "        'optimizer': 'adamw',\n",
        "        'weight_decay': 0.01,\n",
        "        'beta1': 0.9,\n",
        "        'beta2': 0.999,\n",
        "        'schedule': 'cosine_with_warmup',\n",
        "        'min_lr_ratio': 0.1\n",
        "    },\n",
        "    'data': {\n",
        "        'datasets': [\n",
        "            {'name': 'lmsys/toxic-chat', 'config': 'toxicchat0124'}\n",
        "        ],\n",
        "        'max_length': 256,\n",
        "        'tokenizer': 'sentence-transformers/all-MiniLM-L6-v2',\n",
        "        'train_split': 0.8,\n",
        "        'val_split': 0.1,\n",
        "        'test_split': 0.1,\n",
        "        'text_augmentation': True,\n",
        "        'augmentation_prob': 0.1\n",
        "    },\n",
        "    'logging': {\n",
        "        'wandb': {\n",
        "            'project': 'constitutional-ai-colab',\n",
        "            'entity': None,\n",
        "            'tags': ['stage1', 'safety', 'colab', 'gpu']\n",
        "        },\n",
        "        'log_level': 'INFO',\n",
        "        'log_dir': 'logs'\n",
        "    },\n",
        "    'paths': {\n",
        "        'data_dir': 'data',\n",
        "        'checkpoint_dir': 'checkpoints',\n",
        "        'log_dir': 'logs'\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save configuration\n",
        "with open('configs/colab_config.yaml', 'w') as f:\n",
        "    yaml.dump(config, f, default_flow_style=False)\n",
        "\n",
        "print(\"âœ… Configuration created: configs/colab_config.yaml\")\n",
        "print(f\"ğŸ¯ Target: {config['training']['max_steps']} steps with batch size {config['training']['batch_size']}\")\n",
        "print(f\"ğŸ§  Model: {config['model']['num_layers']} layers, {config['model']['embedding_dim']} dim\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-implementation"
      },
      "source": [
        "## ğŸ—ï¸ Model Architecture\n",
        "\n",
        "Implement the Safety Transformer model using JAX/Flax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "transformer-model"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "from flax.linen import Dense, Dropout, LayerNorm, Embed\n",
        "from typing import Callable, Optional, Tuple, Any\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head attention mechanism.\"\"\"\n",
        "    num_heads: int\n",
        "    head_dim: int\n",
        "    dropout_rate: float = 0.1\n",
        "\n",
        "    def setup(self):\n",
        "        self.dense_q = Dense(self.num_heads * self.head_dim, use_bias=False)\n",
        "        self.dense_k = Dense(self.num_heads * self.head_dim, use_bias=False)\n",
        "        self.dense_v = Dense(self.num_heads * self.head_dim, use_bias=False)\n",
        "        self.dense_output = Dense(self.num_heads * self.head_dim)\n",
        "        self.dropout = Dropout(self.dropout_rate)\n",
        "\n",
        "    def __call__(self, x, mask=None, training=True):\n",
        "        batch_size, seq_len, embed_dim = x.shape\n",
        "        \n",
        "        # Compute queries, keys, values\n",
        "        q = self.dense_q(x)\n",
        "        k = self.dense_k(x)\n",
        "        v = self.dense_v(x)\n",
        "        \n",
        "        # Reshape for multi-head attention\n",
        "        q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        k = k.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        v = v.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        \n",
        "        # Transpose to (batch_size, num_heads, seq_len, head_dim)\n",
        "        q = jnp.transpose(q, (0, 2, 1, 3))\n",
        "        k = jnp.transpose(k, (0, 2, 1, 3))\n",
        "        v = jnp.transpose(v, (0, 2, 1, 3))\n",
        "        \n",
        "        # Compute attention scores\n",
        "        attention_scores = jnp.matmul(q, jnp.transpose(k, (0, 1, 3, 2)))\n",
        "        attention_scores = attention_scores / jnp.sqrt(self.head_dim)\n",
        "        \n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            mask = jnp.expand_dims(mask, axis=1)\n",
        "            mask = jnp.expand_dims(mask, axis=1)\n",
        "            attention_scores = jnp.where(mask, attention_scores, -1e9)\n",
        "        \n",
        "        # Apply softmax\n",
        "        attention_weights = jax.nn.softmax(attention_scores, axis=-1)\n",
        "        attention_weights = self.dropout(attention_weights, deterministic=not training)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        attention_output = jnp.matmul(attention_weights, v)\n",
        "        \n",
        "        # Transpose back and reshape\n",
        "        attention_output = jnp.transpose(attention_output, (0, 2, 1, 3))\n",
        "        attention_output = attention_output.reshape(\n",
        "            batch_size, seq_len, self.num_heads * self.head_dim\n",
        "        )\n",
        "        \n",
        "        # Final linear projection\n",
        "        output = self.dense_output(attention_output)\n",
        "        return output, attention_weights\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
        "    hidden_dim: int\n",
        "    output_dim: int\n",
        "    dropout_rate: float = 0.1\n",
        "\n",
        "    def setup(self):\n",
        "        self.dense1 = Dense(self.hidden_dim)\n",
        "        self.dense2 = Dense(self.output_dim)\n",
        "        self.dropout = Dropout(self.dropout_rate)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        x = self.dense1(x)\n",
        "        x = jax.nn.gelu(x)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Single transformer encoder block.\"\"\"\n",
        "    num_heads: int\n",
        "    head_dim: int\n",
        "    feedforward_dim: int\n",
        "    dropout_rate: float = 0.1\n",
        "\n",
        "    def setup(self):\n",
        "        embed_dim = self.num_heads * self.head_dim\n",
        "        self.attention = MultiHeadAttention(\n",
        "            num_heads=self.num_heads,\n",
        "            head_dim=self.head_dim,\n",
        "            dropout_rate=self.dropout_rate,\n",
        "        )\n",
        "        self.feed_forward = FeedForward(\n",
        "            hidden_dim=self.feedforward_dim,\n",
        "            output_dim=embed_dim,\n",
        "            dropout_rate=self.dropout_rate,\n",
        "        )\n",
        "        self.layer_norm1 = LayerNorm()\n",
        "        self.layer_norm2 = LayerNorm()\n",
        "        self.dropout = Dropout(self.dropout_rate)\n",
        "\n",
        "    def __call__(self, x, mask=None, training=True):\n",
        "        # Multi-head attention with residual connection\n",
        "        attn_output, attn_weights = self.attention(x, mask=mask, training=training)\n",
        "        attn_output = self.dropout(attn_output, deterministic=not training)\n",
        "        x = self.layer_norm1(x + attn_output)\n",
        "        \n",
        "        # Feed-forward with residual connection\n",
        "        ff_output = self.feed_forward(x, training=training)\n",
        "        ff_output = self.dropout(ff_output, deterministic=not training)\n",
        "        x = self.layer_norm2(x + ff_output)\n",
        "        \n",
        "        return x, attn_weights\n",
        "\n",
        "print(\"âœ… Transformer components defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "positional-encoding"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
        "    max_length: int\n",
        "    embed_dim: int\n",
        "\n",
        "    def setup(self):\n",
        "        # Create positional encoding matrix\n",
        "        position = jnp.arange(self.max_length)[:, None]\n",
        "        div_term = jnp.exp(\n",
        "            jnp.arange(0, self.embed_dim, 2) * -(jnp.log(10000.0) / self.embed_dim)\n",
        "        )\n",
        "\n",
        "        pe = jnp.zeros((self.max_length, self.embed_dim))\n",
        "        pe = pe.at[:, ::2].set(jnp.sin(position * div_term))\n",
        "        pe = pe.at[:, 1::2].set(jnp.cos(position * div_term))\n",
        "\n",
        "        self.pe = pe\n",
        "\n",
        "    def __call__(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "        if seq_len > self.max_length:\n",
        "            # Handle sequences longer than max_length\n",
        "            position = jnp.arange(seq_len)[:, None]\n",
        "            div_term = jnp.exp(\n",
        "                jnp.arange(0, self.embed_dim, 2) * -(jnp.log(10000.0) / self.embed_dim)\n",
        "            )\n",
        "            pe = jnp.zeros((seq_len, self.embed_dim))\n",
        "            pe = pe.at[:, ::2].set(jnp.sin(position * div_term))\n",
        "            pe = pe.at[:, 1::2].set(jnp.cos(position * div_term))\n",
        "            return x + pe\n",
        "        else:\n",
        "            return x + self.pe[:seq_len]\n",
        "\n",
        "print(\"âœ… Positional encoding defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "safety-transformer"
      },
      "outputs": [],
      "source": [
        "class SafetyTransformer(nn.Module):\n",
        "    \"\"\"Transformer-based safety text classifier.\"\"\"\n",
        "    vocab_size: int\n",
        "    embedding_dim: int\n",
        "    num_layers: int\n",
        "    num_heads: int\n",
        "    feedforward_dim: int\n",
        "    max_sequence_length: int\n",
        "    num_classes: int\n",
        "    dropout_rate: float = 0.1\n",
        "\n",
        "    def setup(self):\n",
        "        self.head_dim = self.embedding_dim // self.num_heads\n",
        "        assert self.embedding_dim % self.num_heads == 0, \"embedding_dim must be divisible by num_heads\"\n",
        "\n",
        "        # Embedding layers\n",
        "        self.token_embedding = Embed(num_embeddings=self.vocab_size, features=self.embedding_dim)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            max_length=self.max_sequence_length, embed_dim=self.embedding_dim\n",
        "        )\n",
        "\n",
        "        # Transformer layers\n",
        "        self.transformer_blocks = [\n",
        "            TransformerBlock(\n",
        "                num_heads=self.num_heads,\n",
        "                head_dim=self.head_dim,\n",
        "                feedforward_dim=self.feedforward_dim,\n",
        "                dropout_rate=self.dropout_rate,\n",
        "            )\n",
        "            for _ in range(self.num_layers)\n",
        "        ]\n",
        "\n",
        "        # Classification head\n",
        "        self.layer_norm = LayerNorm()\n",
        "        self.dropout = Dropout(self.dropout_rate)\n",
        "        self.classifier = Dense(self.num_classes)\n",
        "\n",
        "    def create_attention_mask(self, input_ids):\n",
        "        \"\"\"Create attention mask from input_ids (assuming 0 is padding token).\"\"\"\n",
        "        return input_ids != 0\n",
        "\n",
        "    def __call__(self, input_ids, training=True):\n",
        "        \"\"\"Forward pass of the safety transformer.\"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        # Create attention mask\n",
        "        attention_mask = self.create_attention_mask(input_ids)\n",
        "\n",
        "        # Token embeddings\n",
        "        x = self.token_embedding(input_ids)\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        attention_weights = []\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x, attn_weights = transformer_block(x, mask=attention_mask, training=training)\n",
        "            attention_weights.append(attn_weights)\n",
        "\n",
        "        # Apply final layer norm\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # Global average pooling over sequence dimension\n",
        "        mask_expanded = jnp.expand_dims(attention_mask, axis=-1)\n",
        "        x_masked = x * mask_expanded\n",
        "        seq_lengths = jnp.sum(attention_mask, axis=1, keepdims=True)\n",
        "        pooled = jnp.sum(x_masked, axis=1) / jnp.maximum(seq_lengths, 1)\n",
        "\n",
        "        # Classification\n",
        "        pooled = self.dropout(pooled, deterministic=not training)\n",
        "        logits = self.classifier(pooled)\n",
        "\n",
        "        return {\n",
        "            \"logits\": logits,\n",
        "            \"attention_weights\": attention_weights,\n",
        "            \"hidden_states\": x,\n",
        "        }\n",
        "\n",
        "def create_model(config):\n",
        "    \"\"\"Create a SafetyTransformer model from configuration.\"\"\"\n",
        "    model_config = config[\"model\"]\n",
        "    return SafetyTransformer(\n",
        "        vocab_size=model_config[\"vocab_size\"],\n",
        "        embedding_dim=model_config[\"embedding_dim\"],\n",
        "        num_layers=model_config[\"num_layers\"],\n",
        "        num_heads=model_config[\"num_heads\"],\n",
        "        feedforward_dim=model_config[\"feedforward_dim\"],\n",
        "        max_sequence_length=model_config[\"max_sequence_length\"],\n",
        "        num_classes=model_config[\"num_classes\"],\n",
        "        dropout_rate=model_config[\"dropout_rate\"],\n",
        "    )\n",
        "\n",
        "def initialize_model(model, rng_key, input_shape=None):\n",
        "    \"\"\"Initialize model parameters.\"\"\"\n",
        "    if input_shape is None:\n",
        "        input_shape = (1, model.max_sequence_length)\n",
        "    \n",
        "    dummy_input = jnp.ones(input_shape, dtype=jnp.int32)\n",
        "    params = model.init(rng_key, dummy_input)\n",
        "    return params\n",
        "\n",
        "print(\"âœ… SafetyTransformer model defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data-loading"
      },
      "source": [
        "## ğŸ“Š Data Loading and Preprocessing\n",
        "\n",
        "Load safety datasets and prepare them for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data-loader"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
        "from transformers import AutoTokenizer\n",
        "import yaml\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class SafetyDatasetLoader:\n",
        "    \"\"\"Loads and preprocesses safety classification datasets for training.\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.data_config = config['data']\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.data_config['tokenizer'])\n",
        "        \n",
        "        # Safety categories mapping\n",
        "        self.safety_categories = {\n",
        "            'hate_speech': 0,\n",
        "            'self_harm': 1, \n",
        "            'dangerous_advice': 2,\n",
        "            'harassment': 3\n",
        "        }\n",
        "        \n",
        "    def create_synthetic_dataset(self, size: int = 1000) -> Dataset:\n",
        "        \"\"\"Create a synthetic dataset for testing and development.\"\"\"\n",
        "        logger.info(f\"Creating synthetic dataset with {size} examples...\")\n",
        "        \n",
        "        # Templates for different safety categories\n",
        "        templates = {\n",
        "            'hate_speech': [\n",
        "                \"I hate {group} people, they are terrible\",\n",
        "                \"{group} are the worst, they should not exist\",\n",
        "                \"All {group} are stupid and worthless\",\n",
        "            ],\n",
        "            'self_harm': [\n",
        "                \"I want to hurt myself by {method}\",\n",
        "                \"Instructions for self-harm: {method}\",\n",
        "                \"How to cause pain to yourself using {method}\",\n",
        "            ],\n",
        "            'dangerous_advice': [\n",
        "                \"Here's how to make {dangerous_item} at home\",\n",
        "                \"Instructions for creating {dangerous_item}\",\n",
        "                \"Easy recipe for {dangerous_item}\",\n",
        "            ],\n",
        "            'harassment': [\n",
        "                \"I'm going to find you and {threat}\",\n",
        "                \"You better watch out, I will {threat}\",\n",
        "                \"I know where you live, I will {threat}\",\n",
        "            ],\n",
        "            'safe': [\n",
        "                \"The weather is nice today\",\n",
        "                \"I enjoy reading books in my free time\",\n",
        "                \"Cooking is a wonderful hobby to have\",\n",
        "                \"Exercise is important for good health\",\n",
        "                \"I love spending time with friends and family\",\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        # Fill-in values for templates\n",
        "        groups = [\"women\", \"men\", \"teenagers\", \"elderly\"]\n",
        "        methods = [\"cutting\", \"burning\", \"hitting\"]\n",
        "        dangerous_items = [\"explosives\", \"poison\", \"weapons\"]\n",
        "        threats = [\"hurt you\", \"make you pay\", \"teach you a lesson\"]\n",
        "        \n",
        "        texts = []\n",
        "        labels = []\n",
        "        \n",
        "        # Generate examples for each category\n",
        "        examples_per_category = size // 5  # 5 categories including safe\n",
        "        \n",
        "        for category, category_templates in templates.items():\n",
        "            for _ in range(examples_per_category):\n",
        "                template = np.random.choice(category_templates)\n",
        "                \n",
        "                # Fill in template based on category\n",
        "                if category == 'hate_speech':\n",
        "                    text = template.format(group=np.random.choice(groups))\n",
        "                elif category == 'self_harm':\n",
        "                    text = template.format(method=np.random.choice(methods))\n",
        "                elif category == 'dangerous_advice':\n",
        "                    text = template.format(dangerous_item=np.random.choice(dangerous_items))\n",
        "                elif category == 'harassment':\n",
        "                    text = template.format(threat=np.random.choice(threats))\n",
        "                else:  # safe\n",
        "                    text = template\n",
        "                \n",
        "                texts.append(text)\n",
        "                \n",
        "                # Create one-hot encoded labels\n",
        "                label = [0, 0, 0, 0]\n",
        "                if category != 'safe':\n",
        "                    label[self.safety_categories[category]] = 1\n",
        "                labels.append(label)\n",
        "        \n",
        "        return Dataset.from_dict({\n",
        "            'text': texts,\n",
        "            'labels': labels,\n",
        "            'source': ['synthetic'] * len(texts)\n",
        "        })\n",
        "    \n",
        "    def tokenize_dataset(self, dataset: Dataset) -> Dataset:\n",
        "        \"\"\"Tokenize the text data using the configured tokenizer.\"\"\"\n",
        "        def tokenize_function(examples):\n",
        "            return self.tokenizer(\n",
        "                examples['text'],\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.data_config['max_length'],\n",
        "                return_tensors=None\n",
        "            )\n",
        "        \n",
        "        return dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=['text']\n",
        "        )\n",
        "    \n",
        "    def create_data_splits(self, dataset: Dataset) -> Tuple[Dataset, Dataset, Dataset]:\n",
        "        \"\"\"Split dataset into train, validation, and test sets.\"\"\"\n",
        "        train_split = self.data_config['train_split']\n",
        "        val_split = self.data_config['val_split']\n",
        "        test_split = self.data_config['test_split']\n",
        "        \n",
        "        # First split: separate test set\n",
        "        split_1 = dataset.train_test_split(test_size=test_split, seed=42)\n",
        "        train_val = split_1['train']\n",
        "        test = split_1['test']\n",
        "        \n",
        "        # Second split: separate train and validation\n",
        "        val_size_adjusted = val_split / (train_split + val_split)\n",
        "        split_2 = train_val.train_test_split(test_size=val_size_adjusted, seed=42)\n",
        "        train = split_2['train']\n",
        "        val = split_2['test']\n",
        "        \n",
        "        logger.info(f\"Data splits created: Train={len(train)}, Val={len(val)}, Test={len(test)}\")\n",
        "        return train, val, test\n",
        "    \n",
        "    def load_and_prepare_data(self) -> Tuple[Dataset, Dataset, Dataset]:\n",
        "        \"\"\"Main method to load, process, and prepare all datasets.\"\"\"\n",
        "        logger.info(\"Starting data loading and preparation...\")\n",
        "        \n",
        "        # For Colab, we'll use mostly synthetic data for speed\n",
        "        synthetic = self.create_synthetic_dataset(size=2000)\n",
        "        \n",
        "        # Try to load real data if available\n",
        "        datasets_to_combine = [synthetic]\n",
        "        \n",
        "        try:\n",
        "            # Load a small subset of toxic-chat for real data\n",
        "            toxic_chat = load_dataset(\"lmsys/toxic-chat\", \"toxicchat0124\", split=\"train[:1000]\")\n",
        "            \n",
        "            # Process toxic-chat data\n",
        "            processed_toxic = []\n",
        "            texts = []\n",
        "            labels = []\n",
        "            \n",
        "            for example in toxic_chat:\n",
        "                text = example.get('user_input', '')\n",
        "                toxicity = example.get('toxicity', 0)\n",
        "                \n",
        "                if text and toxicity is not None:\n",
        "                    texts.append(text)\n",
        "                    # Simple mapping: toxic -> harassment, non-toxic -> safe\n",
        "                    label = [0, 0, 0, 0]\n",
        "                    if toxicity > 0:\n",
        "                        label[3] = 1  # harassment\n",
        "                    labels.append(label)\n",
        "            \n",
        "            toxic_dataset = Dataset.from_dict({\n",
        "                'text': texts,\n",
        "                'labels': labels,\n",
        "                'source': ['toxic-chat'] * len(texts)\n",
        "            })\n",
        "            \n",
        "            datasets_to_combine.append(toxic_dataset)\n",
        "            logger.info(f\"Added {len(toxic_dataset)} toxic-chat examples\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not load toxic-chat: {e}\")\n",
        "        \n",
        "        # Combine all datasets\n",
        "        if len(datasets_to_combine) > 1:\n",
        "            combined_dataset = concatenate_datasets(datasets_to_combine)\n",
        "        else:\n",
        "            combined_dataset = datasets_to_combine[0]\n",
        "        \n",
        "        logger.info(f\"Combined dataset size: {len(combined_dataset)}\")\n",
        "        \n",
        "        # Create train/val/test splits\n",
        "        train, val, test = self.create_data_splits(combined_dataset)\n",
        "        \n",
        "        # Tokenize datasets\n",
        "        train_tokenized = self.tokenize_dataset(train)\n",
        "        val_tokenized = self.tokenize_dataset(val)\n",
        "        test_tokenized = self.tokenize_dataset(test)\n",
        "        \n",
        "        logger.info(\"Data preparation completed successfully!\")\n",
        "        return train_tokenized, val_tokenized, test_tokenized\n",
        "\n",
        "print(\"âœ… Data loading classes defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-data"
      },
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "print(\"ğŸ“Š Loading and preparing datasets...\")\n",
        "\n",
        "# Setup HuggingFace authentication (optional but removes warnings)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    if hf_token:\n",
        "        from huggingface_hub import login\n",
        "        login(token=hf_token)\n",
        "        print(\"âœ… HuggingFace authentication successful!\")\n",
        "    else:\n",
        "        print(\"â„¹ï¸  No HF_TOKEN found - using public access (this is fine)\")\n",
        "except ImportError:\n",
        "    print(\"â„¹ï¸  Not in Colab environment - using public access\")\n",
        "except Exception as e:\n",
        "    print(f\"â„¹ï¸  HF authentication skipped: {e}\")\n",
        "\n",
        "# Load config\n",
        "with open('configs/colab_config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Create data loader\n",
        "data_loader = SafetyDatasetLoader(config)\n",
        "\n",
        "# Load and prepare data\n",
        "train_dataset, val_dataset, test_dataset = data_loader.load_and_prepare_data()\n",
        "\n",
        "print(f\"âœ… Data loaded successfully!\")\n",
        "print(f\"   ğŸ“ˆ Train: {len(train_dataset)} examples\")\n",
        "print(f\"   ğŸ“Š Val: {len(val_dataset)} examples\")\n",
        "print(f\"   ğŸ§ª Test: {len(test_dataset)} examples\")\n",
        "\n",
        "# Show sample\n",
        "sample = train_dataset[0]\n",
        "print(f\"\\nğŸ“ Sample data:\")\n",
        "print(f\"   Input IDs: {sample['input_ids'][:10]}... (length: {len(sample['input_ids'])})\")\n",
        "print(f\"   Labels: {sample['labels']}\")\n",
        "print(f\"   Source: {sample['source']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training-setup"
      },
      "source": [
        "## ğŸš€ Training Setup\n",
        "\n",
        "Set up the training loop with JAX/Flax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training-state"
      },
      "outputs": [],
      "source": [
        "from flax import struct\n",
        "from flax.training import train_state, checkpoints\n",
        "import optax\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "@struct.dataclass\n",
        "class TrainState(train_state.TrainState):\n",
        "    \"\"\"Extended train state with additional metrics tracking.\"\"\"\n",
        "    epoch: int\n",
        "    best_val_accuracy: float\n",
        "    steps_since_improvement: int\n",
        "\n",
        "def create_optimizer(config):\n",
        "    \"\"\"Create the optimizer with learning rate schedule.\"\"\"\n",
        "    training_config = config['training']\n",
        "    \n",
        "    # Learning rate schedule\n",
        "    schedule = optax.warmup_cosine_decay_schedule(\n",
        "        init_value=0.0,\n",
        "        peak_value=training_config['learning_rate'],\n",
        "        warmup_steps=training_config['warmup_steps'],\n",
        "        decay_steps=training_config['max_steps'],\n",
        "        end_value=training_config['learning_rate'] * training_config['min_lr_ratio']\n",
        "    )\n",
        "    \n",
        "    # Create optimizer\n",
        "    optimizer = optax.adamw(\n",
        "        learning_rate=schedule,\n",
        "        weight_decay=training_config['weight_decay'],\n",
        "        b1=training_config['beta1'],\n",
        "        b2=training_config['beta2']\n",
        "    )\n",
        "    \n",
        "    # Add gradient clipping\n",
        "    optimizer = optax.chain(\n",
        "        optax.clip_by_global_norm(training_config['gradient_clip_norm']),\n",
        "        optimizer\n",
        "    )\n",
        "    \n",
        "    return optimizer\n",
        "\n",
        "def compute_loss(params, model, batch, rng_key=None, training=True):\n",
        "    \"\"\"Compute loss and metrics for a batch.\"\"\"\n",
        "    # Use provided RNG for dropout if training\n",
        "    rngs = {'dropout': rng_key} if training and rng_key is not None else None\n",
        "        \n",
        "    # Forward pass\n",
        "    outputs = model.apply(params, batch['input_ids'], training=training, rngs=rngs)\n",
        "    logits = outputs['logits']\n",
        "    \n",
        "    # Multi-label classification loss (binary cross-entropy)\n",
        "    labels = jnp.array(batch['labels'], dtype=jnp.float32)\n",
        "    loss = optax.sigmoid_binary_cross_entropy(logits, labels).mean()\n",
        "    \n",
        "    # Compute predictions and metrics\n",
        "    predictions = jax.nn.sigmoid(logits)\n",
        "    predicted_labels = (predictions > 0.5).astype(jnp.int32)\n",
        "    \n",
        "    # Accuracy (exact match for multi-label)\n",
        "    accuracy = jnp.mean(jnp.all(predicted_labels == labels, axis=-1))\n",
        "    \n",
        "    metrics = {\n",
        "        'loss': loss,\n",
        "        'accuracy': accuracy,\n",
        "        'predictions': predictions,\n",
        "        'predicted_labels': predicted_labels\n",
        "    }\n",
        "    \n",
        "    return loss, metrics\n",
        "\n",
        "def create_train_step(model):\n",
        "    \"\"\"Create JIT-compiled training step function.\"\"\"\n",
        "    @jax.jit\n",
        "    def train_step(state, batch, rng_key):\n",
        "        def loss_fn(params):\n",
        "            return compute_loss(params, model, batch, rng_key, training=True)\n",
        "        \n",
        "        # Compute gradients\n",
        "        (loss, metrics), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
        "        \n",
        "        # Update parameters\n",
        "        state = state.apply_gradients(grads=grads)\n",
        "        \n",
        "        # Add gradient norm to metrics\n",
        "        grad_norm = optax.global_norm(grads)\n",
        "        metrics['grad_norm'] = grad_norm\n",
        "        \n",
        "        return state, metrics\n",
        "    \n",
        "    return train_step\n",
        "\n",
        "def create_eval_step(model):\n",
        "    \"\"\"Create JIT-compiled evaluation step function.\"\"\"\n",
        "    @jax.jit\n",
        "    def eval_step(params, batch):\n",
        "        loss, metrics = compute_loss(params, model, batch, rng_key=None, training=False)\n",
        "        return metrics\n",
        "    \n",
        "    return eval_step\n",
        "\n",
        "def create_batch(dataset, batch_size, rng_key):\n",
        "    \"\"\"Create batches from dataset.\"\"\"\n",
        "    dataset_size = len(dataset)\n",
        "    indices = jax.random.permutation(rng_key, dataset_size)\n",
        "    \n",
        "    for i in range(0, dataset_size, batch_size):\n",
        "        batch_indices = indices[i:i + batch_size]\n",
        "        if len(batch_indices) < batch_size and i > 0:\n",
        "            break  # Skip incomplete last batch\n",
        "        \n",
        "        batch = {\n",
        "            'input_ids': jnp.array([dataset[int(idx)]['input_ids'] for idx in batch_indices]),\n",
        "            'labels': jnp.array([dataset[int(idx)]['labels'] for idx in batch_indices])\n",
        "        }\n",
        "        yield batch\n",
        "\n",
        "def save_model_simple(state, path, step):\n",
        "    \"\"\"Simple backup checkpointing method using pickle.\"\"\"\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    model_data = {\n",
        "        'params': state.params,\n",
        "        'step': step,\n",
        "        'best_val_accuracy': getattr(state, 'best_val_accuracy', 0.0),\n",
        "        'epoch': getattr(state, 'epoch', 0)\n",
        "    }\n",
        "    with open(f'{path}/model_step_{step}.pkl', 'wb') as f:\n",
        "        pickle.dump(model_data, f)\n",
        "\n",
        "print(\"âœ… Training functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-training"
      },
      "source": [
        "## ğŸ‹ï¸ Model Training\n",
        "\n",
        "Train the Safety Transformer model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "initialize-training"
      },
      "outputs": [],
      "source": [
        "# Initialize model and training state\n",
        "print(\"ğŸ”§ Initializing model and training state...\")\n",
        "\n",
        "# Create model\n",
        "model = create_model(config)\n",
        "print(f\"âœ… Model created: {config['model']['num_layers']} layers, {config['model']['embedding_dim']} dim\")\n",
        "\n",
        "# Initialize parameters\n",
        "rng = jax.random.PRNGKey(42)\n",
        "rng, init_rng = jax.random.split(rng)\n",
        "\n",
        "print(\"âš™ï¸ Initializing parameters...\")\n",
        "params = initialize_model(model, init_rng)\n",
        "print(\"âœ… Parameters initialized!\")\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = create_optimizer(config)\n",
        "print(\"âœ… Optimizer created!\")\n",
        "\n",
        "# Create JIT-compiled training and eval functions\n",
        "train_step = create_train_step(model)\n",
        "eval_step = create_eval_step(model)\n",
        "print(\"âœ… Training functions compiled!\")\n",
        "\n",
        "# Initialize training state\n",
        "state = TrainState.create(\n",
        "    apply_fn=model.apply,\n",
        "    params=params,\n",
        "    tx=optimizer,\n",
        "    epoch=0,\n",
        "    best_val_accuracy=0.0,\n",
        "    steps_since_improvement=0\n",
        ")\n",
        "\n",
        "# Count parameters\n",
        "param_count = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
        "print(f\"ğŸ“Š Model parameters: {param_count:,}\")\n",
        "print(f\"ğŸ¯ Training for {config['training']['max_steps']} steps\")\n",
        "print(f\"ğŸ“¦ Batch size: {config['training']['batch_size']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train-model"
      },
      "outputs": [],
      "source": [
        "# Main training loop\n",
        "print(\"ğŸš€ Starting training...\")\n",
        "\n",
        "batch_size = config['training']['batch_size']\n",
        "max_steps = config['training']['max_steps']\n",
        "eval_every = config['training']['eval_every']\n",
        "save_every = config['training']['save_every']\n",
        "\n",
        "step = 0\n",
        "training_metrics = []\n",
        "validation_metrics = []\n",
        "\n",
        "# Training progress bar\n",
        "pbar = tqdm(total=max_steps, desc=\"Training\")\n",
        "\n",
        "try:\n",
        "    while step < max_steps:\n",
        "        # Create training batches for this epoch\n",
        "        rng, epoch_rng = jax.random.split(rng)\n",
        "        \n",
        "        epoch_step = 0\n",
        "        for batch in create_batch(train_dataset, batch_size, epoch_rng):\n",
        "            # Get RNG key for this training step\n",
        "            rng, step_rng = jax.random.split(rng)\n",
        "            \n",
        "            # Training step\n",
        "            state, train_metrics = train_step(state, batch, step_rng)\n",
        "            step += 1\n",
        "            epoch_step += 1\n",
        "            \n",
        "            # Update progress bar\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix({\n",
        "                'loss': f\"{float(train_metrics['loss']):.4f}\",\n",
        "                'acc': f\"{float(train_metrics['accuracy']):.4f}\",\n",
        "                'grad_norm': f\"{float(train_metrics['grad_norm']):.3f}\"\n",
        "            })\n",
        "            \n",
        "            # Store metrics\n",
        "            training_metrics.append({\n",
        "                'step': step,\n",
        "                'loss': float(train_metrics['loss']),\n",
        "                'accuracy': float(train_metrics['accuracy']),\n",
        "                'grad_norm': float(train_metrics['grad_norm'])\n",
        "            })\n",
        "            \n",
        "            # Evaluation\n",
        "            if step % eval_every == 0:\n",
        "                print(f\"\\nğŸ“Š Evaluation at step {step}...\")\n",
        "                \n",
        "                # Run evaluation\n",
        "                eval_metrics = []\n",
        "                rng, eval_rng = jax.random.split(rng)\n",
        "                \n",
        "                for val_batch in create_batch(val_dataset, batch_size, eval_rng):\n",
        "                    metrics = eval_step(state.params, val_batch)\n",
        "                    eval_metrics.append(metrics)\n",
        "                \n",
        "                # Aggregate metrics\n",
        "                avg_loss = np.mean([float(m['loss']) for m in eval_metrics])\n",
        "                avg_accuracy = np.mean([float(m['accuracy']) for m in eval_metrics])\n",
        "                \n",
        "                print(f\"   Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")\n",
        "                \n",
        "                # Store validation metrics\n",
        "                validation_metrics.append({\n",
        "                    'step': step,\n",
        "                    'loss': avg_loss,\n",
        "                    'accuracy': avg_accuracy\n",
        "                })\n",
        "                \n",
        "                # Check for improvement\n",
        "                if avg_accuracy > state.best_val_accuracy:\n",
        "                    print(f\"   ğŸ‰ New best accuracy: {avg_accuracy:.4f}\")\n",
        "                    state = state.replace(\n",
        "                        best_val_accuracy=avg_accuracy,\n",
        "                        steps_since_improvement=0\n",
        "                    )\n",
        "                    \n",
        "                    # Save best model\n",
        "                    try:\n",
        "                        checkpoints.save_checkpoint(\n",
        "                            'checkpoints/best',\n",
        "                            state,\n",
        "                            step=step,\n",
        "                            keep=1,\n",
        "                            overwrite=True\n",
        "                        )\n",
        "                    except Exception as e:\n",
        "                        print(f\"   âš ï¸  Checkpoint save failed: {e}\")\n",
        "                        print(\"   Trying simple backup method...\")\n",
        "                        try:\n",
        "                            save_model_simple(state, 'checkpoints/best_backup', step)\n",
        "                            print(\"   âœ… Backup checkpoint saved!\")\n",
        "                        except Exception as e2:\n",
        "                            print(f\"   âš ï¸  Backup also failed: {e2}\")\n",
        "                else:\n",
        "                    state = state.replace(\n",
        "                        steps_since_improvement=state.steps_since_improvement + 1\n",
        "                    )\n",
        "            \n",
        "            # Save checkpoint\n",
        "            if step % save_every == 0:\n",
        "                try:\n",
        "                    checkpoints.save_checkpoint(\n",
        "                        'checkpoints',\n",
        "                        state,\n",
        "                        step=step,\n",
        "                        keep=3,\n",
        "                        overwrite=True\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"   âš ï¸  Regular checkpoint save failed: {e}\")\n",
        "            \n",
        "            if step >= max_steps:\n",
        "                break\n",
        "        \n",
        "        # Update epoch\n",
        "        state = state.replace(epoch=state.epoch + 1)\n",
        "        \n",
        "        if step >= max_steps:\n",
        "            break\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nâ¹ï¸  Training interrupted by user\")\n",
        "\n",
        "finally:\n",
        "    pbar.close()\n",
        "\n",
        "print(f\"\\nğŸ‰ Training completed!\")\n",
        "print(f\"ğŸ“Š Final step: {step}\")\n",
        "print(f\"ğŸ† Best validation accuracy: {state.best_val_accuracy:.4f}\")\n",
        "\n",
        "# Save final checkpoint\n",
        "try:\n",
        "    checkpoints.save_checkpoint('checkpoints/final', state, step=step, keep=1, overwrite=True)\n",
        "    print(\"ğŸ’¾ Final model saved!\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  Final checkpoint save failed: {e}\")\n",
        "    print(\"Model training completed but checkpoint not saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## ğŸ“Š Model Evaluation\n",
        "\n",
        "Evaluate the trained model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate-model"
      },
      "outputs": [],
      "source": [
        "# Final evaluation on test set\n",
        "print(\"ğŸ§ª Running final evaluation on test set...\")\n",
        "\n",
        "# Load best model\n",
        "best_state = checkpoints.restore_checkpoint('checkpoints/best', state)\n",
        "\n",
        "# Run test evaluation\n",
        "test_metrics = []\n",
        "rng, test_rng = jax.random.split(rng)\n",
        "\n",
        "for test_batch in create_batch(test_dataset, batch_size, test_rng):\n",
        "    metrics = eval_step(best_state.params, test_batch)\n",
        "    test_metrics.append(metrics)\n",
        "\n",
        "# Aggregate test metrics\n",
        "test_loss = np.mean([float(m['loss']) for m in test_metrics])\n",
        "test_accuracy = np.mean([float(m['accuracy']) for m in test_metrics])\n",
        "\n",
        "print(f\"\\nğŸ“Š Test Results:\")\n",
        "print(f\"   ğŸ¯ Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"   ğŸ“‰ Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Check if we met our target\n",
        "if test_accuracy >= 0.85:\n",
        "    print(f\"\\nğŸ‰ SUCCESS! Achieved target accuracy of 85%+\")\n",
        "    print(f\"   Stage 1 of Constitutional AI pipeline COMPLETE!\")\n",
        "else:\n",
        "    print(f\"\\nğŸ“ˆ Progress: {test_accuracy:.1%} towards 85% target\")\n",
        "    print(f\"   Consider training longer or adjusting hyperparameters\")\n",
        "\n",
        "print(f\"\\nğŸ† Training Summary:\")\n",
        "print(f\"   ğŸ“Š Total steps: {step}\")\n",
        "print(f\"   ğŸ¯ Best val accuracy: {best_state.best_val_accuracy:.4f}\")\n",
        "print(f\"   ğŸ§ª Final test accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"   ğŸ’¾ Model saved in: checkpoints/best/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demo"
      },
      "source": [
        "## ğŸ­ Interactive Demo\n",
        "\n",
        "Test the trained model on sample safety classifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "safety-demo"
      },
      "outputs": [],
      "source": [
        "# Interactive safety classification demo\n",
        "print(\"ğŸ­ Safety Text Classifier Demo\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Safety categories\n",
        "categories = ['Hate Speech', 'Self-Harm', 'Dangerous Advice', 'Harassment']\n",
        "\n",
        "# Test cases\n",
        "test_cases = [\n",
        "    \"The weather is beautiful today, perfect for a walk.\",\n",
        "    \"I hate all people from that group, they are terrible.\",\n",
        "    \"Here's how to make dangerous explosives at home.\",\n",
        "    \"I'm going to find where you live and hurt you.\",\n",
        "    \"I feel sad and want to hurt myself badly.\",\n",
        "    \"I love spending time with my family and friends.\",\n",
        "    \"You should try the new restaurant downtown.\",\n",
        "    \"I think all members of that community are worthless.\"\n",
        "]\n",
        "\n",
        "print(\"ğŸ” Testing Safety Classification:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for i, text in enumerate(test_cases, 1):\n",
        "    # Tokenize\n",
        "    encoded = data_loader.tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=config['data']['max_length'],\n",
        "        return_tensors=None\n",
        "    )\n",
        "    input_ids = jnp.array([encoded['input_ids']])\n",
        "    \n",
        "    # Predict\n",
        "    outputs = model.apply(best_state.params, input_ids, training=False)\n",
        "    probabilities = jax.nn.sigmoid(outputs['logits'])[0]  # Remove batch dim\n",
        "    predictions = (probabilities > 0.5).astype(int)\n",
        "    \n",
        "    # Find flagged categories\n",
        "    flagged_categories = [categories[j] for j, flag in enumerate(predictions) if flag]\n",
        "    \n",
        "    # Determine status\n",
        "    if flagged_categories:\n",
        "        status = f\"âš ï¸  UNSAFE: {', '.join(flagged_categories)}\"\n",
        "        max_prob = float(jnp.max(probabilities))\n",
        "        status += f\" (confidence: {max_prob:.2f})\"\n",
        "    else:\n",
        "        status = \"âœ… SAFE\"\n",
        "    \n",
        "    print(f\"{i:2d}. \\\"{text[:45]}{'...' if len(text) > 45 else ''}\\\"\")\n",
        "    print(f\"     â†’ {status}\")\n",
        "    \n",
        "    # Show individual category scores\n",
        "    if any(probabilities > 0.1):  # Only show if any category has reasonable confidence\n",
        "        print(\"     ğŸ“Š Category scores:\")\n",
        "        for j, (cat, prob) in enumerate(zip(categories, probabilities)):\n",
        "            if prob > 0.1:\n",
        "                emoji = \"ğŸš¨\" if predictions[j] else \"âš¡\"\n",
        "                print(f\"        {emoji} {cat}: {float(prob):.2f}\")\n",
        "    print()\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"ğŸ‰ Demo completed!\")\n",
        "print(\"\\nğŸ“ˆ Model Performance:\")\n",
        "print(f\"   ğŸ¯ Test Accuracy: {test_accuracy:.1%}\")\n",
        "print(f\"   ğŸ“Š Parameters: {param_count:,}\")\n",
        "print(f\"   âš¡ Framework: JAX/Flax with GPU acceleration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results-visualization"
      },
      "source": [
        "## ğŸ“ˆ Training Results Visualization\n",
        "\n",
        "Visualize the training progress and metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot-results"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Create visualizations\n",
        "print(\"ğŸ“Š Creating training visualizations...\")\n",
        "\n",
        "# Convert metrics to DataFrames\n",
        "train_df = pd.DataFrame(training_metrics)\n",
        "val_df = pd.DataFrame(validation_metrics)\n",
        "\n",
        "# Create subplots\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Safety Text Classifier Training Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Training loss\n",
        "ax1.plot(train_df['step'], train_df['loss'], label='Training Loss', alpha=0.7)\n",
        "if len(val_df) > 0:\n",
        "    ax1.plot(val_df['step'], val_df['loss'], label='Validation Loss', marker='o')\n",
        "ax1.set_xlabel('Training Step')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training & Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Training accuracy\n",
        "ax2.plot(train_df['step'], train_df['accuracy'], label='Training Accuracy', alpha=0.7)\n",
        "if len(val_df) > 0:\n",
        "    ax2.plot(val_df['step'], val_df['accuracy'], label='Validation Accuracy', marker='o')\n",
        "    ax2.axhline(y=0.85, color='r', linestyle='--', alpha=0.7, label='Target (85%)')\n",
        "ax2.set_xlabel('Training Step')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('Training & Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Gradient norm\n",
        "ax3.plot(train_df['step'], train_df['grad_norm'], alpha=0.7)\n",
        "ax3.set_xlabel('Training Step')\n",
        "ax3.set_ylabel('Gradient Norm')\n",
        "ax3.set_title('Gradient Norm During Training')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Performance summary\n",
        "ax4.axis('off')\n",
        "summary_text = f\"\"\"\n",
        "ğŸ“Š TRAINING SUMMARY\n",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "ğŸ¯ Final Test Accuracy: {test_accuracy:.1%}\n",
        "ğŸ† Best Val Accuracy: {best_state.best_val_accuracy:.1%}\n",
        "ğŸ“ˆ Total Training Steps: {step:,}\n",
        "ğŸ’¾ Model Parameters: {param_count:,}\n",
        "\n",
        "ğŸ—ï¸ MODEL ARCHITECTURE\n",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "ğŸ“ Layers: {config['model']['num_layers']}\n",
        "ğŸ§  Embedding Dim: {config['model']['embedding_dim']}\n",
        "ğŸ‘ï¸ Attention Heads: {config['model']['num_heads']}\n",
        "ğŸ“ Max Seq Length: {config['model']['max_sequence_length']}\n",
        "\n",
        "ğŸ“Š DATASET INFO\n",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
        "ğŸš‚ Train Examples: {len(train_dataset):,}\n",
        "âœ… Val Examples: {len(val_dataset):,}\n",
        "ğŸ§ª Test Examples: {len(test_dataset):,}\n",
        "ğŸ¯ Categories: 4 safety types\n",
        "\n",
        "{'ğŸ‰ TARGET ACHIEVED!' if test_accuracy >= 0.85 else 'ğŸ“ˆ PROGRESS MADE!'}\n",
        "\"\"\"\n",
        "\n",
        "ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes, fontsize=10, \n",
        "         verticalalignment='top', fontfamily='monospace')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Visualizations complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download-model"
      },
      "source": [
        "## ğŸ’¾ Download Trained Model\n",
        "\n",
        "Save and download the trained model for use in local development."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save-download"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "print(\"ğŸ’¾ Preparing model for download...\")\n",
        "\n",
        "# Create a comprehensive model package\n",
        "download_files = {\n",
        "    'checkpoints/': 'Model checkpoints',\n",
        "    'configs/colab_config.yaml': 'Model configuration',\n",
        "}\n",
        "\n",
        "# Create model info file\n",
        "model_info = f\"\"\"# Safety Text Classifier - Trained Model\n",
        "\n",
        "**Trained on**: Google Colab with GPU acceleration\n",
        "**Framework**: JAX/Flax\n",
        "**Date**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## Performance Metrics\n",
        "- **Test Accuracy**: {test_accuracy:.1%}\n",
        "- **Best Validation Accuracy**: {best_state.best_val_accuracy:.1%}\n",
        "- **Total Training Steps**: {step:,}\n",
        "- **Model Parameters**: {param_count:,}\n",
        "\n",
        "## Model Architecture\n",
        "- **Layers**: {config['model']['num_layers']}\n",
        "- **Embedding Dimension**: {config['model']['embedding_dim']}\n",
        "- **Attention Heads**: {config['model']['num_heads']}\n",
        "- **Max Sequence Length**: {config['model']['max_sequence_length']}\n",
        "- **Safety Categories**: 4 (Hate Speech, Self-Harm, Dangerous Advice, Harassment)\n",
        "\n",
        "## Dataset Information\n",
        "- **Training Examples**: {len(train_dataset):,}\n",
        "- **Validation Examples**: {len(val_dataset):,}\n",
        "- **Test Examples**: {len(test_dataset):,}\n",
        "- **Data Sources**: Synthetic safety data + toxic-chat dataset\n",
        "\n",
        "## Usage\n",
        "1. Load the model configuration from `colab_config.yaml`\n",
        "2. Restore checkpoint from `checkpoints/best/`\n",
        "3. Use for safety text classification inference\n",
        "\n",
        "## Next Steps - Constitutional AI Pipeline\n",
        "- **Stage 1**: âœ… Safety Text Classifier (COMPLETE)\n",
        "- **Stage 2**: Helpful Response Fine-tuning (Gemma 7B-IT)\n",
        "- **Stage 3**: Critique and Revision System\n",
        "- **Stage 4**: Full Constitutional AI with RLAIF\n",
        "\n",
        "{'ğŸ‰ Stage 1 TARGET ACHIEVED - Ready for Stage 2!' if test_accuracy >= 0.85 else 'ğŸ“ˆ Good progress - consider additional training for 85%+ target'}\n",
        "\"\"\"\n",
        "\n",
        "with open('MODEL_INFO.md', 'w') as f:\n",
        "    f.write(model_info)\n",
        "\n",
        "# Create training log\n",
        "training_log = {\n",
        "    'training_metrics': training_metrics,\n",
        "    'validation_metrics': validation_metrics,\n",
        "    'final_test_accuracy': float(test_accuracy),\n",
        "    'final_test_loss': float(test_loss),\n",
        "    'config': config,\n",
        "    'model_info': {\n",
        "        'total_parameters': int(param_count),\n",
        "        'training_steps': int(step),\n",
        "        'best_val_accuracy': float(best_state.best_val_accuracy)\n",
        "    }\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('training_log.json', 'w') as f:\n",
        "    json.dump(training_log, f, indent=2)\n",
        "\n",
        "print(\"ğŸ“¦ Creating download package...\")\n",
        "\n",
        "# Create zip file\n",
        "zip_filename = f\"safety_text_classifier_trained_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.zip\"\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    # Add model files\n",
        "    for root, dirs, files in os.walk('checkpoints'):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path, file_path)\n",
        "    \n",
        "    # Add config and info files\n",
        "    zipf.write('configs/colab_config.yaml', 'colab_config.yaml')\n",
        "    zipf.write('MODEL_INFO.md', 'MODEL_INFO.md')\n",
        "    zipf.write('training_log.json', 'training_log.json')\n",
        "\n",
        "print(f\"âœ… Package created: {zip_filename}\")\n",
        "print(f\"ğŸ“Š Package size: {os.path.getsize(zip_filename) / 1024 / 1024:.1f} MB\")\n",
        "\n",
        "# Download the model\n",
        "print(\"ğŸš€ Starting download...\")\n",
        "files.download(zip_filename)\n",
        "\n",
        "print(\"\\nğŸ‰ Model download complete!\")\n",
        "print(\"\\nğŸ“‹ What you got:\")\n",
        "print(\"   ğŸ’¾ Trained model checkpoints\")\n",
        "print(\"   âš™ï¸ Model configuration file\")\n",
        "print(\"   ğŸ“Š Training metrics and logs\")\n",
        "print(\"   ğŸ“– Complete model documentation\")\n",
        "\n",
        "if test_accuracy >= 0.85:\n",
        "    print(\"\\nğŸ¯ ğŸ‰ CONGRATULATIONS! ğŸ‰\")\n",
        "    print(\"   Stage 1 of Constitutional AI pipeline COMPLETE!\")\n",
        "    print(\"   Ready to move to Stage 2: Gemma 7B-IT Fine-tuning\")\n",
        "else:\n",
        "    print(f\"\\nğŸ“ˆ Great progress! ({test_accuracy:.1%} towards 85% target)\")\n",
        "    print(\"   Consider training longer or adjusting hyperparameters\")\n",
        "    print(\"   Still good foundation for Stage 2 development\")\n",
        "\n",
        "print(\"\\nğŸš€ Next Steps:\")\n",
        "print(\"   1. Extract the downloaded model package locally\")\n",
        "print(\"   2. Integrate with your local development environment\")\n",
        "print(\"   3. Begin Stage 2: Helpful Response Fine-tuning with Gemma 7B-IT\")\n",
        "print(\"   4. Continue Constitutional AI research pipeline\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}